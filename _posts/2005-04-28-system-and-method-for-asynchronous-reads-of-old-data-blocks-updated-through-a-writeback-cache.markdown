---

title: System and method for asynchronous reads of old data blocks updated through a write-back cache
abstract: A system for asynchronous reads of old data blocks updated through a write-back cache includes a storage device, a write-back cache, a storage consumer, a storage processing node, and device management software. The device management software may be configured to store a new version of a data block in the write-back cache in response to an update request from the first storage consumer and to then send an update completion notification to the first storage consumer. Some time after the update completion notification has been sent, the device management software may be configured to send a previous version of the updated data block to the storage processing node, where it may be required to perform an operation such as a copy-on write operation or a snapshot update.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07636814&OS=07636814&RS=07636814
owner: Symantec Operating Corporation
number: 07636814
owner_city: Mountain View
owner_country: US
publication_date: 20050428
---
This invention relates to computer systems and more particularly to the management of updates within storage environments employing write back caches.

Many business organizations and governmental entities rely upon applications that access large amounts of data often exceeding a terabyte or more of data for mission critical applications. Often such data is stored on many different storage devices which may be heterogeneous in nature including many different types of devices from many different manufacturers. In such storage environments various storage devices may be configured with associated write back caches to reduce the latency for update operations as seen by the requesting applications.

A write back cache may be implemented using memory technology that supports efficient write operations e.g. various kinds of Random Access Memory or RAM especially when compared to the latencies associated with committing updates to an underlying storage device such as a disk. In many environments a non volatile write back cache may be employed e.g. using battery backed RAM so that data stored within the cache is recoverable in the event of certain kinds of failures even if it has not been written to underlying storage. When an application requests that a given block of data within a storage device equipped with a write back cache be updated the updated or new version of the data may be stored within the write back cache. For many kinds of write operations and for various storage devices an indication that the write operation has completed may be provided to the requesting application as soon as the new version of the data block is written to the write back cache instead of waiting until the underlying storage device is updated. Subsequently for example when the proportion of dirty data in the write back cache i.e. updated data that has not been committed to the underlying storage device may be close to a specified threshold level updated versions of data blocks temporarily stored in the write back cache may be written to the underlying storage devices.

For some storage devices that support storage operations such as copy on write snapshots however the previous version of a data block being updated may be needed at a storage processing node other than the data consumer requesting the update before the previous version is overwritten with the new version. The storage processing node may be any software or hardware entity such as a physical or logical device a thread or process a module a host or a server where the previous version of the data block may be needed to perform a particular storage related function. Storage processing nodes may be linked to the storage device e.g. over a network and may include for example other storage consumers other storage devices or metadata servers such as a volume server in a distributed virtualization environment The previous version of a data block from one node of a distributed storage system may be required for example to complete a copy on write operation for a snapshot being maintained at another node of the distributed storage system. Similarly in a distributed virtual RAID device a previous version of a data block at one node of the device and the previous version of a corresponding parity block may be required to compute a new version of the parity block at another node of the device.

In order to prevent an unintended loss of the old version due to an overwrite the previous version may typically be provided to the storage processing node prior to writing the new version of the data in the write back cache in such storage environments. In some cases an acknowledgment may also be required from the storage processing node indicating that the previous version has been received and acted on appropriately e.g. that the processing that required the previous version of the data block has been completed or at least that the previous version is no longer required . In many cases especially when the write back cache may be relatively small compared with the total amount of data being managed within the storage device or as is often the case the cache prioritizes writes over reads the previous version of the data block will not be within the cache of the storage device and will have to be read from the underlying backing storage device and delivered to the storage processing node which may require transmission over a network . The latency of reading a data block from the backing storage device may itself be comparable to the latency of writing a data block to the backing storage device. Any additional processing or delay involved in sending the previous version to the second consumer and receiving any required acknowledgments may further extend the time taken to complete the requested update as seen by the data consumer. Such a synchronous read of a previous version of a data block during an update i.e. reading a previous version of a data block prior to notifying an updating storage consumer that a requested update has completed may thus result in a substantial update latency penalty to the updating consumer. For example the latency of writing into a RAM cache may be replaced by the latency of a read from disk followed by a write to the RAM cache and potentially additional latencies associated with intervening operations such as reading parity or updating a log. The end user or application requesting the update may be unaware that a previous version of the data block being updated is required elsewhere which may add further to the perceived latency penalty. In such environments it may be desirable to develop a technique that reduces data block update latency as seen by an updating storage consumer.

Various embodiments of a system and method for asynchronous reads of old data blocks updated through a write back cache are disclosed. According to a first embodiment a system may include a storage device a write back cache a storage consumer a storage processing node and device management software. The storage device may be a physical storage device such as a disk or a disk array or it may be a virtual storage device such as a file system a logical volume or an object storage device. The storage processing node may be any software or hardware entity where the previous version of the data block may be needed to perform a particular storage related function. The device management software may be configured to store a new version of a data block in the write back cache in response to an update request from the storage consumer and to then send an update completion notification to the storage consumer. Some time after the update completion notification has been sent the device management software may be configured to send a previous version of the updated data block to the storage processing node where it may be needed in order to perform an operation such as a copy on write operation for a snapshot update. By providing the previous version to the second storage consumer asynchronously with respect to the update request the device management software may help to reduce the update latency seen at the storage consumer. After the previous version is sent to the storage processing node and after processing based on the previous version has completed the device management software may write the updated data block to a backing storage device overwriting the previous version.

In a second embodiment a system may include a first and a second storage server wherein each storage server includes a respective write back cache and a respective backing storage device a virtual device server and a storage consumer. The virtual device server may be configured to aggregate storage within the first and second storage servers may as a distributed virtual storage device using redundancy for high availability. That is an update operation requested by the storage consumer may require redundancy information to be computed using at least a previous version of a first data block stored at the first storage server and a previous version of a second data block stored at the second storage server . In response to a request from the data consumer to update the first data block where the request includes a new version of the first data block the first storage server may be configured to store the new version of the first data block in its write back cache and send an update completion notification to the data consumer. Later prior to replacing the previous version of the first data block at its backing storage device the first storage server may be configured to send a copy of the previous version of the first data block to the second storage server. The redundancy computation may then be performed at the second storage server.

In one particular embodiment the distributed virtual storage device may be a virtual RAID device that is the redundancy computation may be a RAID parity computation and the second data block may be a parity block. The write back caches at the first and second storage servers may be non volatile caches in such an embodiment. In addition to sending the update request to the first storage server the data consumer may also be configured to send a parity update request also including the new version of the first data block to the second storage server. The second storage server may be configured to respond to the parity update request by storing the new version of the first data block in its write back cache and sending a parity update completion notification to the data consumer. Later and asynchronously with respect to the parity update request the second storage server may derive a new version of the parity block i.e. the second data block using the previous versions of the first data block and the second data block and the new version of the first data block e.g. by a series of XOR operations.

While the invention is susceptible to various modifications and alternative forms specific embodiments are shown by way of example in the drawings and are herein described in detail. It should be understood however that drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed but on the contrary the invention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.

In general a write back cache such as cache may be any efficient front end memory storage area for a corresponding back end storage device such as a disk or a disk array where a data block updated by a storage consumer e.g. may be temporarily stored before being written to the back end storage device. The back end storage device which may also be termed a backing storage device may contain the physical data corresponding to one or more logical storage devices such as file systems and logical volumes thus a write back cache may also be associated with one or more logical storage devices. In an environment employing a write back cache the previous version of the data block may be available at the back end storage device while the updated version is available in the cache. A write back cache differs from a write through cache in that in the case of a write through cache an updated data block may be written in parallel to both the back end storage device and the write through cache and the update for a write through cache is not usually considered complete until it completes on the back end storage device. In some embodiments a write back cache may be configured as a non volatile cache i.e. a mechanism such as a battery back up may be used to ensure that data stored within the cache is not lost in the event of a failure such as a power loss.

A storage consumer such as may send a request to update data within storage device to device management software . As described below in further detail the characteristics of storage device and the storage functionality provided by storage device may differ in different embodiments. For example in some embodiments storage device may be a block storage device while in others it may be an object storage device. Further storage device may be a physical storage device e.g. a disk array device or a virtual storage device such as a file system a mirrored logical volume or a distributed virtual RAID device. A physical storage device may comprise backing storage i.e. long term persistent storage such as disk storage while a logical storage device may have associated backing storage i.e. physical storage where the data of the logical storage device may be persistently stored . Both physical and logical storage devices may have an associated front end write back cache which may be a non volatile cache memory to which writes are staged prior to eventually being written to the backing storage. Multiple backing storage devices may be associated with a single storage device in some embodiments. Depending upon the storage functionality being provided by storage device some or all of the update requests from storage consumer may require special handling in that the previous version of the data being updated may be required at a storage processing node before it is overwritten within storage device .

As used herein a storage processing node may be any software or hardware entity such as a physical or logical device a thread or process a module a host or a server where the previous version of the data block may be needed to perform a particular storage related function. Storage processing node may be linked to the storage device e.g. over a network and may include for example storage consumers other than storage consumer storage devices other than storage device or metadata servers such as a volume server in a distributed virtualization environment as described below in further detail. In some embodiments storage processing node may be a software module executing at the same host as the storage consumer or even a different thread of the same executable program as the storage consumer . Storage processing node may require the previous version of the data to perform or complete one or more operations such as snapshot creation or update a redundancy computation etc. Further examples of the conditions under which the previous version of the updated data may be required at a storage processing node are provided below.

Device management software may be configured to utilize write back cache to provide the special handling required for update requests in an efficient manner. a block diagram and a flow diagram collectively illustrate a sequence of operations performed in one embodiment of system to update a unit of data e.g. a block or an object such as a file of storage device . A request to update the data containing a new version of the data to be stored may be sent by storage consumer block of and received at device management software operation of . Device management software may be configured to store the new version of the data within write back cache in response to the update request operation of block of . When the new version has been stored in write back cache device management software may send a notification indicating that the update request has been completed back to storage consumer operation of block of thereby allowing storage consumer to proceed to the next operation after receiving the notification blocks of .

After the notification has been sent device management software may provide a previous version of the data block to storage processing node operation of block of . The previous version of the data block may have to be read from storage device by device management software . The previous version may be provided to storage processing node some time after the notification has been sent i.e. it may be provided asynchronously with respect to the update request. This asynchronous operation may ensure that latency associated with obtaining the previous version and providing it to storage processing node B which may require one or more messages to be sent over a network in some embodiments does not affect the responsiveness of the system to the update request as seen at storage consumer . As described below in further detail one or more stimuli such as an explicit request for the previous version from storage processing node may trigger the scheduling of the provision of the previous version to storage processing node .

Once the previous version of the data has been supplied to storage processing node device management software may overwrite the previous version with the new version operation of block of . Having received the previous version of the data block of storage processing node may proceed to perform the desired operation upon the previous version of the data block of . These two operations the overwriting of the previous version of the data by device management software and the operation upon the previous version of the data by storage processing node may be scheduled independently of each other based upon various factors. For example the previous version of the data may be overwritten in order to free up space within write back cache while the operation upon the previous version of the data may be scheduled by an operating system scheduler at a server hosting storage processing node .

As noted earlier in some embodiments storage device may be a physical storage device such as a physical block device. Generally speaking a physical block device may comprise any hardware entity that provides a collection of linearly addressed data blocks that can be read or written. For example in one embodiment a physical block device may be a single disk drive configured to present all of its sectors as an indexed array of blocks. In another embodiment as illustrated in a set of individual disk drives A C may be organized as a single storage device in the form of a hardware disk array . Hardware disk array may present data from its constituent disk drives A C as a single contiguous storage address range to a storage consumer . It is contemplated that any suitable type of storage device may be configured singly or as part of a collection or an array as a physical block device such as fixed or removable magnetic media drives e.g. hard drives floppy or Zip based drives writable or read only optical media drives e.g. CD or DVD tape drives solid state mass storage devices or any other type of storage device. Physical storage devices such as disks or tape drives may be configured to present some form of SCSI interface though other interfaces are possible and contemplated.

In some embodiments storage device may be a virtual device such as a file system or a virtual block device e.g. a logical volume rather than a physical device. is a block diagram illustrating one embodiment where a virtual device server is configured to aggregate storage within physical storage devices A and B into a virtual storage device and to make virtual storage device accessible to storage consumers such as virtual device consumers . As stated above a storage processing node may be a storage consumer in some embodiments. Therefore in the subsequent discussion on virtualization the term virtual device consumer will generally include those storage processing nodes that are also storage consumers that is the general description of virtual device consumers may be assumed to apply to the subset of storage processing nodes that are also storage consumers. In the illustrated embodiment access to a given physical storage device i.e. A or B may be provided by a corresponding storage server A or B . Each storage server may incorporate a corresponding write back cache and may be configured to execute device management software . In some embodiments a single write back cache may be employed for a virtual device whose data may be spread over multiple physical storage devices . Virtual device server virtual device consumers and storage servers may communicate with each other over a network . If virtual storage device is a logical volume i.e. if it provides a block level interface to storage consumers virtual device server may be termed a volume server and virtual device consumers may be termed volume consumers.

It is noted that in the storage virtualization environment depicted in virtual device server may require access to storage within virtual storage device for example to perform configuration or recovery operations on device and may thus be considered an additional storage processing node for device . Similarly for some storage operations such as the parity computations described below device management software from another storage server e.g. from storage server B may also be a storage processing node for the part of virtual storage device mapped to physical storage device A at a give storage server A.

Storage within physical devices may be mapped into a hierarchy of different kinds of virtual storage devices in some embodiments for example a file system may use one or more logical volumes while a given logical volume may map storage from one or more physical storage devices. It is contemplated that in some embodiments data blocks may be uniformly sized across different physical and logical block devices while in other embodiments physical and logical block devices may employ different block sizes. It is also contemplated that in some embodiments block sizes may vary among particular physical block devices and or particular logical block devices or even within a given block device.

A block device may differ from a file in that it may not require use of a file system for access that is a consumer of a block device may read or write blocks directly to the device bypassing any file system that may be in use. In some embodiments a block storage device presented by an operating system for use by a consumer may present relatively few primitives through which the device may be manipulated. For example in one embodiment a block storage device may support open close read and write primitives plus a few miscellaneous control and query primitives. In contrast a file system may provide a richer set of primitives such as support for creating and removing files appending to files creating and removing directories etc.

In one embodiment where block virtualization is employed a storage server may be configured to implement an advanced block I O interface optimized to service requests from virtual device servers and virtual device consumers . That is virtual device servers and virtual device consumers may communicate with storage server over network using an advanced storage access protocol that may differ from more traditional interfaces such as variants of SCSI or iSCSI . The advanced storage access protocol may support features such as access security and tagged directives for distributed I O operations that may not be adequately supported by the traditional interfaces alone. In such an embodiment storage server may use device management software to translate data access requests from the advanced storage protocol to a lower level protocol or interface such as SCSI that may be presented by the physical storage devices managed at storage server . While the advanced storage access protocol may provide enhanced functionality it may still allow block level access to physical storage devices . E.g. virtual device consumers and virtual device servers may still access a block of physical block device without requiring the use of a file system.

Generally speaking a logical volume such as virtual device may comprise a block device that may be presented directly for use by a block device consumer e.g. a virtual device consumer . In one embodiment a virtual device consumer may be a file system or an application such as a database application for example that can directly use block devices. As described in greater detail below in some embodiments employing block device virtualization a given volume such as virtual device may be associated with several logical or physical block devices. In such embodiments each block device included in the logical organization of a given volume or virtualized block device may be referred to as a storage object or logical storage object.

A volume may differ from a block device interface implemented in a hardware device or that is accessed through a system disk driver in that the latter block devices may not present a system independent block device interface that can be opened for direct use by a consumer. Instead a system dependent disk driver may be required to access such block devices. In embodiments employing block virtualization such a disk driver may be generally unaware of block virtualization and may in some instances present a barrier to using some virtualization techniques whereas a volume implementing various block virtualization features may be directly accessible by a consumer without the issues presented by such disk drivers.

A volume manager such as virtual device server may introduce virtualization of blocks creating some number of virtualized block devices out of one or more physical or logical block devices. In some embodiments devices such as intelligent disk arrays and virtualization switches may also be configured to perform block virtualization. In one embodiment of block virtualization one or more layers of software and or hardware rearrange blocks from one or more block devices such as disks and add various kinds of functions. The resulting rearranged collection of blocks may then be presented to a block device consumer such as an application or a file system as one or more aggregated devices with the appearance of one or more basic disk drives. That is the more complex structure resulting from rearranging blocks and adding functionality may be presented as if it were one or more simple arrays of blocks or logical block devices. It is noted that a virtualized block device may also be referred to as a logical block device and that in some embodiments multiple layers of virtualization may be implemented within a given logical volume. That is one or more block devices may be mapped into a particular virtualized block device which may be in turn mapped into still another virtualized block device allowing complex storage functions to be implemented with simple block devices.

In various embodiments block virtualization can support the creation of virtualized block devices implementing numerous different types of storage functions. For example in one embodiment a virtualized block device may implement device striping where data blocks may be distributed among multiple physical or logical block devices and or device spanning in which multiple physical or logical block devices may be joined to appear as a single large logical block device. In some embodiments virtualized block devices may provide mirroring and other forms of redundant data storage the ability to create a snapshot or static image of a particular block device at a point in time and or the ability to replicate data blocks among storage systems connected through a network such as a local area network LAN or a wide area network WAN for example. Additionally in some embodiments virtualized block devices may implement certain performance optimizations such as load distribution for example and or various capabilities for online reorganization of virtual device structure such as online data migration between devices. Block virtualization may provide any or all of these capabilities in a fashion transparent to virtualized block device consumers. That is virtualized block devices may appear as generic storage devices to consumers such as file systems and applications.

A volume server e.g. virtual device server may provide functions such as configuration management of virtualized block devices and distributed coordination of block device virtualization. For example in one embodiment virtual device server may be aware of the type and quantity of physical storage devices such as physical block devices that are available within a storage system. In various embodiments the virtualization functions provided by a virtual device server may be provided at different levels in the storage hierarchy between a virtual device consumer and physical storage devices .

For example in one embodiment virtual device consumers may be provided with a description of a virtualized block device and may be configured to directly access constituent block devices comprising the virtualized device. Such virtualization may also be referred to as host based or client based virtualization. In response to a request to configure a virtual block device for example according to a desired set of virtualization features virtual device server may be configured to build a volume description that describes how a collection of storage objects compliant with the desired features maps to underlying physical block devices. The volume description identifying a particular volume e.g. virtual storage device may be distributed to one or more virtual device consumers . Each virtual device consumer may be configured to interact with virtual device server for certain functions for example management or administrative functions. For typical block read and write activity each virtual device consumer may be configured to interact directly with various storage servers e.g. via device management software according to the volume description distributed by virtual device server .

Distribution of a virtualized block device as a volume to one or more virtual device consumers may also be referred to as distributed block virtualization. In some embodiments after virtual device server has distributed a volume description of a given virtual block device to a given virtual device consumer as a particular volume the given virtual device consumer may interact with that particular volume to read and write blocks without further involvement on the part of virtual device server as described above. That is the given virtual device consumer may use the structure of the particular volume to transform logical I O requests into physical I O requests directed to specific physical storage devices such as devices .

In some embodiments details of block virtualization may not be directly available to individual virtual device . In some such embodiments the virtualization function of virtual device server may be implemented in a device or layer of abstraction in between virtual device clients and physical storage devices such as a switch or virtualization appliance. Such virtualization may also be referred to as switch based or appliance based virtualization.

Additionally in some embodiments multiple layers of virtualization may be employed for example at the host level as well as at the switch or appliance level. In such embodiments some aspects of virtualization may be visible to virtual device consumers as in the host based model while some aspects may be implemented transparently by an intermediate device as in the switch based model. Further in some multilayer embodiments the virtualization details of one block device e.g. one volume may be fully defined to a virtual device consumer i.e. without further virtualization at the switch layer while the virtualization details of another block device e.g. another volume may be partially or entirely transparent to virtual device consumer .

In some embodiments a virtual device server may be configured to distribute all defined volumes to each virtual device consumer present within a system. Such embodiments may be referred to as symmetric distributed block virtualization systems. In other embodiments specific volumes may be distributed only to respective virtual device consumers such that at least one volume is not common to two virtual device consumers . Such embodiments may be referred to as asymmetric distributed block virtualization systems.

As described above various virtualization features such as striping mirroring replication and snapshot capabilities may be available for a given storage device or volume . In some embodiments a technique known as copy on write COW may be employed to provide various virtualization functions for example to create a snapshot or point in time copy of a volume. The use of a COW technique may result in the requirement for special handling of updates described previously i.e. where a previous version of a data block targeted for an update may be required at a second storage consumer as described below in further detail.

Generally speaking a snapshot of a data object contains an image of the data as of a particular point in time. Snapshots may be used for a variety of reasons including backups such as off host backups and offline analysis of data e.g. by decision support systems or data mining applications. One method of creating a snapshot of a logical volume utilizes mirroring functionality provided by a volume server such as virtual device server . Using this method multiple mirrors of the logical volume may first be created for example by attaching and synchronizing one or more mirrors at an existing volume so that the entire data set of the logical volume is replicated at each mirror. A snapshot may then be created by breaking off one mirror in a controlled manner. That is applications accessing the volume may be quiesced i.e. prevented from issuing new I O requests in flight I O operations may be completed one mirror may be detached from the parent logical volume and placed in a separate volume unaffected by future I Os to the parent volume and finally applications may be allowed to resume I O on the parent volume. However a mirror attach and synchronization for a large volume e.g. a volume containing a terabyte of data may take many hours or even days.

Instead of copying the entire data set of a logical volume a COW technique may be used to more efficiently create or update a lightweight snapshot of logical volume . If a data block on the parent or source volume is to be updated after the COW snapshot is taken the previous version of the data block is copied over to the snapshot prior to being overwritten and the block number or identifier is stored as part of snapshot metadata. This operation may be termed a COW push operation. Subsequent updates on the COW pushed data block by storage consumers are handled normally. If a snapshot data block is to be read for example by a data mining application the old version must be provided. If the block number of the requested block has been stored in the snapshot metadata it is identified as a COW pushed block and the block may be read from the snapshot itself. If the block number is not stored in the snapshot metadata the read request is satisfied by using the block from the parent volume rather than the snapshot.

In addition to their use in creating snapshots COW techniques may also be used during snapshot updates. If a snapshot S is created from a parent volume at time T a snapshot update may be performed at a later time T to re synchronize S with volume such that changes made to the data of volume between time T and a later time T are included within updated snapshot S. COW techniques similar to those described above for volume snapshot creations and updates may also be used to create and update snapshots of file systems or other storage objects such as disk arrays.

Any suitable storage device with a front end write back cache where such a COW technique is used may employ the special handling of updates described in conjunction with the descriptions of and above. Thus instead of performing a COW push synchronously with an update requested at a parent storage device the old data block may be read and or pushed to the snapshot device which may be considered the storage processing node asynchronously allowing the an updating storage consumer to proceed quickly to its next operation without waiting for the COW push to complete. Such asynchronous COW pushes may help reduce update latency substantially especially in storage environments where the snapshot device may be remote with respect to the parent storage device e.g. where one or more messages over a relatively slow network such as a WAN may be required to transfer the old data block to the snapshot device .

In addition to storage devices employing COW techniques other classes of storage devices may also benefit from asynchronous reads of old data blocks in the manner described above. For example storage devices configured to support high availability using redundancy e.g. using parity or any other suitable redundancy computations may utilize such asynchronous reads. is a block diagram illustrating one embodiment of a storage system employing redundancy in a distributed virtual storage device. The storage system may include a first storage server A and a second storage server B where each storage server may include a corresponding write back cache e.g. write back cache A for storage server A and write back cache B for storage server B and a corresponding backing storage device . In addition each storage server may incorporate corresponding device management software e.g. device management software A at storage server A and device management software B at storage server B. As described below the device management software B may represent a storage processing node for blocks of distributed virtual device stored at storage server A.

A virtual device server may be configured to aggregate storage from the first and second storage servers into a virtual storage device such that an update to a first data block of the virtual storage device requires a redundancy computation using a previous version of data block stored at the first storage server A and a previous version of a second block stored at the second storage server B. In the illustrated embodiment the second block may include the redundancy information i.e. the results of the redundancy computation may be stored within the second block . The redundancy computation may also require other inputs as well as the previous versions of the first and second data blocks such as a new version of the first data block for example as described below in conjunction with the descriptions of and in one specific embodiment the redundancy computation may include a series of XOR logical exclusive OR operations on the previous version of data block the new i.e. updated version of data block and the old version of a parity block .

Virtual device server may be configured to make virtual storage device accessible to virtual device consumer e.g. to allow virtual device consumer to perform I O operations on virtual storage device . is a flow diagram illustrating aspects of the operation of virtual storage consumer and storage servers A and B during an update of a data block according to one embodiment. As shown in block of virtual device consumer may be configured to send a first copy of the updated data block to device management software A at first storage server A and a second copy of the updated data block to device management software B at second storage server B. In order to compute the new redundancy information corresponding to block a previous version of the data block the new version of the data block and the previous version of the data block may be required at second storage server B. It is noted that in one embodiment the contents of the updated data block may be forwarded from storage server A to storage server B e.g. instead of being sent to both storage servers by the virtual device consumer . In another embodiment storage server B may send a request for the contents of the updated data block to storage server A and storage server A may send the contents to storage server B in response to the request.

Device management software A at storage server A may store the new version of data block in local write back cache A block and may send a first update completion notification to virtual storage consumer block . Similarly device management software B at storage server B may store the new version of data block in write back cache B block and send a second update completion notification to volume consumer . Volume consumer may receive the first and second update completion notifications block indicating that copies of the new version of data block have been stored at two different storage servers. If a failure occurs at either storage server after the two copies of new data block have both been stored at a respective storage server cache a recovery process may be able to obtain the new data block from the other storage server e.g. if a failure occurs at storage server A the new version of data block may be obtained from storage server B . Thus having received the two update completion notifications collectively indicating that the new version of data block has been redundantly stored volume consumer may proceed to its next operation block .

Some time after sending the first update completion notification device management software A may send the previous version of data block to device management software B at storage server B block . After receiving the previous version of data block block device management software B at storage server B may have all the data needed to compute the new redundancy information. Thus device management software B may read the previous version of data block block perform the desired redundancy computation block and write the new version of the parity block to local backing storage device B. Such a separation of redundancy computation steps into asynchronous operations may result in a substantial reduction in update latency as viewed by virtual device consumer . In some embodiments storage server A may wait to receive an acknowledgement from storage server B that the desired redundancy computation has completed and has been safely persisted i.e. either to a non volatile cache e.g. in embodiments where write back cache B is a non volatile cache or to backing storage prior to replacing the previous version of data block block . By waiting for the acknowledgement storage server A may ensure that the previous version remains accessible in case it is needed for recovery e.g. if a recomputation of the redundancy value is needed as a result of a system crash at the second storage server before the redundancy computation results are made persistent.

In one specific embodiment illustrated in a block device may be configured as a distributed virtual RAID 5 Redundant Array of Inexpensive Disks Level 5 volume that is the redundancy computation described above may be a RAID parity computation. A virtual device server such as volume server may aggregate storage within backing storage devices A E spread across multiple storage servers A E and present the aggregated storage to a volume consumer as a single virtual RAID 5 device i.e. a device implementing the RID 5 architecture where logically consecutive data units are interleaved across multiple physical storage devices and parity data units are distributed among all the physical storage devices . For example as shown the data of device may consist of equally sized data stripe units A A A and A forming a data stripe A logically followed by a data stripe B containing data stripe units B B B B followed by a data stripe C and so on. For each data stripe consisting of four data stripe units a parity stripe unit of the same size as one data stripe unit may be derived and stored for example parity stripe unit P A may be obtained by performing logical exclusive OR XOR operations on corresponding bits of data stripe units A A A and A. Thus bit of the parity stripe unit P A may be obtained by performing an XOR operation between bit of A and bit of A to yield a result bit R followed by an XOR of R with bit of A to yield a second result bit R followed by an XOR of R with bit of A.

As shown in each set of five stripe units four data stripe units and a corresponding parity stripe unit may be distributed across the five backing storage devices according to the RAID 5 architecture. Thus for data stripe A data stripe units A A may be placed at backing storage device A D and parity stripe unit P A at backing storage device E while for data stripe B data stripe units B B may be placed at backing storage device A C parity stripe unit P B at backing storage device D and data stripe unit B at backing storage device E. Similarly the data stripe units and parity stripe units for each successive data stripe may be spread over each of the five backing storage devices shown. Each storage server may incorporate a corresponding non volatile write back cache i.e. cache A at storage server A cache B at storage server B etc. and device management software to handle updates as described below. The configuration of a virtual distributed RAID 5 device in the manner described above may provide data availability in the event of a failure of a storage server in contrast a hardware RAID 5 device may become unavailable if its hosting storage server fails .

An update to a data stripe unit such as A requested by an application at volume consumer may require that a new parity value for data stripe A be calculated and stored at the appropriate physical block device e.g. E in the case of A . An XOR of the old and new versions of data stripe unit A followed by an XOR of the result of that operation with the old parity stripe unit P A may yield the desired new parity stripe unit. Using this technique two stripe unit reads old data old parity and two stripe unit writes new data new parity may be required to update a data stripe unit and its associated parity. This may be more efficient than reading all old data stripe units A A performing the required XORs on the new data stripe unit A and the old data stripe units and writing the new data stripe unit and the new parity stripe unit a total of five stripe units read or written and three stripe unit XOR operations for one data stripe update in the depicted configuration .

Device management software A at storage server A may store the new version in local non volatile write back cache A block and may send a data update completion notification to volume consumer block . Similarly device management software B at parity storage server B may store the new version of D in non volatile write back cache B block and send a parity update completion notification to volume consumer . Volume consumer may receive completion notifications for both the data update and the parity update block and may then proceed to its next operation block . Some time after sending the update completion notification device management software A may send the previous version of data block D to device management software B at parity storage server B block . After receiving the previous version of D block device management software B at parity storage server B may have all the data needed to compute the new parity. Thus device management software B may read the previous version of the parity block P D block compute the new version of the parity block block and write the new version of the parity block to local backing storage device B. After an acknowledgment indicating that the new version of the parity block has been made persistent is received at storage server A the new version of D may be written to local backing storage device A block . In some embodiments the acknowledgment may be sent as soon as the new version of the parity block has been written to a non volatile cache instead of being sent after the new version has been written to backing storage.

It is noted that various extensions of the above technique may be utilized in different embodiments to handle cases such as update requests that span multiple consecutive data blocks overwrites e.g. multiple updates of different parts of the same data block in close time proximity and the like. In one embodiment for example in response to an update request that spans multiple data blocks of the same data stripe e.g. data blocks A A and A of stripe A device management software at a parity storage server e.g. B may be configured to read the previous version of the parity block just once perform the XOR operations on all the updated blocks and write the updated parity just once. In another embodiment instead of sending the previous version of a data block to a parity storage server B device management software at the data storage server A may be configured to perform XOR operations on the previous version and the new version or versions in the case of overwrites and send the result of the XOR operations to the parity storage server B. In one specific embodiment in order to ensure that a volume consumer does not inadvertently corrupt a parity block by sending inconsistent versions of the data block to the data storage server A and the parity storage server B volume consumer may be configured to send the data block only to the data storage server A. Device management software at data storage server A may be configured to forward a copy of the updated data block to the parity storage server B in such an embodiment.

Various other distributed virtual RAID devices may also be provided in a block virtualization environment in accordance with different RAID architectures in different embodiments e.g. RAID 4. It is also noted that the data and parity of a distributed virtual RAID device may be distributed across more or fewer physical storage devices such as backing storage devices than depicted in as long as more than two backing storage devices are used for a RAID 4 or RAID 5 device and that multiple backing storage devices may be managed within a single storage server in some embodiments. In one embodiment for example multiple logical columns of a RAID device such as a first column including data stripes A B C and D from and a second column including stripes A B C and P D may be stored at a single storage server e.g. using two different backing storage devices . In other embodiments a virtual distributed RAID device may employ multiple layers of virtualization i.e. instead of utilizing physical block devices virtual block devices may be employed to store data and parity stripe units and the virtual block devices may each be aggregated from storage within other virtual or physical block devices. The technique of asynchronous provision of a previous version of a data block for parity computation may be employed in various embodiments of parity based or calculated redundancy storage architectures.

As noted earlier in contrast to block virtualization environments in some embodiments object based virtualization may be employed that is a virtual object device server may be configured to organize storage within storage devices as higher level logical objects such as files instead of using the block based interface described above. is a block diagram illustrating an embodiment of an object based storage virtualization system that conforms generally to the system illustrated in . In an object virtualization environment virtual storage may be named managed and made accessible using any desired base object as implemented by virtual object device server such as a file object or a database table object. Thus in one embodiment an object storage consumer A may be presented with a virtual storage device consisting of a collection of named files and may perform file based operations such as reads from a file writes to a file increasing the size of a file truncating a file etc. directly on the virtual storage device. Object based virtualization may thus allow the offloading of functionality traditionally performed at a host computer system such as the translation of a file name and offset within a file to a block device address to a storage device such as an object storage device or OSD e.g. devices A and B in that may be optimized to perform the needed storage operations freeing up resources at the host computers. In addition once virtual objects have been created and configured virtual object device server may distribute metadata on the virtual objects to object storage consumers allowing the object storage consumers to perform input output I O operations on the virtual objects without further interaction with virtual object device server . Each OSD may include a corresponding OSD processor an OSD RAM and a write back cache . In the embodiment illustrated OSD A includes a collection of physical disks A B and OSD B includes physical disks C D. Device management software may be executable via instructions stored within OSD RAM at each OSD.

In some embodiments a virtual object device may support snapshot operations other COW operations or calculated redundancy mechanisms similar to those described earlier for block virtualization environments that may benefit from asynchronous reads of previous versions of updated data objects. Device management software incorporated within a given OSD may provide the desired previous versions to a storage processing node such as second storage consumer B virtual object device server or device management software incorporated within a different OSD.

It is noted that various combinations of the different virtualization techniques described above may be employed within different embodiments of system . For example in one embodiment a single storage environment may employ host based volume managers virtualization switches object storage devices and other virtualization mechanisms one or more of which may utilize the techniques for asynchronous provision of a previous version of a data block or object described above. In general a virtual device server for either block based virtualization or object based virtualization may be any device or software module capable of providing virtualization functionality as described above such as a server computer system including one or more processors and one or more system memories.

While the technique of asynchronously providing a previous version of a data block to a second storage consumer has been described above in the contexts of parity computation and snapshot management its use is not limited to those two contexts. The same technique or variants thereof may be employed in other embodiments of storage environments where the following conditions hold first that a write back cache is available and second that there is a requirement for an operation of a previous version of the data block being updated. For example some storage environments may be configured to provide a complete auditable history of the changes made to a given set of data objects or blocks e.g. to log each version of a data block as it changes over time to support fine grained recovery mechanisms. Such change histories may be maintained while keeping update latencies acceptable using the techniques described above.

In some embodiments an indication as to whether a particular update request requires special handling i.e. whether an asynchronous provision of the previous version of the data is required at a storage processing node may be provided to device management software . For example device management software may be configured to handle I O requests targeted at any of several storage devices where some of the storage devices may be involved in a COW operation while others are not. In one embodiment device management software may provide a set of specific application programming interfaces APIs to be used by storage consumers to indicate that the special handling is required for a given update. In one such embodiment a storage consumer may utilize one of the specific APIs to request a first update operation on a storage device that requires special handling and may utilize a normal or standard I O API to request a second update operation that does not require special handling. In another embodiment an indication that special handling may be required for a given update may be provided via a flag a directive or a tag passed to device management software along with the update request for example as part of an enhanced storage access protocol supported by storage servers . At least a portion of device management software may also be incorporated within a virtual device server in embodiments employing virtualization.

The scheduling of the provision of the previous version of a data block to storage processing node may be triggered in a variety of ways in different embodiments. A cached version of an updated data block may be marked or tagged with an indication that it requires special handling before its previous version is overwritten. That is in some embodiments write back cache may support a mechanism to distinguish data blocks that require the asynchronous provision of a previous version from those that do not require such special handling. A tag associated with the dirty data block may include an identification of the destination i.e. storage processing node to which the previous version may be sent.

In many storage environments employing write back caches a configuration parameter e.g. a parameter called max dirty percent may be used to specify a maximum acceptable proportion of dirty data blocks within write back cache . A dirty data block may be defined as an updated data block in the cache that has not yet been written to the backing storage device. Once the data block has been written to the backing storage device the copy that may remain in the cache may be considered a clean data block. When the proportion of dirty data blocks within write back cache reaches the specified maximum device management software may be configured to write dirty data blocks to the backing storage device until the proportion of dirty data blocks reaches a desired value. The desired value to which the proportion of dirty data blocks may be reduced may be specified by a second configuration parameter e.g. dirty percent after clean . For example if max dirty percent is set to a value of 80 and dirty percent after clean is set to 40 device management software may write enough dirty data blocks to a backing storage device to reduce the percentage of dirty data blocks from 80 to 40 . The dirty data blocks that require a read of the previous version and a provisioning of the previous version to a storage processing node may be tagged as noted above allowing device management software to perform the required read and provisioning operations. In another embodiment the dirty data blocks that require asynchronous reads may be handled independently from the other dirty data blocks e.g. a configuration parameter specifying the maximum acceptable proportion of dirty blocks that require asynchronous reads e.g. max async dirty writes may be specified. Device management software may schedule a set of write operations to a backing storage device when the proportion or number of special dirty blocks i.e. those requiring asynchronous reads of their previous versions reaches the specified threshold level even if the overall percentage of dirty data blocks remains acceptable.

The storage processing node that may require the previous version of an updated data block may explicitly request the previous version in some embodiments. For example in the distributed virtual RAID 5 device described above space taken up by the new version of data stripe unit A sent to storage server B by volume consumer within write back cache B may have to be given up as the proportion of dirty blocks within cache B rises. If write back cache B at storage server B is close to reaching its max dirty percent threshold therefore device management software B may send a request for the previous version of a data stripe unit A to device management software A to allow the required parity computation to be completed before the new version of the data block is overwritten in cache B. Similarly a storage processing node that requires a previous version of a data block for a snapshot update operation may send a request to device management software requesting that the previous version be provided.

It is noted that in providing the previous version of an updated data block to storage processing node in some embodiments device management software may be configured to search for a copy of the previous version within one or more caches such as a read cache separate from write back cache and to read the previous version from a backing storage device only if a copy is not found in a cache. For example if the previous version of the data block is present in write back cache or in a separate read cache when the updated request arrives from storage processing node in some embodiments the previous version may be tagged as invalid for example but may still remain in the cache until it is overwritten. If the tagged previous version remains in the cache at the time when it is required at storage processing node device management software may obtain it efficiently from the cache instead of performing a read operation on a backing storage device.

A storage consumer may be any type of application or device external to a given storage device and its front end write back cache that is capable of interacting with the given storage device for data storage and retrieval. Thus for example firmware used within a hardware RAID device would not be considered a storage consumer for the hardware RAID device. In contrast both a database application requesting an update on the hardware RAID device and a replication application performing a snapshot update on a logical volume whose data is stored at the hardware device would be considered storage consumers of the hardware RAID device. In one embodiment a storage consumer may be an application hosted at a computer host connected to a storage device via a network where the network may form part of a LAN a WAN and or a storage area network. A storage consumer may also be hosted within a virtualization switch that may provide additional layers of virtualization on top of a logical volume . In other embodiments a storage consumer may be an embedded system configured to use application specific integrated circuit ASIC or field programmable gate array FPGA technology to execute operations whereby a given storage device may be accessed. Numerous other configurations of storage consumer are possible and contemplated.

In different embodiments device management software storage consumers and storage processing nodes may be provided to a computer system using a variety of computer accessible media including electronic media e.g. flash memory magnetic media such as RAM e.g. SDRAM RDRAM SRAM etc. optical storage media such as CD ROM etc. as well as transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link. It is also noted that a write back cache may be implemented using a variety of media such as various forms of RAM e.g. SDRAM RDRAM SRAM etc. .

Although the embodiments above have been described in considerable detail numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.

