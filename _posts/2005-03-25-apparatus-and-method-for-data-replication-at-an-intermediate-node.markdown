---

title: Apparatus and method for data replication at an intermediate node
abstract: An embodiment of the invention provides an apparatus that includes a primary node, a secondary node, and an intermediate node. Data is synchronously copied from the primary node to the intermediate node, and data is asynchronously copied from the intermediate node to the secondary node. The data stored in the intermediate node is a subset of the data set that is stored in the primary node. Therefore, an embodiment of the invention advantageously provides a multiple hop data replication method that allows data recovery if disaster occurs at the location of the primary node. Additionally, an embodiment of the invention advantageously provides a tolerable latency value from the time that a client device sends a write request to the primary node to the time that an acknowledgement message is received by the client device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07631021&OS=07631021&RS=07631021
owner: Netapp, Inc.
number: 07631021
owner_city: Sunnyvale
owner_country: US
publication_date: 20050325
---
Embodiments of the invention relate generally to storage networks and more particularly to an apparatus and method for data replication at an intermediate node.

Storage devices are employed to store data that are accessed by computer systems. Examples of storage devices include volatile and non volatile memory floppy drives hard disk drives tape drives optical drives or other types of storage units. A storage device may be locally attached to an input output I O channel of a computer. For example a hard disk drive may be connected to a computer s disk controller.

A storage device may also be accessible over a network. Examples of such a storage device include network attached storage NAS and storage area network SAN devices. A storage device may be a single stand alone component or may include a system of storage devices such as in the case of Redundant Array Of Inexpensive Disks RAID groups and some Direct Access Storage Devices DASD . Generally disk storage is typically implemented as one or more storage volumes i.e. data volumes that are formed by physical storage disks and define an overall logical arrangement of the storage space. Each volume is typically associated with its own file system. The storage disks within a volume may be typically organized as one or more groups of RAID.

For mission critical applications requiring high availability of stored data various techniques for enhancing data reliability are typically employed. One such technique is to provide a mirror for each storage device. In a mirror arrangement data are written to at least two storage devices. Thus identical data may be read from either of the two storage devices so long as the two devices are operational and contain the same data. In other words either of the two storage devices may process the read requests so long as the two storage devices are in synchronization. Synchronous mirroring applications are typically used to achieve data synchronization between the two storage devices.

Common synchronous mirroring applications require client transactions to be committed locally at a primary or first site and to be committed remotely at a backup or secondary site before a transaction is deemed to be committed and acknowledged to a client device. In other words client transactions are usually performed on the primary site and the backup site. However high latency is inherent in these applications that use synchronous mirroring applications. Relaying the client transaction from the primary site to the backup site and sending an acknowledgement of the transaction completion from the backup site to the primary site and then to the client device will incur a latency value equal to the round trip transmission time RTT concerning the transaction. This latency value is directly proportional to the distance between the primary site and backup site with the two sites being related by the speed of light which is an invariant . In implementations where the primary site and the backup site are separated by a far distance this latency value adds a significant delay to the client response time and this delay can greatly degrade the application performance.

Therefore the current technology is limited in its capabilities and suffers from at least the above constraints and deficiencies.

An embodiment of the invention advantageously solves the high latency problem that is inherent in most current disaster recovery solutions. An embodiment of the invention may be used in for example synchronous mirroring applications or and applications that replicate duplicate or copy user data to a remote site.

An embodiment of the invention provides an apparatus system that includes a primary node a secondary node and an intermediate node. Data is synchronously copied from the primary node to the intermediate node and data is asynchronously copied from the intermediate node to the secondary node. The data stored in the intermediate node is a subset of the data set that is stored in the primary node. Therefore an embodiment of the invention advantageously provides a multiple hop data replication method that allows data to be replicated copied or duplicated from a primary node to an intermediate node and then replicated to a secondary node and allows data recovery if disaster occurs at the location of the primary node. Additionally an embodiment of the invention advantageously provides a tolerable latency value from the time that a client device sends a write request to the primary node to the time that an acknowledgement message is received by the client device.

As one of various options the transactions via a link between the primary node and the secondary node may be for example compressed or encrypted end end in accordance with standard compression or encryption techniques.

These and other features of an embodiment of the present invention will be readily apparent to persons of ordinary skill in the art upon reading the entirety of this disclosure which includes the accompanying drawings and claims.

In the description herein numerous specific details are provided such as examples of components and or methods to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize however that an embodiment of the invention can be practiced without one or more of the specific details or with other apparatus systems methods components materials parts and or the like. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of embodiments of the invention.

A client device can send write requests in order to write data into the primary node . The data is typically sent as a data stream from the client device to the primary node . The client device can also send read requests in order to read data from the primary node . The data is typically sent as a data stream from the primary node to the client device . As an example the client device may be without limitation a personal computer server mini computer mainframe computer portable computer such as a laptop or notebook computer workstation wireless terminal personal digital assistant cellular phone or another type of processing device. In accordance with an embodiment of the invention when the client device sends the data to be written to the primary node the primary node will receive and store the data . As an example the data creates a file or folder that is stored in the primary node or updates a file or folder . Therefore the data becomes part of the data set in the primary node . The primary node will also send a copy of the data to the intermediate node . After the intermediate node receives the data copy the intermediate node will store or buffer the data copy in a buffer space as discussed in further detail below. When the data copy is stored in the buffer space the data copy is now stored in a storage device in the intermediate node . After a copy of the data is stored in the buffer space the intermediate node will then send an acknowledgment message in order to indicate or confirm that the data copy has been successfully stored in the intermediate node . Therefore the data copy is not required to reach the secondary node in order to send an acknowledgment message and as a result the use of the intermediate node in the system avoids the latency effects of the acknowledgment messages in the previous approaches. The primary node will then receive the acknowledgment message . Upon receipt of acknowledgement by the primary node from the intermediate node the primary node acknowledges the completion of the transaction to the client device i.e. acknowledges the buffering of the data into the intermediate node . Typically this acknowledgement of the completion of the transaction is performed by forwarding the acknowledgment message from the primary node to the client device although other methods may be used by the primary node to acknowledge the completion of the transaction. When the client device receives the acknowledgement of the transaction completion from the primary node the client device can proceed onwards to perform known operations. An example of an operation when the client device proceeds onwards after receiving the transaction acknowledgement is as follows. Some Network File System NFS clients will typically wait for an acknowledgement in response to a new file creation request to a Filer e.g. primary node before issuing writes against such a file. Typically users of client computers are unaware of the fact that there is a backup copy. The additional latency hit for the client device is now a function of the distance between the primary node and the intermediate node .

In an embodiment of the invention the intermediate node also forwards the copy of the data to the secondary node so that the secondary node can store a backup copy of the data . Therefore the intermediate node functions as a device that stores data and forwards data and as an intermediate hop device between the primary node and the secondary node . The intermediate node typically sends the copy of data to the secondary node during a background operation of the intermediate node or during a later time although the intermediate node can send the data copy to the secondary node immediately or in real time or by any other suitable known manner. There may also be a time delay in the transmission of the copy of data from the intermediate node to the secondary node . Therefore data is synchronously copied from the primary node to the intermediate node and data is asynchronously copied from the intermediate node to the secondary node . As an example the data copy creates a file or folder in the secondary node where the file or folder is a copy of the file or folder in the primary node . As another example the data copy updates the file or folder in the secondary node . The intermediate node and secondary node provide a multiple hop configuration and solution for replicating data that is useful in disaster recovery applications. In other words the data is written by the client device and the data is copied replicated into the intermediate node as data . The data is then copied replicated into the secondary node .

Note that the intermediate node stores in the buffer space data which is a less than a full copy of the files or data set which are stored in the primary node . All that needs to be stored is the data which has been received by the intermediate node and not yet sent to the secondary node . In other words the data in the buffer space is a subset of the data in the data set . As an example the data set may have a total size of approximately 1000 gigabytes or more while the data may only be approximately 3 gigabytes of the data in the data set . As discussed below the size of the data in the buffer space may be at other values. At a minimum the data buffered in the buffer space is the data that is written by the client device into the primary node . Note that the data will be a subset of the data set when the client device writes the data into the primary node . Note also that the entire data in the data set is not stored in the buffer space at any particular time due to the limited size of the buffer space . As an end result of the above process the files or data set are eventually copied as data set copy in the secondary node .

After the data copy is stored in the secondary node the second node sends a confirmation message to the intermediate node where the confirmation message indicates that the data has been stored in the secondary node . After receiving the confirmation message and in response to the confirmation message the intermediate node typically deletes the data in the buffer space in order to free memory space in the buffer space in one embodiment of the invention.

Two factors are typically used for example in determining the distance between the primary node and the intermediate node . First the intermediate node is typically located outside of a crater zone for purposes of disaster recovery consideration. In other words the intermediate node is typically located sufficiently far away from the primary node so that the intermediate node is not affected by any disaster that affects the primary node . Second the distance between the primary node and the intermediate node is such that the latency hit for the clients is tolerable i.e. does not greatly degrade application performance . In other words the distance provides a tolerable latency value from the time that the client device sends a write request to the primary node to the time that the acknowledgement message is received by the client device .

In the multiple hop solution provided by the system as long as the disaster does not encompass both the primary node and intermediate node a synchronous copy of the user data is available at the secondary node backup site eventually since data is asynchronously copied from the intermediate node to the secondary node . The synchronous copy of the user data at the secondary node is the end result of the user data transmitting draining from the intermediate node to the secondary node . This scheme is very beneficial because it separates the location of the primary node and secondary node from the latency and disaster tolerance requirements of synchronous mirroring. The primary node and secondary node are typically for example data centers. The primary node and secondary node are often dictated by the different geographical areas where an entity e.g. corporation may have offices as an example.

The network connectivity between the primary node and the intermediate node and between the intermediate node and the secondary node may use any suitable types of standard network connections and protocols such as for example TCP IP Transmission Control Protocol Internet Protocol and or Ethernet or Fibre Channel.

Typically there is also an alternate link connectivity between the primary node and the secondary node . The link can be used for the purpose of direct communication between the primary node and the secondary node . Direct communication between the primary node and the secondary node is useful when the intermediate node is down or is unavailable for communication in the network or system .

An issue with other or conventional multi hop solutions for data replication on the other hand is that these types of solutions usually require a full copy of the data at an intermediate node. As the user data keeps accumulating in the buffer space this issue may affect system performance. Furthermore the intermediate node is typically not a full fledged data center. One example of a conventional multi hop solution is provided by NETWORK APPLIANCE INCORPORATED and utilizes the SYNC SNAPMIRROR SLIDERULER and ASYNC SNAPMIRROR products which are commercially available from NETWORK APPLIANCE INC.

The conventional multi hop solutions typically require at least three 3 copies of the same data i.e. data set and this configuration leads to additional expense required management of an intermediate node that stores the full copy of the data set and result in idle data that is stored in the intermediate node and that is not particularly useful. In contrast an embodiment of the invention provides an intermediate node configuration that provides various advantages as described below.

The intermediate node has a primary purpose of being able to facilitate a disaster recovery implementation. Managing a large amount of data at this intermediate node which is often a remote site may be a problem in some situations particularly if the stored data increases in size. Also the data is also mostly sitting idle at the intermediate node and is typically not useful to a customer. In order to solve these issues the system advantageously eliminates the requirement of storing a full copy of the data set at the intermediate node and therefore eliminates or reduces the attendant cost and administrative problems with storing a full copy of the data set while providing all the benefits of a multi hop disaster recovery solution and at a lower cost. In an embodiment of the invention the intermediate node is populated with a small amount of stable storage that forms the buffer space . Typically this stable storage may be formed by for example NVRAM non volatile random access memory and or magnetic disk drives and or other suitable data storage devices. The buffered data that is stored in the buffer space is less or only a portion of the data set in the primary node . A buffered data is flushed from the intermediate node and replaced with new data if there is not sufficient free memory space in the buffer space for storing the new data . The network of system may for example be one of various types of communication networks such as wide area networks local area networks the Internet or another type of network. Other nodes in the system such as for example gateways routers bridges switches firewalls and or the like are not depicted in for clarity of illustration. Standard components are also not shown in the components in in order to focus on the functionalities in the embodiments of the invention. The components shown in are also not necessarily drawn to scale for clarity of illustration.

In block the secondary node sends a confirmation message to the intermediate node where the confirmation message indicates that the data has been stored in the secondary node . In block after the intermediate node receives the confirmation message and in response to the confirmation message the intermediate node deletes the data in the intermediate node.

In an embodiment of the invention the storage operating system further includes an intermediate module that processes and manages the incoming data stream e.g. data from the primary node manages the buffer space buffers the incoming data stream into the buffer space and forwards the data from the buffer space to the secondary node .

Typically the intermediate module straddles the network layer and the file system layer in the storage operating system . The data that is received from the primary node is placed in the file system buffered in the buffer space in the storage system and then eventually forwarded to the secondary node . By buffering the data that is received from the primary node and then forwarding the buffered data to the secondary node the intermediate module reduces the latency problem of previous approaches.

The storage system provides the memory space for the buffer space . The storage system may include one or more storage devices.

Components of the intermediate node may be implemented in hardware software and or firmware. For example the intermediate node may be a computer having one or more processors running computer readable program code of the storage operating system in memory. Software components of the intermediate node may be distributed on computer readable storage media e.g. CD ROMS tapes disks ZIP drive and or another type of storage media or transmitted over wired or wireless link by a device connected to the intermediate node . Note that other standard hardware e.g. ports memories processors and other hardware standard software and standard firmware components that permit the intermediate node and the nodes and to perform standard computing or packet processing operations are not shown in in order to focus on the details in the embodiments of the invention.

The network interface includes components for receiving and sending storage related service requests over the network to and from the primary node and to and from the secondary node . The protocol service parses and services the received requests. The protocol service can support various types of network communication protocols such as for example Network File System NFS Common Internet File System CIFS Hypertext Transfer Protocol HTTP TCP IP and or other network protocols. The network interface and protocol service will then forward a received service request to the storage operating system including the intermediate module . For example data which is transmitted by the primary node to the intermediate node will be sent by the intermediate module to the buffer space for buffering. The intermediate module selects the memory space in the buffer space for buffering the data and the file system will write the data into the buffer space . The file system manages files that are stored in storage system . In one embodiment the file system organizes files in accordance with the Write Anywhere File Layout WAFL as disclosed in the following commonly assigned disclosures which are hereby fully incorporated herein by reference U.S. Pat. No. 6 289 356 U.S. Pat. No. 5 963 962 and U.S. Pat. No. 5 819 292. However the invention is not so limited in accordance with WAFL and an embodiment of the invention may also be used with other file systems and layouts. For example an embodiment of the invention may be used with SANs or other block based systems or combined SAN NAS systems.

The file system will then read the buffered data from the buffer space and the intermediate module will then forward the data from the buffer space to the secondary node . The data is then stored in the secondary node .

The storage device manager assists the file system in managing the storage devices in the storage system . The storage device manager receives read and write commands from the file system and processes the commands by accordingly accessing the storage system .

The size of the buffer space may vary depending on how much outage is desired to be tolerated. For example if an outage of approximately 60 seconds is to be tolerated and assuming that approximately 50 megabytes of data per second can be sent to the intermediate node then the buffer space will have a size of approximately 3000 megabytes 50 megabytes second 60 seconds . Therefore the size of stable storage buffer space required at the intermediate node is not related to the size of the user data. Rather the size of the stable storage is normally a function usually of the latency jitter of communication between the intermediate node and secondary node and of the throughput of the primary node .

The primary node includes a network interface a protocol service a storage operating system and a storage system . The secondary node includes similar components but these similar components are not shown in . The storage operating system may further include a file system and a storage device manager . The storage system may include one or more storage devices. Components of the primary node may be implemented in hardware software and or firmware. For example the primary node may be a computer having one or more processors running computer readable program code of the storage operating system in memory. Software components of the primary node may be distributed on computer readable storage media e.g. CD ROMS tapes disks ZIP drive and or another type of storage media or transmitted over wired or wireless link by devices connected to the primary node .

The network interface includes components for receiving storage related service requests over a network from for example a connected device and for sending storage related service requests to connected devices. A protocol service parses and services the received requests. The protocol service can support various types of network communication protocols such as for example Network File System NFS Common Internet File System CIFS Hypertext Transfer Protocol HTTP TCP IP and or other network protocols. The network interface and protocol service will forward a received service request to the storage operating system which processes the request by reading data from the storage system in the case of a read request or by writing data to the storage system in the case of a write request. Data read from the storage system are transmitted over the network to the requesting client device . Similarly data to be written to the storage system are received over network from a client device and written by the file system into the storage system and a copy of this data is forwarded by the file system to the intermediate node .

The storage device manager manages the storage devices in the storage system . The storage device manager receives read and write commands from the file system and processes the commands by accordingly accessing the storage system . In one embodiment the storage device manager manages storage devices in accordance with RAID Redundant Array of Independent or Inexpensive Disks .

The file system manages files that are stored in storage system . In one embodiment the file system organizes files in accordance with the Write Anywhere File Layout WAFL as disclosed in the following commonly assigned disclosures U.S. Pat. No. 6 289 356 U.S. Pat. No. 5 963 962 and U.S. Pat. No. 5 819 292. However the invention is not so limited in accordance with WAFL and an embodiment of the invention may also be used with other file systems and layouts. For example an embodiment of the invention may be used with SANs or other block based systems.

The storage operating system receives data from the client device and stores the data into the storage system . The storage operating system will also forward a copy of the data to the intermediate node and the data copy is buffered in the buffer space as discussed above.

Synchronous mode refers to the steady state where the primary node and the intermediate node are in data synchrony with each other. Synchronous mode can be defined in precise terms as follows. Synchronous mode occurs when every data i.e. user data which has been committed performed at the primary node has also been acknowledged to a client by either logging the user data in stable storage i.e. buffer space at the intermediate node or the user data has been copied by the intermediate node and committed at the secondary node . Logging the user data means that the intermediate node has recorded the user data in the buffer space so that the user data in the data stream is no longer volatile when a failure event occurs.

Step 1 each user data is performed locally at the primary node and the user data is stored in the storage system in the primary node .

Step 1a step 1a is typically performed concurrently with step 1 . The user data or a copy of the user data is forwarded to the intermediate node .

Step 2 The intermediate node logs stores the user data to stable storage buffer space . This step 2 involves buffering the user data or buffering the copy of the user data .

Step 3 Upon completion of step 2 the intermediate node sends an acknowledgement to the primary node .

Step 4 Upon receipt of acknowledgement by the primary node from the intermediate node the primary node acknowledges to the client device that the user data has been stored in the intermediate node . For example the primary node can forward the acknowledgement to the client device to acknowledge the completion of the storage of the user data .

At the intermediate node further processing of the user data logged stored to stable storage takes place as follows 

Step 1 At any point after receipt of a data by the intermediate node from the primary node the data is stored in the buffer space and also forwarded by the intermediate node to the secondary node .

Step 2 The secondary node commits stores the data at the secondary node . This step 2 involves storing user data into the storage system of the secondary node .

Step 3 The secondary node then sends an acknowledgement back to the intermediate node . A storage operating system in the secondary node not shown in is configured to send the acknowledgement .

Step 4 Upon receipt of the acknowledgement by the intermediate node the intermediate node removes the data from the buffer space . Typically the data is stored in the buffer space . The intermediate module is configured to detect the acknowledgement from the secondary node and to remove the data from the buffer space .

Thus the intermediate node can be seen as performing a store and forward functionality. Typically data are logged and relayed by the intermediate node to the secondary node in the order that the data were received from the primary node .

Note also that the intermediate node typically requires very little computing power. The intermediate node does not actually perform additional processing of the user data . Instead the intermediate node merely logs the user data by buffering the user data .

The asynchronous mode is defined to be the state where the primary node has committed stored and acknowledged user data which have not been committed stored to the secondary node and have not been logged stored into the intermediate node . This operation permits data to be copied from the primary node to the secondary node when the intermediate node is not functioning and therefore provides a response by the system for tolerating a failure occurrence in the intermediate node . In an embodiment of the invention in the asynchronous mode the storage operating system in the primary node and the storage operating system in the secondary node permit the primary node and secondary node to directly communicate with each other when there is an outage in the intermediate node . By this direct communication between the nodes and the primary node can transmit its stored data set to the secondary node so that the secondary node can store the data set as a backup copy

As one of various options the transactions via a link between the primary node and the secondary node may be for example compressed and or encrypted end end in accordance with standard compression or encryption techniques.

In the asynchronous mode the primary node negotiates directly with the secondary node and manages the transition from the asynchronous mode to the synchronous mode. Once the synchronous mode is reentered the intermediate node can be automatically reintroduced into the data path between the primary node and the secondary node .

Note that an embodiment of the invention may perform an automatic split mode of operation where in a network outage then the primary node and secondary node will break their mirroring relationship so that data is temporarily not mirrored from primary node to secondary node and the primary node will continue to receive and store data from a client s . Therefore the data service to the client s is not disrupted. The primary node can then communicate directly with the secondary node so that the data set in the primary node becomes synchronous with the data set in the secondary node . Note that the automatic split mode differs from a manual split mode as follows. In the manual split mode a primary node will not accept data from a client s if the primary node is not able to successfully communicate with a secondary node.

There are two kinds of network outages which may affect this scheme provided by the system in . The first type of network outage is when the network outage occurs in a link between intermediate node and secondary node or the network outage can also occur due to the failure of the secondary node . Note that the primary node does not detect the loss of connectivity between the secondary node and the intermediate node . The intermediate node informs the primary node about this type of loss of connectivity if the loss of connectivity occurs beyond a predetermined time period .

Brief outages in the above cases can be handled by buffering at the intermediate node . The primary node keeps sending transactions to the intermediate node and the intermediate node will buffer the data from the primary node and will pretend that the above synchronous mode still applies even though connection between the intermediate node and secondary node has been lost. The intermediate node can then later send the buffered data to the secondary node after the connection between the intermediate node and secondary node has been restored or after the secondary node has been restored after failure . As long as the buffer s assigned to a given pair of primary node and secondary node does not overflow which may occur during a brief network outage data storage service can be continued without entering into an asynchronous mode . In the asynchronous mode of operation the primary node will communicate directly with the secondary node as discussed above.

To tolerate a reboot power cycle of the secondary node without reverting to the asynchronous mode of operation sufficient buffer space should be allocated at the intermediate node in order to be able to buffer data at the intermediate node for the duration of the reboot power cycle.

The second type of network outage is when the network outage occurs in a link between the primary node and intermediate node . The storage operating system in the primary node can be configured to detect this type of network outage for example if the intermediate node does not reply to a query from the primary node after a predetermined time period. Normally in this second type of network outage the primary node will enter into the asynchronous mode where the primary node will communicate directly with the secondary node as discussed below.

Machine outage occurs when at least one of the three major components in primary node intermediate node and secondary node suffers an outage. Note that an outage of the primary node will prevent any user data from the client device from being written into the primary node . Discussed below are methods for tolerating an outage in the intermediate node and an outage in the secondary node so that data that is written into the primary node is eventually copied into the secondary node . Note also that the below methods do not cover the case when the primary node is subject to an outage. When an outage occurs in the primary node the primary node is not able to receive data from a client device . Therefore there is no existing data that is to be copied from the primary node to the intermediate node and then to the secondary node in this case when the primary node has an outage.

In case when the intermediate node fails the primary node will enter into the asynchronous mode where the primary node will directly communicate with the secondary node so that the primary node will directly transmit data to the secondary node and the transmitted data is then stored in the secondary node . The storage operating system in the primary node can be configured to detect this type of machine outage.

The case when the secondary node fails is treated in a similar manner as the case where the connectivity between the intermediate node and secondary node is lost as previously discussed above. Therefore when the secondary node fails the intermediate node will continue to buffer the data received from the primary node and will transmit the buffered data to the secondary node when the secondary node is repaired as discussed above.

The intermediate module handles the network outages that may occur in the system . Note that some network outages can be tolerated without disrupting services in the system . Therefore the buffer space can continue to store data from the primary node if there is sufficient memory space in the buffer space in situations where the intermediate node is not able to forward the buffered data to the secondary node . Therefore the size of the buffer space determines the length of time that network outages can be tolerated by the intermediate node and or amount of data received by the intermediate node during a network outage. Larger sizes of the buffer space permit the intermediate node to tolerate a relatively longer length of the network outages and or receive a relatively larger amount of data during a network outage. In an embodiment of the invention when there is not sufficient memory space in the buffer space to receive new data during a network outage the intermediate module will inform the primary node to stop sending data to the intermediate node .

In block if the network outage is due to a failure between the intermediate node and the secondary node or due to a failure of the secondary node then the intermediate node will continue to store data that is received from the primary node .

In block after the failure between the intermediate node and secondary node has been eliminated or after the failure of the secondary node has been eliminated then the intermediate node will transmit the stored buffered data to the secondary node .

On the other hand in block if the network outage is due to a failure between the primary node and intermediate node or due to a failure of the intermediate node then the primary node will directly transmit the data to the secondary node .

A buffer space is associated with each pair of primary node and secondary node. For example a thread in the intermediate module would manage the buffer space and data transmitted between the pair of primary node and secondary node . A thread in the intermediate module would manage the buffer space and data transmitted between the pair of primary node and secondary node . Therefore a single device intermediate node may be able to perform the intermediary data buffering role as described above for multiple sets of primary nodes and secondary nodes .

As an example operation data that is transmitted from primary node is buffered by the thread into the buffer space . The data is a copy of a data that was written to the node by for example a client device . The thread will then forward the data to the secondary node . The secondary node will then store the data . The thread identifies the data based upon the packet header information of the data indicating that the destination address of the data is the secondary node

Data that is transmitted from primary node is buffered by the thread into the buffer space . The data is a copy of a data that was written to the node by for example a client device . The thread will then forward the data to the secondary node . The secondary node will then store the data . The thread identifies the data based upon the packet header information of the data indicating that the destination address of the data is the secondary node

One application of the embodiment in is by placing an intermediate node at for example a data center that is managed by an internet service provider. The intermediate node could be shared and used by different customers where each pair of primary node and secondary node is associated with a different customer. Quality of service for each primary node secondary node set that share the intermediate node can be achieved by limiting the network bandwidth that any primary node can transmit data. This can be achieved by use standard networking protocols. Also each primary node secondary node set is assigned to a certain buffer space stable storage . Additionally fairness in servicing requests from each primary node can be achieved by use of for example known methods e.g. round robin .

Other features discussed in the system of e.g. acknowledgement message and or confirmation message and or other features can also be included in the system in .

The intermediate node and the above data buffering method in the intermediate node may be used in for example a synchronous mirroring system although embodiments of the invention are not required to be used in synchronous mirroring systems. One example of a synchronous mirroring system is of the type known as SYNC SNAPMIRROR SLIDERULER which is commercially available from NETWORK APPLIANCE INCORPORATED. The intermediate node and data buffering method in the intermediate node may be used in other types of data storage systems that do not necessarily have the synchronous mirroring feature. Additionally an embodiment of the invention is not limited to use of any particular data format. Any suitable data format may be used for the data streams that are buffered in the intermediate node.

As noted the intermediate node can buffer updates from the primary node and then send the updates asynchronously to the secondary node . In this operation however typically the intermediate node must in some cases follow the ordering rules as follows. If the intermediate node receives two updates A and B in that order then if the secondary node has received the update B from the intermediate node then the secondary node must also have received the update A from the intermediate node . This is known as the causal ordering of the updates and is important for data consistency. The storage operating system not shown in of the secondary node performs the updates in the order that they were received from the intermediate node .

Furthermore as an option the intermediate node can batch the updates by taking advantage of overwrites. If there are two transactions A and B both of which overwrite the same location in the storage system in the primary node then the intermediate node can batch these two updates A and B and forward a single update to the secondary node with the result of the last transaction B. Therefore the single update reflects the last update B. This method can substantially reduce the traffic between the intermediate node and the secondary node . The storage operating system in the intermediate node batches the updates A and B and forwards the single update to the secondary node .

As another option one technique to provide both batching and ordering is the notion of consistency points epochs. In this scheme the storage operating system in the intermediate node defines points in time t t t etc. and all updates from for example the time point from time tto time tare batched by the storage operating system and sent from the intermediate node to the secondary node in one large transfer aggregated update . Therefore this one large transfer will include all of the updates received by the intermediate node from the primary node from time tto time t. The secondary node will take this large transfer and then apply it atomically to its storage system. Thus the secondary node will move forward atomically from time period tto time period t i.e. the data received in the secondary node will include all of the data updates received by the intermediate node from time period tto time period t . As an example assume that from the time tto time t an update A will write data denoted DATA at a first location in the buffer space an update B will write data denoted DATA at a second location in the buffer space and an update C will write data denoted DATA at a third location in the buffer space . The transfer will then forward and write the DATA DATA and DATA at different locations in the storage devices in the secondary node . This scheme can be implemented using for example WAFL SNAPSHOTS and SNAPRESTORE which are products commercially available from NETWORK APPLIANCE INC. and lets the system combine ordering and batching into one solution.

The larger the time gap between the consistency points e.g. from time period tto time period t the more efficiency of batching. But this larger time gap will require more buffer spaces at the intermediate node . In general this scheme will typically require a buffer space at least twice the space required to buffer updates during the time interval for a consistency point.

Various elements in the drawings may be implemented in hardware software firmware or a combination thereof.

The various engines software or modules discussed herein may be for example computer software firmware commands data files programs code instructions or the like and may also include suitable mechanisms.

Reference throughout this specification to one embodiment an embodiment or a specific embodiment means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the invention. Thus the appearances of the phrases in one embodiment in an embodiment or in a specific embodiment in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics may be combined in any suitable manner in one or more embodiments.

Other variations and modifications of the above described embodiments and methods are possible in light of the foregoing disclosure. Further at least some of the components of an embodiment of the invention may be implemented by using a programmed general purpose digital computer by using application specific integrated circuits programmable logic devices or field programmable gate arrays or by using a network of interconnected components and circuits. Connections may be wired wireless and the like.

It will also be appreciated that one or more of the elements depicted in the drawings figures can also be implemented in a more separated or integrated manner or even removed or rendered as inoperable in certain cases as is useful in accordance with a particular application.

It is also within the scope of an embodiment of the present invention to implement a program or code that can be stored in a machine readable medium to permit a computer to perform any of the methods described above.

Additionally the signal arrows in the drawings Figures are considered as exemplary and are not limiting unless otherwise specifically noted. Furthermore the term or as used in this disclosure is generally intended to mean and or unless otherwise indicated. Combinations of components or steps will also be considered as being noted where terminology is foreseen as rendering the ability to separate or combine is unclear.

As used in the description herein and throughout the claims that follow a an and the includes plural references unless the context clearly dictates otherwise. Also as used in the description herein and throughout the claims that follow the meaning of in includes in and on unless the context clearly dictates otherwise.

It is also noted that the various functions commands modules e.g. intermediate node variables or other parameters shown in the drawings and discussed in the text have been given particular names for purposes of identification. However the function names variable names or other parameter names are only provided as some possible examples to identify the functions variables or other parameters. Other function names variable names or parameter names may be used to identify the functions variables or parameters shown in the drawings and discussed in the text.

The above description of illustrated embodiments of the invention including what is described in the Abstract is not intended to be exhaustive or to limit the invention to the precise forms disclosed. While specific embodiments of and examples for the invention are described herein for illustrative purposes various equivalent modifications are possible within the scope of the invention as those skilled in the relevant art will recognize.

These modifications can be made to the invention in light of the above detailed description. The terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification and the claims. Rather the scope of the invention is to be determined entirely by the following claims which are to be construed in accordance with established doctrines of claim interpretation.

