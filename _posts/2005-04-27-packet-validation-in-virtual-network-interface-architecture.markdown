---

title: Packet validation in virtual network interface architecture
abstract: Roughly described, a network interface device receiving data packets from a computing device for transmission onto a network, the data packets having a certain characteristic, transmits the packet only if the sending queue has authority to send packets having that characteristic. The data packet characteristics can include transport protocol number, source and destination port numbers, source and destination IP addresses, for example. Authorizations can be programmed into the NIC by a kernel routine upon establishment of the transmit queue, based on the privilege level of the process for which the queue is being established. In this way, a user process can use an untrusted user-level protocol stack to initiate data transmission onto the network, while the NIC protects the remainder of the system or network from certain kinds of compromise.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07634584&OS=07634584&RS=07634584
owner: Solarflare Communications, Inc.
number: 07634584
owner_city: Irvine
owner_country: US
publication_date: 20050427
---
The invention relates to network interfaces and more particularly to mechanisms for validating network traffic sent or received by user level libraries in a virtual network architecture.

A typical computer system includes a processor subsystem including one or more processors a memory subsystem including main memory cache memory etc. and a variety of peripheral devices connected to the processor subsystem via a peripheral bus. Peripheral devices may include for example keyboard mouse and display adapters disk drives and CD ROM drives network interface devices and so on. The processor subsystem communicates with the peripheral devices by reading and writing commands and information to specific addresses that have been preassigned to the devices. The addresses may be preassigned regions of a main memory address space an I O address space or another kind of configuration space. Communication with peripheral devices can also take place via direct memory access DMA in which the peripheral devices or another agent on the peripheral bus transfers data directly between the memory subsystem and one of the preassigned regions of address space assigned to the peripheral devices.

Most modern computer systems are multitasking meaning they allow multiple different application programs to execute concurrently on the same processor subsystem. Most modern computer systems also run an operating system which among other things allocates time on the processor subsystem for executing the code of each of the different application programs. One difficulty that might arise in a multitasking system is that different application programs may wish to control the same peripheral device at the same time. In order to prevent such conflicts another job of the operating system is to coordinate control of the peripheral devices. In particular only the operating system can access the peripheral devices directly application programs that wish to access a peripheral devices must do so by calling routines in the operating system. The placement of exclusive control of the peripheral devices in the operating system also helps to modularize the system obviating the need for each separate application program to implement its own software code for controlling the hardware.

The placement of exclusive control of the peripheral devices in the operating system also permits management of another potential difficulty that of improper control or handling of the peripheral device. For network interface devices for example improper or inappropriate control of the devices could compromise other applications running in the computer system or could compromise or otherwise negatively impact operation of the network to which the device is connected. In established operating systems much of the software code for controlling these devices has evolved over a number of years and has been updated and improved in response to numerous tests by numerous people on numerous types of network interface devices. The software code in the operating system has therefore developed a certain level of trust users network administrators network architects and other network devices can presume that the great majority of packets originating from this software code will conform to network protocol specifications. Additional code for controlling each particular peripheral device is incorporated into the operating system in the form of a device driver specific to the particular peripheral device. Device drivers are usually written by or in association with the manufacturer of the particular peripheral device so they too are afforded a certain level of trust.

The part of the operating system that controls the hardware is usually the kernel. Typically it is the kernel which performs hardware initializations setting and resetting the processor state adjusting the processor internal clock initializing the network interface device and other direct accesses of the hardware. The kernel executes in kernel mode also sometimes called trusted mode or a privileged mode whereas application level processes execute in a user mode. Typically it is the processor subsystem hardware itself which ensures that only trusted code such as the kernel code can access the hardware directly. The processor enforces this in at least two ways certain sensitive instructions will not be executed by the processor unless the current privilege level is high enough and the processor will not allow user level processes to access memory locations including memory mapped addresses associated with specific hardware resources which are outside of a user level physical or virtual address space already allocated to the process. As used herein the term kernel space or kernel address space refers to the address and code space of the executing kernel. This includes kernel data structures and functions internal to the kernel. The kernel can access the memory of user processes as well but kernel space generally means the memory including code and data that is private to the kernel and not accessible by any user process. The term user space or user address space refers to the address and code space allocated by a code that is loaded from an executable and is available to a user process excluding kernel private code data structures. As used herein all four terms are intended to accommodate the possibility of an intervening mapping between the software program s view of its own address space and the physical memory locations to which it corresponds. Typically the software program s view of its address space is contiguous whereas the corresponding physical address space may be discontiguous and out of order and even potentially partly on a swap device such as a hard disk drive. Address spaces are sometimes referred to herein as virtual address spaces in order to emphasize the possibility of such mappings.

Although parts of the kernel may execute as separate ongoing kernel processes much of the kernel is not actually a separate process running on the system. Instead it can be thought of as a set of routines to some of which the user processes have access. A user process can call a kernel routine by executing a system call which is a function that causes the kernel to execute some code on behalf of the process. The current process is still the user process but during system calls it is executing inside of the kernel and therefore has access to kernel address space and can execute in a privileged mode. Kernel code is also executed in response to an interrupt issued by a hardware device since the interrupt handler is found within the kernel. The kernel also in its role as process scheduler switches control between processes rapidly using the clock interrupt and other means to trigger a switch from one process to another. Each time a kernel routine is called the current privilege level increases to kernel mode in order to allow the routine to access the hardware directly. When the kernel relinquishes control back to a user process the current privilege level returns to that of the user process.

When a user level process desires to communicate with the NIC conventionally it can do so only through calls to the operating system. The operating system implements a system level protocol processing stack which performs protocol processing on behalf of the application and also performs certain checks to make sure outgoing data packets have authorized characteristics and are not malformed. In particular an application wishing to transmit a data packet using TCP IP calls the operating system API e.g. using a send call with data to be transmitted. This call causes a context switch to invoke kernel routines to copy the data into a kernel data buffer and perform TCP send processing. Here protocol is applied and fully formed TCP IP packets are enqueued with the interface driver for transmission. Another context switch takes place when control is returned to the application program. Note that kernel routines for network protocol processing may be invoked also due to the passing of time. One example is the triggering of retransmission algorithms. Generally the operating system provides all OS modules with time and scheduling services driven by the hardware clock interrupt which enable the TCP stack to implement timers on a per connection basis. The operating system performs context switches in order to handle such timer triggered functions and then again in order to return to the application.

It can be seen that network transmit and receive operations can involve excessive context switching and this can cause significant overhead. The problem is especially severe in networking environments in which data packets are often short causing the amount of required control work to be large as a percentage of the overall network processing work.

One solution that has been attempted in the past has been the creation of user level protocol processing stacks operating in parallel with those of the operating system. Such stacks can enable data transfers using standard protocols to be made without requiring data to traverse the kernel stack. In one implementation TCP and other protocols are implemented twice once built into the kernel and once built into a user level transport library accessible to application programs. In order to control and or communicate with the network interface device an application issues API application programming interface calls. Some API calls may be handled by the user level transport libraries and the remainder can typically be passed on through the interface between the application and the operating system to be handled by the libraries that are available only to the operating system. For implementation with many operating systems it is convenient for the transport libraries to use existing Ethernet IP based control plane structures e.g. SNMP and ARP protocols via the OS interface.

There are a number of difficulties in implementing transport protocols at user level. Most implementations to date have been based on porting pre existing kernel code bases to user level. Examples of these are Arsenic and Jet stream. These have demonstrated the potential of user level transports but have not addressed a number of the problems required to achieve a complete robust high performance commercially viable implementation.

One particular problem with user level transport libraries is that in bypassing many of the routines normally performed in the kernel they also lose the trust normally accorded those routines. This is because the kernel no longer has control of the user level routines and cannot enforce their identity with those in the kernel. Users or application programs are able to modify the user level transport routines or replace them with others provided by a third party. As a result the support of user level transport libraries to bypass kernel routines and avoid context switches increases the risk of malformed or even malicious traffic driven onto the network.

Part of the risk of permitting user level transport libraries can be overcome by virtualizing the network interface device in such a way that each process is aware of only its own resources. The hardware can be virtualized in such a way that one process cannot transmit or receive data on behalf of another nor can one process see the data belonging to another process. But this kind of virtualization does not prevent a process from transmitting problematic data packets out onto the network through its own assigned resources hence trust is still not ensured.

In order to address issues like the latter roughly described a network interface device receiving data packets from a computing device for transmission onto a network the data packets having a certain characteristic transmits the packet only if the sending queue has authority to send packets having that characteristic. The data packet characteristics can include transport protocol number source and destination port numbers source and destination IP addresses for example. Authorizations can be programmed into the NIC by a kernel routine upon establishment of the transmit queue based on the privilege level of the process for which the queue is being established. In this way a user process can use an untrusted user level protocol stack to initiate data transmission onto the network while the NIC protects the remainder of the system from certain kinds of compromise.

The following description is presented to enable any person skilled in the art to make and use the invention and is provided in the context of a particular application and its requirements. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art and the general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present invention. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

The network interface card provides an interface to outside networks including an interface to the network and is coupled via network to corresponding interface devices in other computer systems. The physical hardware component of network interfaces are referred to herein as network interface cards NICs although they need not be in the form of cards for instance they could be in the form of integrated circuits ICs and connectors fitted directly onto a motherboard or in the form of macrocells fabricated on a single integrated circuit chip with other components of the computer system.

Network may comprise many interconnected computer systems and communication links. These communication links may be wireline links optical links wireless links or any other mechanism for communication of information. While in one embodiment network is the Internet in other embodiments network may be any suitable computer network or combination of networks. In and embodiment described herein network supports an Ethernet protocol.

Host memory subsystem typically includes a number of memories including a main random access memory RAM for storage of instructions and data during program execution and a read only memory ROM in which fixed instructions and data are stored. One or more levels of cache memory may also be included in the host memory subsystem . For simplicity of discussion the host memory subsystem is sometimes referred to herein simply as host memory . As used herein virtual memory is considered part of the host memory subsystem even though part of it may be stored physically at various times on a peripheral device.

The communication channel provides a mechanism for allowing the various components and subsystems of computer system to communicate with each other. In one embodiment the communication channel comprises a PCI bus. Other embodiments may include other buses and may also include multiple buses.

Computer system itself can be a varying types including a personal computer a portable computer a workstation a computer terminal a network computer a television a mainframe a server or any other data processing system or user devices. Due to the ever changing nature of computers and networks the description of computer system depicted in is intended only as a specific example for purposes of illustrating an embodiment of the present invention. Many other configurations of computer system are possible having more or less components and configured similarly or differently than the computer system depicted in .

The NIC can support resources of a number of types i.e. resources having capabilities of different natures. Examples include DMA queues event queues timers and support resources for remote apertures of the type described in WO2004 025477. Each type of resource is provided from a dedicated hardware resource pool which can support numerous instances of resources of the respective type. In order for such an instance to be made operational it must be configured by means of instructions from the computing device as described in more detail below.

The NIC communicates with the computing device over the bus . In this example the bus is a PCI bus but the invention is not limited to such a bus. Data transmitted over the PCI bus is associated with a destination address and is received by whichever entity that is connected to the bus has had that address allocated to it. In a typical PC implementation the addresses are allocated in pages of 4 or 8 kB. One or more of these pages may be allocated to the NIC . Blocks and represent allocated pages on the PCI bus .

The NIC has a bus interface controller a resource configuration unit and a bus mapping table . The resource configuration unit processes communications received from the computer that provide instructions on the allocation re allocation and de allocation of resources on the NIC and configures the resources in accordance with such instructions. The kernel driver stores a record of which resources on the NIC are allocated. When a resource is to be allocated the driver identifies a suitable free resource of the required type on the NIC and transmits an allocation instruction to the NIC . The instruction identifies the resource and specifies the details of how it is to be allocated including details of the internal configuration of the resource e.g. in the case of a timer the amount of time it is to run for . That instruction is passed to the resource configuration unit. The resource configuration unit then loads the specified configuration into the identified resource. The instruction also includes an ownership string which may be an identification of which application or process on the computer is using the resource. The resource configuration unit stores these in a row of the bus mapping table. An example of entries in the bus mapping table is shown in and is described in more detail below. When a resource is to be re allocated the relevant entries in the resource s own configuration store and in the bus mapping table are altered as necessary. When a resource is to be de allocated it is disabled and any rows of the bus mapping table that relate to it are deleted.

The general operation of the system of for the transfer of data to and from the network will now be described.

During setup of the system one or more pages on the bus are allocated to the NIC . Part of this address space page can be used by the kernel driver to send instructions to the NIC . Other pages e.g. page can be used for communication between application processes such as application and the resources . The resource configuration unit stores a record of the pages that are allocated to the NIC for use by resources. Note that in some embodiments some or all of the functions of the resource configuration unit may alternatively be provided by the kernel driver itself.

When an application wishes to open a data connection over the network it calls a routine in the user level transport library to cause the NIC resources that are required for the connection to be allocated. Standard types of network connection require standard sets of resources for example an event queue transmit TX and receive RX DMA queues and a set of direct memory accessible DMA able memory buffers. For example a typical set may contain one TX queue one RX queue two timers and on the order of 100 DMA memory buffers.

The user level transport library includes routines that can be called directly by the application process and that initiate the allocation of such standard sets of resources including set numbers of resources of different types. The transport library also includes routines that allow a resource of each type to be allocated re allocated or de allocated individually. The presence of both these types of instruction means that standard connections can be set up efficiently and yet non standard groups of resources can be created and existing connections can be reconfigured on a resource by resource basis. As used herein a user level stack is any protocol processing software that runs in unprotected mode. A protocol stack is the set of data structures and logical entities associated with the networking interfaces. This includes sockets protocol drivers and the media device drivers.

The routines for allocation re allocation and de allocation of resources require access to restricted memory mapped addresses such as page for sending configuration instructions to the NIC . Since the user level transport library lacks the necessary privilege level to perform these accesses these routines in the user level transport library make calls to the kernel driver . In a Unix environment for example such calls might take the form of IOCtl system calls. These calls cause an initial context switch to a kernel level process which in turn communicate the instructions to the NIC for the allocation of the resources as specified in the routines. Those instructions specify the identity of the application or process with which the resources are to be associated and the nature of the resources. The instructions are processed by the resource configuration unit of the NIC .

A feature of the system of is that the space on the bus that is allocated to the NIC can be split dynamically between the resources on the bus . Once one or more pages have been allocated to the NIC for use by resources those resources can be allocated one or more individual sub page addresses within that page corresponding to locations as illustrated at . Thus each resource can have a part of the total space allocated to it. A record of which part of the total space is allocated to which resource is stored in the bus mapping table . The effect is that a single page of the bus can be used for communication to resources of multiple types and or resources that relate to multiple connections and or resources that are associated with multiple applications or process on the computer . As a result the total bus space can be used relatively efficiently.

The usage of the allocated bus space is managed by the kernel driver . When a resource is to be allocated the RCU identifies using a data store whose content it manages an unused block in the space on the bus that has already been allocated for use by resources of the NIC the space being of the size required for the resource. It then stores in that data store the identity of the resource resource ID the address of the block within the allocated space sub page ID and the identity of the application or process that is to use the resource process tag and sends a message to the resource configuration unit RCU to cause it to store corresponding data in the bus mapping table as shown in . If the RCU finds that table indicates the address to be already occupied then it returns an error code to the driver. The sub page address may need to be supplemented with the address of the page in which the sub page lies if that cannot be inferred as a result of only a single page having been allocated for use by the resources. If the total space allocated for use by resources is insufficient then the kernel driver allocates it more space. Having allocated the resources the RCU returns a success message to the kernel driver. The allocated page and sub page addresses are returned to and mapped into the virtual address space of the user level process that requested the resources in order that it can access them by means of that data. Another context switch then takes place back to the user level calling process.

An application that has had resources allocated to it can access them by sending data e.g. by means of load store cycles through a virtual memory mapping to the relevant bus page at the sub page address corresponding to the respective resource. Since these addresses are part of the application s virtual address space no context switch to any kernel level processes are required in order to perform these accesses. Any data sent to pages allocated to resources is picked off the bus by the bus interface controller . It directs that data to the appropriate one of the resources by performing a look up in the table to identify the identity of the resource to which the sub page address has been allocated. An application can also access a resource by means other than a bus write for example by means of direct memory access DMA . In those instances the NIC checks that the identity of the application process from which the access has been received matches the identity indicated in the table for the resource. If it does not match the data is ignored. If it matches it is passed to the relevant resource. This adds to security and helps to prevent corruption of the resources by other applications.

The set of resources allocated to an application or process may be considered to constitute a virtual network interface VNIC .

Once a virtual interface has been composed it may be reconfigured dynamically. As one example of dynamic reconfiguration a resource that is no longer required may be freed up. To achieve this the application using the resource calls a de allocation routine in the user level transport library . The de allocation routine calls the kernel driver which instructs the RCU to de allocate the resource by disabling it clearing its status and deleting its row in the table .

As another example of dynamic reconfiguration additional resources may be added to the VNIC. The process is analogous to that described above for initial composition of the VNIC.

As yet another example of dynamic reconfiguration resources may be passed from one application or process to another. This is most useful in the situation where a single application has multiple processes and wants to pass control of a resource from one process to another for example if data from the network is to be received into and processed by a new process. To achieve this the application using the resource calls a re allocation routine in the transport library . The re allocation routine calls the kernel driver which instructs the RCU to re allocate the resource modifying its row in the table to specify the identity of the application or process that is taking over its control.

In some instances it may be desirable for resources of one type to communicate with resources of another type. For example data received from the network may be being passed to an application for processing. The application has a queue in a memory connected to the bus . The queue is managed in part by the transport library which provides a DMA queue resource on the NIC with an up to date pointer to the next available location on the queue . This is updated as the application reads data from the queue . When data is received from the network it is passed to an event queue resource which writes it to the location identified by the pointer and also triggers an event such as an interrupt on the computing device to indicate that data is available on the queue. In order for this to happen the event queue resource must learn the pointer details from the DMA queue resource . This requires data to be passed from the DMA queue resource to the event queue resource.

To achieve this the process tag column of the table can be treated more generally as an ownership tag and can link the DMA queue to the related event queue. To achieve this the ownership tag of the event queue can be set to the identity of the related DMA queue. When the DMA queue needs to pass data to the related event queue it can identify the event queue from the table by performing a look up on its own identity in the ownership tag column.

Data intended to be passed from one resource to another can be checked by the bus controller to ensure that it is compatible with the settings in the table . Specifically when data is to be sent from one resource to another the bus controller checks that there is a row in the table that has the identity of the resource that is the source of the data in the ownership tag field and the identity of the resource that is the intended destination of the data in the resource ID field. If there is no match then the data is prevented from reaching its destination. This provides additional security and protection against corruption. Alternatively or in addition it may be permitted for one resource to transmit data to another if both are in common ownership in this example if their resource ID fields indicate that they are owned by the same process application or other resource.

The identities of resources linked in this way can also be reconfigured dynamically by means of the re configuration routines in the transport library.

In a step when the application first starts up its libraries are initialized. This includes the user level transport library which is initialized into the application s virtual address space.

Step begins an example sequence of steps in which the application process uses a UDP transport protocol. In step the application makes a call to the socket routine of the user level transport library specifying that it would like a UDP socket. In step the application process binds the socket to a port using a call to the bind routine of the user level transport library and in step it begins writing its transmit data into the applications buffers in the application s virtual address space. In step after sufficient data has been written into the buffers for one or more data packets the application process makes a call to the sendTo routine of the user level transport library specifying the socket handle the buffer or buffers the destination IP address and the destination port to which the packet is to be sent. Steps and are repeated many times most likely interspersed with many other functions performed by the application process. When the application has finished with the socket that it had created in step then in step the application makes a call to the close routine of the user level transport library in order to close the socket.

Alternatively to the UDP sequence beginning with step step begins an example sequence of steps in which the application process uses a TCP transport protocol. In step instead of calling the socket routine of the user level transport library to specify the UDP protocol it calls the socket routine to specify the TCP protocol. In step the application process calls the bind routine similarly to step in order to bind the socket to a port. In step since the transport protocol is now TCP the application process calls the connect routine of the user level transport library in order to form a TCP connection with a specified destination IP address and port. In step the application process writes transmit data into buffers in the application program s virtual address space similarly to step and in step when ready the application process calls the send routine of the user level transport library in order to have the data packet processed according to the TCP protocol and transmitted out to the network via network interface card . Again steps and can be repeated many times and when the application process has finished with the socket it calls the close routine of the user level transport library step .

As can be seen of all the steps illustrated only the step of initializing the transport library need involve a context switch to a kernel level process. In many embodiments all of the remaining steps can be performed by the user level transport library without involvement of the kernel driver . While this feature can help improve performance markedly it also creates a risk that non standard or third party transport libraries will be installed for the application program in place of trusted code. As will be seen the network interface card itself protects against one or more of the risks that might arise.

In step as part of the initialization of the user level transport library a resource allocation routine in the kernel driver is invoked. The kernel level routine is required for allocating resources in the network interface card and the host memory subsystem since these resources are outside the virtual address space of the application or involve direct hardware accesses that advisedly are restricted to kernel processes. After resource allocation the user level driver initialization routine may perform a number of other steps before it returns to the application in step .

The kernel resource allocation routine allocates memory and an initial set of resources for the application program and maps these into the application s virtual address space. Before discussing the particular steps performed by the kernel resource allocation routine it will be useful to understand some of the formats in which the system maintains its queue structures.

Individual buffers may be either 4 k or 8 k bytes long in one embodiment and they are chained together into logically contiguous sequences by means of physically contiguous descriptors in a buffer descriptor table stored in the NIC . For example one transmit queue might occupy buffers and in host memory which are discontiguous and possibly out of order regions of memory. They are chained together into a single logically contiguous space by the physically contiguous entries and in the buffer descriptor table . The entries and are written and managed by the host and are viewed as a wrap around ring. So for example if the host wishes to define a transmit buffer list having 64 k entries for transmit data buffer descriptors and each buffer is 4 k in size then the host will allocate a physically contiguous sequence of 16 entries in buffer descriptor table for this transmit buffer list. Similarly one event queue might occupy buffers and in host memory . These buffers are discontiguous and possibly out of order in host memory but are chained together into a single logically contiguous wrap around space by the physically contiguous entries and in the buffer descriptor table . The buffer descriptor table is indexed by buffer ID and each of its entries identifies among other things the base address of the corresponding buffer in host memory .

In order to keep track of the state of each of the transmit receive and event queues for the many user level applications that might be in communication with NIC at the same time the NIC includes a transmit queue descriptor table a receive queue descriptor table and an event queue descriptor table . Each transmit queue has a corresponding transmit queue ID which is used as an index into the transmit queue descriptor table . The designated entry in the transmit queue descriptor table is the starting point for describing the state and other characteristics of that particular transmit queue as viewed by the NIC . Each such entry identifies among other things 

In order to retrieve current transmit data from a particular transmit queue in host memory the NIC first uses the ID of the particular transmit queue to look up in the transmit queue descriptor table the buffer ID of the base buffer containing the transmit descriptor queue. The NIC also obtains from the same place the current device centric buffer list read pointer into that transmit descriptor queue. It then uses the base buffer ID as a base and the device centric buffer list read pointer high order bits as an offset into the buffer descriptor table to obtain the base address in host memory of the buffer that contains the particular transmit buffer list . The NIC then uses that base address as a base and the device centric buffer list read pointer low order bits times the number of bytes taken up per descriptor as an offset to retrieve from host memory the current entry in the particular transmit descriptor queue.

The transmit queue descriptor table entry designated by the transmit queue ID as previously mentioned also contains the ID of the transmit event queue associated with the particular transmit queue. Similarly the receive queue descriptor table entry designated by the receive queue ID contains the ID of the event queue associated with the particular receive queue. All of the event queues for all the applications are described by respective entries in the event queue descriptor table . The entry in the event queue descriptor table identified by a queue ID from the transmit or receive queue descriptor table or is the starting point for describing the state and other characteristics of that particular event queue as viewed by the NIC .

Note that as illustrated in whereas each slot e.g. shown in the buffer descriptor table represents a single descriptor each slot e.g. in the host memory represents a memory page of information. A page might be 4 k or 8 k bytes long for example so if a transmit data buffer descriptor in a transmit queue occupies either 4 or 8 bytes then each slot or as shown in might hold 512 1 k or 2 k transmit data buffer descriptors.

In step the kernel routine allocates a minimum set of the buffers for each of the transmit receive and event queues requested and programs their buffer IDs into the transmit receive and event queue descriptor tables and . In step the kernel routine determines the doorbell address in the NIC for each of the transmit and receive queues and maps them as well into the application s virtual address space. The doorbell address is the address to which the application will write a value in order to notify the NIC either that a transmit buffer is ready or that a receive buffer can be released. For transmit queues the doorbell address is the address of the device centric transmit queue read pointer in the transmit queue descriptor table entry for the particular transmit queue. For receive queues the doorbell address is the address of the device centric receive queue write pointer in the receive queue descriptor table entry for the particular receive queue.

In step the kernel routine programs into the NIC certain access rights authorization rights that are to be associated with the particular transmit queue. These are the authorization rights to which the NIC will look in order to determine whether a particular sending process has authority to send packets having certain characteristics and the kernel routine programs them in dependence upon the privilege level of the process that made the kernel resource allocation call. Note that although the privilege level of an application process running in the computer system is maintained on a per process basis the authorization rights are maintained on the NIC on a per queue basis. This enables the NIC to validate outgoing transmit packets without having to know anything about the particular operating system running in the host computer or the privilege mechanisms that it uses.

In different embodiments the NIC can refer to different characteristics of a transmit data packet in order to determine whether the transmit queue has sufficient authority to send it. In one embodiment the NIC checks only whether the packet is formed according to an allowed transport protocol. For example in one embodiment user level processes may be permitted to send packets using only the TCP or UDP transport protocols and no others. Each data packet has a header field which identifies the transport protocol according to which it was formed and the NIC can compare that protocol number with those that have been programmed into the NIC as being allowed for the transmit queue from which the data packet was retrieved.

If the total array of transport protocols supported by the NIC is short enough then a field may be allocated in each entry of the transmit queue descriptor table for identifying the allowed protocols. For example if only eight protocols are supported and eight bit field might be used with each bit representing one of the protocols. If a bit is active then the corresponding transport protocol is allowed if it is inactive then it is not.

Alternatively the allowed protocols may be listed in a separate authorizations database maintained in the NIC such as that shown in . In the authorizations database takes the form of a table in which each entry contains a queue ID and an indication of an allowed protocol for that queue. If several different protocols are allowed for particular queue then the queue ID appears in several different entries in the table. When the NIC is checking the validity of a transmit packet from a particular transmit queue it searches the table for an entry that contains both the transmit queue ID and the transport protocol according to which the packet was formed retrieved from the packet header . If the table does contain such an entry then the packet is valid. If not then the packet is rejected. As used herein the term database does not necessarily imply any unity of structure. For example two or more separate databases when considered together still constitute a database as that term is used herein. 

In other embodiments the NIC can validate other characteristics of a transmit data packet. For example it can validate the source IP address the source port number the destination IP address and destination port number either instead of or additionally to the allowed protocols. illustrates an example authorizations database that can support validating all such characteristics. As shown in each entry in the database table contains six fields source IP address source port number destination IP address destination port number queue ID and allowed protocol number. In order to keep table short some of these fields may be filled with indications of numeric ranges rather than only a specific number. For example in some systems only privileged processes can transmit packets indicating that they were sourced from a port number in the range 0 1023. In this case the source port field of the table a might contain only a single bit indicating whether source port numbers within the range 0 1023 are permitted. As with an embodiment using the table when the NIC is checking the validity of a transmit packet from a particular transmit queue it searches the table of for a single entry that contains or includes if numeric ranges are specified the source IP address the source port number destination IP address destination port number and the allowed protocol all taken from the packet header as well as the ID of the transmit queue from which the packet was retrieved. If the table does contain such an entry then the packet is valid. If not then it is rejected.

Returning to after the kernel resource allocation routine programs the authorization rights for the transmit queue into the NIC it returns to the application with handles for the resources allocated with the base virtual addresses of the transmit receive and event queues and virtual memory addresses corresponding to the doorbells allocated in the transmit and receive queue descriptor tables and step .

At least four different mechanisms might be used in the same or different embodiments to ensure that different processes do not interfere with each other s use of particular IP address port number combinations and that user processes do not improperly operate through physical or logical port numbers or other resources that should be reserved for the kernel. In one mechanism a system wide policy exists which allocates all port numbers within a particular range to the user stack only. The user level bind routine can be designed to immediately accept only those requests from a user level process to bind to one of such port numbers or to immediately pass such requests on to the kernel to handle.

In a second mechanism during the resource allocation step performed during initialization of a particular instance of the user level driver the kernel allocates a unique IP address for that instance to use as it wishes. If each instance of the user level driver has its own exclusively assigned IP address then the instance can manage the available port numbers for use with that IP address without risking interference with any other process. This mechanism is useful only if there are sufficient numbers of IP addresses available to the computer system to allocate to the various requesting processes. Again this mechanism can be used in conjunction with the first to reject or pass on to the kernel all user level requests to bind to a kernel only port number regardless of the exclusivity of an assigned IP address.

In a third mechanism again during initialization of a particular instance of the user level driver the initialization routine makes a number of anticipatory bind calls to the kernel in order to form a pool of port numbers that the user level driver instance can later allocate to the application program upon receipt of bind calls to the user level driver. This mechanism can succeed with far fewer IP addresses available to the computer system but also undesirably involves a context switch during library initialization for each port number to be added to the pool.

In yet a fourth mechanism no IP address port number combinations are pre allocated to the particular instance of the user level driver. Instead the user level bind routine invokes the kernel bind routine for each user level bind call received. This mechanism utilizes IP address port number combinations most conservatively but may require more context switches than any of the first second and third mechanisms. In an embodiment this fourth mechanism is used only as a backup for example if the user level process requires more port numbers than were made available using the anticipatory bind calls in the third mechanism.

If in step the user level bind routine determines that the requested port number is not available to the current instance of the user level driver or otherwise cannot determine whether is available then in step the routine makes a call to the kernel bind routine to pass the request on to the kernel to handle. If the fourth mechanism above is the only way that the particular embodiment avoids conflicting or illegal allocation of address port number combinations then step will be taken during every user level call to the bind routine . Otherwise step will be taken only as a backup if pre allocated port numbers have been exhausted or if the routine otherwise cannot determine that the requested port number is available.

If the specified port number is legal or if a port number was assigned by the routine in step then in step the routine updates the application s state internally to bind the port number with the specified socket. The routine returns to the caller in step .

Although the user level bind routine of attempts to prevent the allocation of problematical port numbers to user level processes as described above it will be appreciated that a different implementation of the user level bind routine may not be as careful. It is a feature of the invention that the NIC can be designed to detect and reject transmit packets which do designate a source port number that is illegal or for which the sending process lacks sufficient privilege to use. Therefore no damage will occur if transport library routines are used which do not follow the steps set forth as long as any errors they cause are of a type that the NIC is designed to detect downstream.

Note that all the steps of take place entirely within the virtual address space of the current user level process. There is no need to copy data into the kernel address space nor is there any need to perform a context switch to a kernel process either to perform the protocol processing to enqueue the new UDP packet or to notify the NIC of the availability of a new packet in the transmit queue. Additionally as with the user level socket and bind routines a user level transport library routine which does not perform the steps faithfully as set forth in will not cause the transmission of malformed or illegal packets to the extent the NIC is designed to reject them during the validation process performed on the NIC .

In operation as a data packet is received from the bus the first section received is the IP header. This section contains the source and destination IP addresses as well as an identification of the transport layer protocol. Next comes the transport layer header which contains the source and destination port numbers. Based on this information the header validation logic compares the data packet characteristics to those in the authorizations database to determine whether the data packet arriving from the bus is authorized. If it is not then the header validation logic can cause the DMA controller to abort the current transfer and can also cause the FIFO control logic to unwind its write pointer back to the end of the previous packet in the transmit FIFO .

In step the NIC determines whether the device centric write pointer for the current transmit queue modulo exceeds the device centric read pointer for current transmit queue. These values are available to be NIC in the transmit queue descriptor table entry for the current transmit queue and the test will be positive if one of the transport libraries or has updated the device centric write pointer to notify the NIC of the availability of the data packet for transmission. The term modulo exceeds is used herein to accommodate wrap around circular queues. That is the device centric write pointer modulo exceeds the device centric read pointer for a queue if the write pointer exceeds the read pointer modulo the queue length.

If the test of step is negative then in step the NIC proceeds to examine the next transmit queue according to its algorithm.

If the test of step is positive then in step the NIC reads one or more transmit descriptors from the current transmit queue beginning at the entry pointed to by the device centric read pointer. In step the NIC programs the DMA controller to retrieve the packet from host memory into transmit FIFO . In step during the retrieval process the NIC examines the header information on the packet as it is being retrieved and tests the current queue s authority to send packets having the characteristics of that being retrieved. If the NIC determines that the packet is authorized step then in step after packet retrieval the NIC will updated its device centric transmit queue read pointer. The NIC then writes a transmit completion event into the event queue associated with the current transmit queue for eventual retrieval by the user level process. In some embodiments the NIC might wait to complete retrieval of a number of transmit data packets before writing a batched transmit completion event covering all of them. The process then returns to step for the queue selection algorithm to select the same or another transmit queue. Eventually in step the NIC transmits the packet from the head of the transmit FIFO out onto the network .

If in step it is determined that the current packet is not authorized to be sent from the current transmit queue then in step the header validation logic causes the DMA controller to abort the current transfer thereby freeing up the bus . It also notifies the FIFO control logic to unwind the transmit FIFO queue write pointer as previously described. The NIC may also report an error back to the application program.

It can be seen that the NIC transmits packets onto network only if the sending transmit queue is authorized to transmit packets having the characteristics for which header validation logic checks. In some embodiments still other requirements might be necessary before the NIC will allow the packet to go out.

As used herein identification of an item of information does not necessarily require the direct specification of that item of information. Information can be identified in a field simply by referring to the actual information through one or more layers of indirection or by identifying one or more items of different information which are together sufficient to determine the actual item of information. In addition the term indicate is used herein to mean the same as identify .

The foregoing description of preferred embodiments of the present invention has been provided for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Obviously many modifications and variations will be apparent to practitioners skilled in this art. In particular and without limitation any and all variations described suggested or incorporated by reference in the Background section of this patent application are specifically incorporated by reference into the description herein of embodiments of the invention. The embodiments described herein were chosen and described in order to best explain the principles of the invention and its practical application thereby enabling others skilled in the art to understand the invention for various embodiments and with various modifications as are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalents.

