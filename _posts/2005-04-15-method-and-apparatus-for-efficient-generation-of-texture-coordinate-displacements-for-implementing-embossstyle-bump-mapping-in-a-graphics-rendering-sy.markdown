---

title: Method and apparatus for efficient generation of texture coordinate displacements for implementing emboss-style bump mapping in a graphics rendering system
abstract: A graphics system including a custom graphics and audio processor produces exciting 2D and 3D graphics and surround sound. The system includes a graphics and audio processor including a 3D graphics pipeline and an audio digital signal processor. Emboss style effects are created using fully pipelined hardware including two distinct dot-product computation units that perform a scaled model view matrix multiply without requiring the Normal input vector and which also compute dot-products between the Binormal and Tangent vectors and a light direction vector in parallel. The resulting texture coordinate displacements are provided to texture mapping hardware that performs a texture mapping operation providing texture combining in one pass. The disclosed pipelined arrangement efficiently provides interesting embossed style image effects such as raised and lowered patterns on surfaces.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07307640&OS=07307640&RS=07307640
owner: Nintendo Co., Ltd.
number: 07307640
owner_city: Kyoto
owner_country: JP
publication_date: 20050415
---
This application claims the benefit of U.S. Provisional Application Ser. No. 60 226 892 filed Aug. 23 2000 the entire content of which is hereby incorporated by reference.

This application is also related to the following commonly assigned co pending applications identified below which focus on various aspects of the graphics system described herein. Each of the following applications are incorporated herein by reference 

The present invention relates to computer graphics and more particularly to interactive graphics systems such as home video game platforms. Still more particularly this invention relates to efficient generation of texture coordinate displacements for implementing emboss style bump mapping effects for diffuse lit textures on a rendered object.

Many of us have seen films containing remarkably realistic dinosaurs aliens animated toys and other fanciful creatures. Such animations are made possible by computer graphics. Using such techniques a computer graphics artist can specify how each object should look and how it should change in appearance over time and a computer then models the objects and displays them on a display such as your television or a computer screen. The computer takes care of performing the many tasks required to make sure that each part of the displayed image is colored and shaped just right based on the position and orientation of each object in a scene the direction in which light seems to strike each object the surface texture of each object and other factors.

Because computer graphics generation is complex computer generated three dimensional graphics just a few years ago were mostly limited to expensive specialized flight simulators high end graphics workstations and supercomputers. The public saw some of the images generated by these computer systems in movies and expensive television advertisements but most of us couldn t actually interact with the computers doing the graphics generation. All this has changed with the availability of relatively inexpensive 3D graphics platforms such as for example the Nintendo 64 and various 3D graphics cards now available for personal computers. It is now possible to interact with exciting 3D animations and simulations on relatively inexpensive computer graphics systems in your home or office.

One problem that graphics system designers have often confronted in the past was the efficient rendering of a 3D object that displays realistic looking surface characteristics that react to various lighting conditions in a manner similar to the surface of an actual object having for example random surface flaws irregularities roughness bumps or other slight non planar surface variations. While in some instances such minute surface characteristics might be actually modeled the time required for translating and rendering a 3D object with such a complex surface would be prohibitive for most real time or interactive gaming applications. Consequently various solutions to this problem were offered. For example a technique generally known as bump mapping was developed which allowed one to approximate the effect that non planar surface variations would produce on lighted object. See for example J. F. Blinn Simulation of Wrinkled Surfaces SIGRAPH 78 Proceedings vol. 12 No. 3 pp. 286 292 August 1978 Models of Light Reflection for Computer Synthesized Pictures Proc. 4Conference on Computer Graphics and Instructive Techniques 1977 and Programming with OpenGL Advanced Rendering by Tom McReynolds and David Blythe SIGGRAPH 97 course Section 8.3 Bump Mapping with Textures . Basically this technique allows a graphics application programmer to add realism to an image without using a lot of geometry by modeling small surface variations as height differences and then applying those difference values over a surface as perturbations to a surface Normal vector used in computing surface lighting effects. Effectively a bump map modifies the shading of a polygon by perturbing the surface Normal on a per pixel basis. The shading makes the surface appear bumpy even though the underlying geometry is relatively flat.

Most conventional approaches toward implementing simple forms of bump mapping effects with diffuse lit textured surfaces generally entail computing for each pixel the difference between a first sample of a bump map texture image at a particular texture coordinate and a second sample of the same texture image at a texture coordinate displacement. In addition computing a texture coordinate displacement map generally involves computations using eye space components of surface Tangent and Binormal vectors binormals . In particular to implement a simple form of bump mapping having an embossing type effect on a texture image it is most efficient to compute and apply the texture coordinate displacements in the eye space view space camera space reference frame which is more conducive to a subsequent rasterizing process prior rendering for display. Consequently texture coordinate displacement for emboss style bump mapping is preferably computed and generated after vertex position and surface binormals at a vertex are transformed from model space into eye space for pixel rendering.

Typically in low cost graphics processing systems such as a home video game system vertex transformation and lighting T L operations are commonly performed by the application program using the graphics system host CPU primarily because a software T L implementation although more computationally taxing on the host CPU is usually less expensive than using specialized hardware. Hardware implementation of T L however may be preferable in gaming systems because it typically results in much faster renderings and can free up host CPU processing time for performing other desirable tasks such as game strategy and AI computations for improved game performance. Moreover in graphics rendering arrangements where T L operations are performed by the application software on the host CPU additional processing tasks such as performing texture coordinate computations for bump mapping can significantly add to the processing overhead.

In graphics rendering systems where the T L operations are performed by dedicated graphics hardware the host CPU typically provides model space vertex attributes to the dedicated T L hardware and then allows the hardware to perform all the coordinate space transformations and lighting computations. Consequently it is not particularly efficient to require the host CPU to compute texture coordinate displacements for bump mapping purposes subsequent to the T L hardware performing space transformations of the vertex position and surface normal binormal vectors. Essentially this would effectively undermine rendering speed improvements gained from utilizing dedicated T L hardware whenever bump mapping operations are performed.

The present invention solves this problem by providing techniques and arrangements in a graphics rendering system for the efficient generation of texture coordinate displacements for implementing at least an emboss style bump mapping texture effect without the need for the host CPU application software to compute the required texture coordinate displacements. An enhanced API applications program interface vertex attribute function capable of specifying three surface normals per vertex i.e. the Normal Tangent and Binormal is utilized and the host CPU application software need only compute the required additional Tangent and Binormal surface vectors per vertex in object space model space in addition to providing the surface Normal and other conventional per vertex attributes.

In accordance with one aspect of the present invention a graphics rendering system is provided with enhanced vertex transformation and lighting T L hardware that is capable of performing at least simple emboss style bump mapping in addition to the conventional T L operations. This style of bump mapping is useful when the surface geometry of an object is being animated. The vector geometry processing portion of the T L hardware is enhanced to accommodate processing a transformation of object space vertex surface binormals i.e. the Tangent and Binormal vectors to eye space and the computation of a texture coordinate displacement based on light direction light to vertex vector dot products with the transformed binormals.

In accordance with another aspect of the present invention an enhanced vertex attribute description API function provides three vertex surface normals N B and T to the T L vector geometry processing hardware along with vertex position and light source position. The geometry processing hardware then transforms the surface normals to eye space computes the light vector in eye space and uses the vector components to compute the appropriate texture coordinate displacements for use in producing an emboss style bump mapped texture effect.

In this example system is capable of processing interactively in real time a digital representation or model of a three dimensional world. System can display some or all of the world from any arbitrary viewpoint. For example system can interactively change the viewpoint in response to real time inputs from handheld controllers or other input devices. This allows the game player to see the world through the eyes of someone within or outside of the world. System can be used for applications that do not require real time 3D interactive display e.g. 2D display generation and or non interactive display but the capability of displaying quality 3D images very quickly can be used to create very realistic and exciting game play or other graphical interactions.

To play a video game or other application using system the user first connects a main unit to his or her color television set or other display device by connecting a cable between the two. Main unit produces both video signals and audio signals for controlling color television set . The video signals are what controls the images displayed on the television screen and the audio signals are played back as sound through television stereo loudspeakers L R.

The user also needs to connect main unit to a power source. This power source may be a conventional AC adapter not shown that plugs into a standard home electrical wall socket and converts the house current into a lower DC voltage signal suitable for powering the main unit . Batteries could be used in other implementations.

The user may use hand controllers to control main unit . Controls can be used for example to specify the direction up or down left or right closer or further away that a character displayed on television should move within a 3D world. Controls also provide input for other applications e.g. menu selection pointer cursor control etc. . Controllers can take a variety of forms. In this example controllers shown each include controls such as joysticks push buttons and or directional switches. Controllers may be connected to main unit by cables or wirelessly via electromagnetic e.g. radio or infrared waves.

To play an application such as a game the user selects an appropriate storage medium storing the video game or other application he or she wants to play and inserts that storage medium into a slot in main unit . Storage medium may for example be a specially encoded and or encrypted optical and or magnetic disk. The user may operate a power switch to turn on main unit and cause the main unit to begin running the video game or other application based on the software stored in the storage medium . The user may operate controllers to provide inputs to main unit . For example operating a control may cause the game or other application to start. Moving other controls can cause animated characters to move in different directions or change the user s point of view in a 3D world. Depending upon the particular software stored within the storage medium the various controls on the controller can perform different functions at different times.

In this example main processor e.g. an enhanced IBM Power PC 750 receives inputs from handheld controllers and or other input devices via graphics and audio processor . Main processor interactively responds to user inputs and executes a video game or other program supplied for example by external storage media via a mass storage access device such as an optical disk drive. As one example in the context of video game play main processor can perform collision detection and animation processing in addition to a variety of interactive and control functions.

In this example main processor generates 3D graphics and audio commands and sends them to graphics and audio processor . The graphics and audio processor processes these commands to generate interesting visual images on display and interesting stereo sound on stereo loudspeakers R L or other suitable sound generating devices.

Example system includes a video encoder that receives image signals from graphics and audio processor and converts the image signals into analog and or digital video signals suitable for display on a standard display device such as a computer monitor or home color television set . System also includes an audio codec compressor decompressor that compresses and decompresses digitized audio signals and may also convert between digital and analog audio signaling formats as needed. Audio codec can receive audio inputs via a buffer and provide them to graphics and audio processor for processing e.g. mixing with other audio signals the processor generates and or receives via a streaming audio output of mass storage access device . Graphics and audio processor in this example can store audio related information in an audio memory that is available for audio tasks. Graphics and audio processor provides the resulting audio output signals to audio codec for decompression and conversion to analog signals e.g. via buffer amplifiers L R so they can be reproduced by loudspeakers L R.

Graphics and audio processor has the ability to communicate with various additional devices that may be present within system . For example a parallel digital bus may be used to communicate with mass storage access device and or other components. A serial peripheral bus may communicate with a variety of peripheral or other devices including for example 

A further external serial bus may be used to communicate with additional expansion memory e.g. a memory card or other devices. Connectors may be used to connect various devices to busses .

3D graphics processor performs graphics processing tasks. Audio digital signal processor performs audio processing tasks. Display controller accesses image information from main memory and provides it to video encoder for display on display device . Audio interface and mixer interfaces with audio codec and can also mix audio from different sources e.g. streaming audio from mass storage access device the output of audio DSP and external audio input received via audio codec . Processor interface provides a data and control interface between main processor and graphics and audio processor .

Memory interface provides a data and control interface between graphics and audio processor and memory . In this example main processor accesses main memory via processor interface and memory interface that are part of graphics and audio processor . Peripheral controller provides a data and control interface between graphics and audio processor and the various peripherals mentioned above. Audio memory interface provides an interface with audio memory .

Command processor receives display commands from main processor and parses them obtaining any additional data necessary to process them from shared memory . The command processor provides a stream of vertex commands to graphics pipeline for 2D and or 3D processing and rendering. Graphics pipeline generates images based on these commands. The resulting image information may be transferred to main memory for access by display controller video interface unit which displays the frame buffer output of pipeline on display .

Command processor performs command processing operations that convert attribute types to floating point format and pass the resulting complete vertex polygon data to graphics pipeline for rendering rasterization. A programmable memory arbitration circuitry see arbitrates access to shared main memory between graphics pipeline command processor and display controller video interface unit .

Transform unit performs a variety of 2D and 3D transform and other operations see . Transform unit may include one or more matrix memories for storing matrices used in transformation processing . Transform unit transforms incoming geometry per vertex from object space to screen space and transforms incoming texture coordinates and computes projective texture coordinates . Transform unit may also perform polygon clipping culling . Lighting processing also performed by transform unit provides per vertex lighting computations for up to eight independent lights in one example embodiment. As discussed herein in greater detail Transform unit also performs texture coordinate generation for emboss style bump mapping effects.

Setup rasterizer includes a setup unit which receives vertex data from transform unit and sends triangle setup information to one or more rasterizer units performing edge rasterization texture coordinate rasterization and color rasterization.

Texture unit which may include an on chip texture memory TMEM performs various tasks related to texturing including for example 

Texture unit performs texture processing using both regular non indirect and indirect texture lookup operations. A more detailed description of the example graphics pipeline circuitry and procedures for performing regular and indirect texture look up operations is disclosed in commonly assigned co pending patent application Ser. No. 09 722 382 entitled Method And Apparatus For Direct And Indirect Texture Processing In A Graphics System and its corresponding provisional application Ser. No. 60 226 891 filed Aug. 23 2000 both of which are incorporated herein by reference.

Texture unit outputs filtered texture values to the Texture Environment Unit for texture environment processing . Texture environment unit blends polygon and texture color alpha depth and can also perform texture fog processing to achieve inverse range based fog effects. Texture environment unit can provide multiple stages to perform a variety of other interesting environment related functions based for example on color alpha modulation embossing detail texturing texture swapping clamping and depth blending. Texture environment unit can also combine e.g. subtract textures in hardware in one pass. For more details concerning the texture environment unit see commonly assigned application Ser. No. 09 722 367 entitled Recirculating Shade Tree Blender for a Graphics System and its corresponding provisional application Ser. No. 60 226 888 filed Aug. 23 2000 both of which are incorporated herein by reference.

Pixel engine performs depth z compare and pixel blending . In this example pixel engine stores data into an embedded on chip frame buffer memory . Graphics pipeline may include one or more embedded DRAM memories to store frame buffer and or texture information locally. Z compares can also be performed at an earlier stage in the graphics pipeline depending on the rendering mode currently in effect e.g. z compares can be performed earlier if alpha blending is not required . The pixel engine includes a copy operation that periodically writes on chip frame buffer to main memory for access by display video interface unit . This copy operation can also be used to copy embedded frame buffer contents to textures in the main memory for dynamic texture synthesis effects. Anti aliasing and other filtering can be performed during the copy out operation. The frame buffer output of graphics pipeline which is ultimately stored in main memory is read each frame by display video interface unit . Display controller video interface provides digital RGB pixel values for display on display .

Briefly the graphics pipeline renders and prepares images for display at least in part in response to polygon vertex attribute data and texel color data stored as a texture image in an associated memory. The graphics rendering pipeline is provided with vertex transformation and lighting T L hardware that is capable of performing simple bump mapping operations in addition to the more conventional T L operations. Pipelined hardware efficiently generates texture coordinate displacements for implementing emboss style bump mapping effects utilizing object space model space surface normals supplied per vertex for example by a graphics application running on the main CPU of the graphics system. An enhanced vertex attribute description command function facilitates the communication and processing of plural surface normals per vertex in addition to other vertex attributes such as vertex position light source position and texture coordinates. The enhanced vertex attribute function specifies Normal Tangent and Binormal surface vectors N T B provided by the host CPU in object space coordinates and uses separate memory indexes per vertex for each of the three surface vectors so as to effectively compress the amount of data needed for bump mapping. A vector geometry processing portion of the T L hardware is also enhanced by providing two distinct dot product computation units to transform the Tangent and Binormal surface vectors to eye space using a scaled model view matrix compute a light direction vector in eye space and perform parallel dot product computations between the computed light direction vector and the transformed Tangent and Binormal vectors to efficiently generate the appropriate texture coordinate displacements for use in creating an embossed texture effect.

In one example embodiment system first stores a texture image in texture memory see for use with the bump mapping operation block . Command Processor then provides object space basis Tangent and Binormal vector data to transform Transform Unit using vertex attribute functions defined in an appropriate graphics API block . The Transform Unit transforms the Tangent and Binormal vector data to eye space block . Transform Unit also computes a light direction light to vertex vector and a normalized light direction vector block . Transform Unit then computes texture coordinate displacements and new texture coordinate values per vertex blocks . Texture Environment TEV unit develops an embossed texture from the original texture stored in texture memory minus the offset texture defined by the displacements block . In other words the original texture is looked up using both non displaced coordinates s t and displaced coordinates s s t t and the texture values are subtracted per pixel. The result is combined with per vertex local diffuse lighting in graphics pipeline and the resulting embossed image is rendered for display on display block . The embossed image results may also be combined with other textures.

In more detail bump mapping described above generates at least 1 texture coordinate displacements s t based on incoming texture coordinates block 2 a normalized light direction block and 3 a per vertex coordinate basis function block . The preferred basis function used is an orthogonal object space coordinate basis. The three orthogonal axes of this coordinate basis are defined by the surface Normal vector a surface Tangent vector and a second mutually perpendicular surface tangent Binormal vector with the Tangent T and Binormal B vectors oriented in directions corresponding to the texture gradient in s and the texture gradient in t i.e. increasing s or t . The two orthogonal surface tangent vectors T and B are also called binormals . Block provides these values. An object space coordinate light vector projected onto this coordinate basis block is then used to compute texture coordinate displacements for bump mapping. More specifically the projection of the light direction vector onto each of the two binormals T and B gives the amount of texture space displacement the light causes. Basically the light on the texture i. e. the light direction vector is decomposed into its surface normal component and its s t coordinate components corresponding to the respective texture gradients. These s t coordinate components of the light direction vector are the s t texture coordinate displacements block used for bump mapping.

To perform the above operations properly for efficient rendering object oriented Tangent and Binormal vectors at each vertex which map in object space to the texture s and t axis are preferably first converted to eye space. Consequently in the example implementation of the present invention Command Processor supplies these two binormals per vertex to Transform Unit block . The Transform Unit will then transform the binormals to eye space block . For the present example embodiment even where the supplied binormals are constant for example with flat surfaces Command Processor supplies the binormals to Transform Unit on a per vertex basis. Mathematically the following operations are performed by Transform Unit are 

where Tv Txv Tyv Tzv and Bv Bxv Byv Bzv are the per vertex binormals supplied to Transform Unit by Command Processor . The Tv vector should preferably be normalized and aligned with the s texture axis in object space and the Bv vector should preferably be normalized and aligned with the t texture axis in object space. The Model View transformation matrix should be purely rotational which would maintain the unit length of the binormals. However if scaling of the binormals is required then the Model View transformation matrix can be multiplied by a scalar. The scale applied would then be the new unit length of the binormals. This could be used to visually increase the bump mapping effect without changing the source data or the algorithm.

Given the binormal basis system the light rotation matrix used by Transform Unit block is as follows 

where Tx Ty Tz is the transformed binormal oriented along the s axis in the direction of increasing s while Bx By Bz is the transformed binormal oriented along the t axis in the direction of increasing t.

The light vector is computed block by normalizing the difference between the light position in eye space and the current transformed vertex in eye space as follows 

The computed per vertex delta offsets s t are then added to the post transform i.e. after transform to eye space texture coordinate generated per vertex block to obtain new texture coordinates S and T 

To efficiently implement the above computation for emboss style bump mapping Transform Unit includes hardwired computational logic circuitry to perform at least the following emboss bump mapping related vector and coordinate computations Compute Compute Compute Compute L Compute T L Compute B L Compute 1 1 sqrt Compute Compute Compute 11 00 

where T and B are the respective object space Tangent and Binormal vectors MV is a transformation matrix having element values for converting vectors to eye space Lis the light position vector Vis the vertex position vector L is the light to vertex vector L is the normalized light direction vector S T are the regular transformed texture coordinates s t are the generated texture coordinate displacement values and S T are the new texture coordinates from which an offset texture used in emboss bump mapping is obtained.

Referring again to block outlines specific vector dot product processing hardware which may also be employed by Transform Unit in performing computations other than that related to emboss style bump mapping. While block outlines Transform Unit hardware more specifically useful in emboss bump mapping computations block hardware may also be useful in performing other functions. For emboss style bump mapping a first dot product computation unit computes the eye space transformation of the Tangent and Binormal vectors. The transformed results are temporarily stored in multiplexing staging buffer . A light to vertex vector computation is performed on vertex position vector data V and light position vector data L to provide light direction vector data L. A second dot product computation unit is utilized to compute in parallel the following 

The Lvector product is subsequently provided to inverse square root computation unit for computing an inverse magnitude value of the light direction vector. The Binormal and Tangent vector lighting dot products T L and B L from dot unit are provided to floating multiplier along with the computed inverse magnitude value of the light direction vector from unit . Floating point multiplier then computes the texture coordinate displacements S and T which are passed to floating point adder . Transformed texture coordinates S and T are provided per vertex to delay FIFO and are passed in a timely fashion to floating point adder for combination with computed coordinate displacements S and T. The new texture coordinates generated S and T are then passed to a vertex buffer unit not shown within transform unit and subsequently passed via graphics pipeline to texture unit for texture lookup. In the preferred embodiment the texture combining unit used is capable of performing texture subtraction in one pass instead of multiple passes. The preferred texture combining operation does not use an accumulation buffer but instead does texture combining in texture hardware.

Vector dot unit includes floating multipliers and and floating point adders and for computing vector dot products of the light direction vector and the Tangent and Binormal eye space vector components. Dot unit may also include multiplexor for receiving and staging light direction vector and transformed eye space Tangent and Binormal vector data from floating point adder and dot unit . Floating point multipliers through are used in combination with floating point adders and to provide a light direction vector squared product L a Tangent lighting vector dot product T L and a Binormal lighting dot product B L at the output of floating point adder . A table illustrating an example schedule of computational events for accomplishing emboss style bump mapping occurring per pipeline data clocking cycle stage within Transform Unit using dot unit and dot unit is provided immediately below 

During relative cycles stages numbered 1 through 8 the Tangent and vectors are loaded into dot unit and the transforms to eye space are During cycles through light direction vector components L L an Lare computed by floating point adder using eye space vertex on components and negative signed light position components. During cycles the computed Tangent vector eye space components are loaded into multiplexing staging buffer . During Cycle the computed light direction vector L and the computed Tangent eye space vector Teye T T T are loading into the vector dot unit for computing the T L dot product. On cycle the computed light direction vector L is again loaded into the vector dot unit to compute the light direction vector squared product L. Finally the binormal eye space vector Beye B B B is loaded on cycle to compute the B L dot product. The hardware described above is fully pipelined and can compute the required values in a minimal number of distinct operations.

In the preferred embodiment an enhanced graphics API function is used to initiate texture coordinate generation within transform unit . In addition to conventional texture coordinate generation wherein current vertex attribute information is used to generate a texture coordinate the preferred graphics API supports an enhanced texture generation function that is capable of calling and using other texture coordinate generation functions. An example enhanced API texture coordinate generation function may be defined as follows 

The above example API function defines general texture coordinate generation in addition to supporting other texture coordinate generation functions. The MatIdx is set as the default texture matrix index by which the generated texture coordinates are to be transformed. In the present example embodiment to implement emboss style bump mapping the above API function is used with Func set to GX TG BUMP where is a number from 0 7 indicative of one of up to eight possible different lights light source positions which may be selected for embossing.

The following is an example C C language implementation of the above general texture coordinate generation function 

With func set to GX TG BUMP system performs emboss style bump mapping by perturbing input texture coordinates based on per vertex specified binormals and light direction information. The original and offset texture coordinates are used to look up texels from a height field bump map texture stored in memory . TEV unit can be used to subtract these values in hardware in one pass to find the bump height which value can be added to the final color of the pixel to provide emboss style bump mapping. GX BUMP indicates that light will be used GX BUMP indicates that light will be used etc. in the bump map calculation.

The dst coord for bump maps should be numbered sequentially i.e. base texture coordinate n and bump offset texture coordinate n 1. Bump map texture coordinates should be generated after coordinates generated from transforms GX TG MTX24 and GX TG MTX34 and before coordinates generated from lighting channels GX TG SRTG . An example follows 

 source for a bump mapped coordinate transformed by a matrix GXSetTexCoordGen GX TEXCOORD GX TG MTX24 GX TG TEX GX TEXMTX 

 perturbed coordinate offset from TEXCOORD above light . Matrix mtx is not used for the perturbed coordinates therefore use an identity matrix .

Certain of the above described system components could be implemented as other than the home video game console configuration described above. For example one could run graphics application or other software written for system on a platform with a different configuration that emulates system or is otherwise compatible with it. If the other platform can successfully emulate simulate and or provide some or all of the hardware and software resources of system then the other platform will be able to successfully execute the software.

As one example an emulator may provide a hardware and or software configuration platform that is different from the hardware and or software configuration platform of system . The emulator system might include software and or hardware components that emulate or simulate some or all of hardware and or software components of the system for which the application software was written. For example the emulator system could comprise a general purpose digital computer such as a personal computer which executes a software emulator program that simulates the hardware and or firmware of system .

Some general purpose digital computers e.g. IBM or MacIntosh personal computers and compatibles are now equipped with 3D graphics cards that provide 3D graphics pipelines compliant with DirectX or other standard 3D graphics command APIs. They may also be equipped with stereophonic sound cards that provide high quality stereophonic sound based on a standard set of sound commands. Such multimedia hardware equipped personal computers running emulator software may have sufficient performance to approximate the graphics and sound performance of system . Emulator software controls the hardware resources on the personal computer platform to simulate the processing 3D graphics sound peripheral and other capabilities of the home video game console platform for which the game programmer wrote the game software.

As one example in the case where the software is written for execution on a platform using an IBM PowerPC or other specific processor and the host is a personal computer using a different e.g. Intel processor emulator fetches one or a sequence of binary image program instructions from storage medium and converts these program instructions to one or more equivalent Intel binary image program instructions. The emulator also fetches and or generates graphics commands and audio commands intended for processing by the graphics and audio processor and converts these commands into a format or formats that can be processed by hardware and or software graphics and audio processing resources available on host . As one example emulator may convert these commands into commands that can be processed by specific graphics and or or sound hardware of the host e.g. using standard DirectX OpenGL and or sound APIs .

An emulator used to provide some or all of the features of the video game system described above may also be provided with a graphic user interface GUI that simplifies or automates the selection of various options and screen modes for games run using the emulator. In one example such an emulator may further include enhanced functionality as compared with the host platform for which the software was originally intended. In the case. where particular graphics support hardware within an emulator does not include the embossed bump mapping functions shown in the emulator designer has a choice of either 

While the flowchart can be implemented entirely in software entirely in hardware or by a combination of hardware and software the preferred embodiment performs most of these calculations in hardware to obtain increased speed performance and other advantages. Nevertheless in other implementations e.g. where a very fast processor is available the computations and steps of may be implemented in software to provide similar or identical imaging results.

A number of program modules including emulator may be stored on the hard disk removable magnetic disk optical disk and or the ROM and or the RAM of system memory . Such program modules may include an operating system providing graphics and sound APIs one or more application programs other program modules program data and game data. A user may enter commands and information into personal computer system through input devices such as a keyboard pointing device microphones joysticks game controllers satellite dishes scanners or the like. These and other input devices can be connected to processing unit through a serial port interface that is coupled to system bus but may be connected by other interfaces such as a parallel port game port Fire wire bus or a universal serial bus USB . A monitor or other type of display device is also connected to system bus via an interface such as a video adapter .

System may also include a modem or other network interface means for establishing communications over a network such as the Internet. Modem which may be internal or external is connected to system bus via serial port interface . A network interface may also be provided for allowing system to communicate with a remote computing device e.g. another system via a local area network or such communication may be via wide area network or other communications path such as dial up or other communications means . System will typically include other peripheral output devices such as printers and other standard peripheral devices.

In one example video adapter may include a 3D graphics pipeline chip set providing fast 3D graphics rendering in response to 3D graphics commands issued based on a standard 3D graphics application programmer interface such as Microsoft s DirectX 7.0 or other version. A set of stereo loudspeakers is also connected to system bus via a sound generating interface such as a conventional sound card providing hardware and embedded software support for generating high quality stereophonic sound based on sound commands provided by bus . These hardware capabilities allow system to provide sufficient graphics and sound speed performance to play software stored in storage medium .

While the invention has been described in connection with what is presently considered to be the most practical and preferred embodiment it is to be understood that the invention is not to be limited to the disclosed embodiment but on the contrary is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims.

