---

title: Low power microprocessor cache memory and method of operation
abstract: Techniques for processing transmissions in a communications (e.g., CDMA) system including the use of a digital signal processor. The digital signal processor includes a cache memory system and associates a plurality of cache memory match lines with addressable memory lines of an addressable memory. Each of the cache memory match lines associates with one of corresponding sets of the cache memory. The method and system maintain each of the cache memory match lines at a low voltage. Once the digital signal processor initiates a search of the cache memory for retrieving data from a selected one of the corresponding sets of the cache memory, a match line drive circuit drives one of the cache memory match lines from a low voltage to a high voltage. The selected one of the cache memory match lines corresponds to the selected one of the corresponding sets of the cache memory. The digital signal processor compares the selected one of the cache memory match lines to an associated one of the addressable memory lines. Following the comparison step, the process returns the one of the cache memory match lines to the low voltage.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07620778&OS=07620778&RS=07620778
owner: QUALCOMM Incorporated
number: 07620778
owner_city: San Diego
owner_country: US
publication_date: 20050525
---
The disclosed subject matter relates to data communications. More particularly this disclosure relates to a novel and improved low power microprocessor cache memory and method of operation in a digital signal processor.

A modern day communications system must support a variety of applications. One such communications system is a code division multiple access CDMA system that supports voice and data communication between users over a satellite or terrestrial link. The use of CDMA techniques in a multiple access communication system is disclosed in U.S. Pat. No. 4 901 307 entitled SPREAD SPECTRUM MULTIPLE ACCESS COMMUNICATION SYSTEM USING SATELLITE OR TERRESTRIAL REPEATERS and U.S. Pat. No. 5 103 459 entitled SYSTEM AND METHOD FOR GENERATING WAVEFORMS IN A CDMA CELLULAR TELEHANDSET SYSTEM both assigned to the assignee of the claimed subject matter.

A CDMA system is typically designed to conform to one or more standards. One such first generation standard is the TIA EIA IS 95 Terminal Base Station Compatibility Standard for Dual Mode Wideband Spread Spectrum Cellular System hereinafter referred to as the IS 95 standard. The IS 95 CDMA systems are able to transmit voice data and packet data. A newer generation standard that can more efficiently transmit packet data is offered by a consortium named 3Generation Partnership Project 3GPP and embodied in a set of documents including Document Nos. 3G TS 25.211 3G TS 25.212 3G TS 25.213 and 3G TS 25.114 which are readily available to the public. The 3GPP standard is hereinafter referred to as the W CDMA standard.

Digital signal processors DSPs may find valuable use in wireless handsets and other electronic devices that comply with the above standards. In particular wireless handsets and such devices are requiring and increasingly will require that the DSP execute instructions from user orientation scientific and multimedia applications as well as many other types of applications. The processor resource requirements may vary widely and dynamically for applications such as television broadcasts streaming message tickers electronic mail including messages with attached documents as well as resident applications such as photography and PDA applications all from the same DSP.

In wireless handsets and similar communications devices there is the need to extend the battery life. Energy conservation therefore presents a critical consideration in the operating characteristics of such devices. With this consideration in mind it is important to consider that memory and data cache resources constitute a major portion of the wireless handset electrical load. In fact the power to operate data and instruction caches represents as much as 30 of the total power consumption of a wireless handset microprocessor.

In cache memory devices there is the need to conserve energy both dynamically and statically. That is there is the need to conserve energy in the use of a cache memory device at all times of DSP operation and even when no direct access to the cache memory occurs i.e. during static operation. Also there is the need to conserve energy when the DSP accesses the cache memory i.e. during dynamic operation. One approach to saving energy in the use of cache memories for certain low power processors implements a cache lookup or searching operations serially.

In a serial cache lookup a hit condition is determined prior to data access. The hit condition may be determined by accessing a content addressable memory CAM based tag. A CAM based tag is organized into selectable sets so that the cache tag value to match is only presented to the selected set. The selected cache set is determined by a set index of the accessed address of main memory. So if a hit will occur the CAM based tag specifies and activates a match line corresponding to the hit location.

By selectably identifying a match line a reduction in energy consumption occurs for the serially addressable cache memory as compared to the parallel addressable cache memory. This is because parallel addressable cache memories fire all cache ways simultaneously during dynamic operation. Since all cache memory sets fire during parallel addressable cache operation those sets not matching or containing valid content fire unnecessarily. The result is unnecessary energy loss.

In both known serially and parallel addressable cache memories match lines are always pre charged. CAM based cache configurations include match lines that are pre charged in the pre charge phase. The match lines remain high in the evaluate phase on a match or hit and are discharged on a mismatch or miss. In such circuits a foot or sleep transistor may be used to reduced leakage currents that arise during both static and dynamic cache memory operation. While this approach may reduce some energy leakage the existence of a pre charge on all match lines yet results in energy loss. Moreover in cache memories that employ a sleep transistor otherwise undesirable impedance exists in the cache memory circuit.

Accordingly a need exists for a cache memory circuit for operation in a DSP or similar electronic circuit that avoids both static and dynamic energy losses during DSP operation.

A further need exists for a DSP that avoids the use of a foot or sleep transistor and the concomitant inefficiencies and leakage that may occur in associating such an element with a cache memory circuit.

Techniques for making and using a low power microprocessor cache memory and method of operation are disclosed which techniques improve both the operation of the associated digital signal processor and promote the energy efficient use of memory circuitry associated with such digital signal processors and similar component. Such techniques may provide beneficial results in a variety of applications such as personal computers personal digital assistants wireless handsets and similar electronic devices.

According to one aspect of the disclosed subject matter there is provided a method and a system for providing a low power cache memory circuit for a digital signal processor that associates the cache memory with a plurality of cache memory match lines. The cache memory match lines associate with addressable memory lines of an addressable memory. Each of the cache memory match lines associates with one of corresponding sets of the cache memory. The method and system maintain each of the cache memory match lines at a low voltage. Once the digital signal processor initiates a search of the cache memory for retrieving data from a selected one of the corresponding sets of the cache memory a match line drive circuit drives one of the cache memory match lines from a low voltage to a high voltage. The selected one of the cache memory match lines corresponds to the selected one of the corresponding sets of the cache memory. The digital signal processor compares the selected one of the cache memory match lines to an associated one of the addressable memory lines. Following the comparison step the process returns the one of the cache memory match lines to the low voltage.

These and other aspects of the disclosed subject matter as well as additional novel features will be apparent from the description provided herein. The intent of this summary is not to be a comprehensive description of the claimed subject matter but rather to provide a short overview of some of the subject matter s functionality. Other systems methods features and advantages here provided will become apparent to one with skill in the art upon examination of the following FIGURES and detailed description. It is intended that all such additional systems methods features and advantages that are included within this description be within the scope of the accompanying claims.

At a receiver unit the transmitted signal is received by an antenna and provided to a receiver RCVR . Within receiver the received signal is amplified filtered down converted demodulated and digitized to generate in phase I and Q samples. The samples are then decoded and processed by a receive RX data processor to recover the transmitted data. The decoding and processing at receiver unit are performed in a manner complementary to the coding and processing performed at transmitter unit . The recovered data is then provided to a data sink .

The signal processing described above supports transmissions of voice video packet data messaging and other types of communication in one direction. A bi directional communications system supports two way data transmission. However the signal processing for the other direction is not shown in for simplicity.

Communications system can be a code division multiple access CDMA system a time division multiple access TDMA communications system e.g. a GSM system a frequency division multiple access FDMA communications system or other multiple access communications system that supports voice and data communication between users over a terrestrial link. In a specific embodiment communications system is a CDMA system that conforms to the W CDMA standard.

IQ in IU keeps a sliding buffer of the instruction stream. Each of the six threads T T that DSP supports has a separate eight entry IQ where each entry may store one VLIW packet or up to four individual instructions. Decode and issue circuitry logic is shared by all threads for decoding and issuing a VLIW packet or up to two superscalar instructions at a time as well as for generating control buses and operands for each pipeline SLOT SLOT. In addition decode and issue circuitry does slot assignment and dependency check between the two oldest valid instructions in IQ entry for instruction issue using for example using superscalar issuing techniques. PLC logic is shared by all threads for resolving exceptions and detecting pipeline stall conditions such as thread enable disable replay conditions maintains program flow etc.

In operation general register file GRF and control register file CRF of selected thread is read and read data is sent to execution data paths for SLOT SLOT. SLOT SLOT in this example provide for the packet grouping combination employed in the present embodiment. Output from SLOT SLOT returns the results from the operations of DSP .

The present embodiment therefore may employ a hybrid of a heterogeneous element processor HEP system using a single microprocessor with up to six threads T T. Processor pipeline has six pipeline stages matching the minimum number of processor cycles necessary to fetch a data item from IU . DSP concurrently executes instructions of different threads T T within a processor pipeline . That is DSP provides six independent program counters an internal tagging mechanism to distinguish instructions of threads T T within processor pipeline and a mechanism that triggers a thread switch. Thread switch overhead varies from zero to only a few cycles.

Turning to the present micro architecture for DSP includes control unit CU which performs many of the control functions for processor pipeline . CU schedules threads and requests mixed 16 bit and 32 bit instructions from IU . CU furthermore schedules and issues instructions to three execution units shift type unit SU multiply type unit MU and load store unit DU . CU also performs superscalar dependency checks. Bus interface unit BIU interfaces IU and DU to a system bus not shown .

SLOT and SLOT pipelines are in DU SLOT is in MU and SLOT is in SU . CU provides source operands and control buses to pipelines SLOT SLOT and handles GRF and CRF file updates. GRF holds thirty two 32 bit registers which can be accessed as single registers or as aligned 64 bit pairs. Micro architecture features a hybrid execution model that mixes the advantages of superscalar and VLIW execution. Superscalar issue has the advantage that no software information is needed to find independent instructions. A register file pipeline stage RF provides for registry file updating. Two execution pipeline stages EX and EX support instruction execution while a third execution pipeline stage EX provides both instruction execution and register file update. During the execution EX EX and EX and writeback WB pipeline stages IU builds the next IQ entry to be executed. Finally writeback pipeline stage WB performs register update. The staggered write to register file operation is possible due to IMT micro architecture and saves the number of write ports per thread. Because the pipelines have six stages CU may issue up to six different threads.

DU executes load type store type and 32 bit instructions from ALU . The major features of the DU include fully pipelined operation in all of DSP pipeline stages RF EX EX EX and WB pipeline stages using the two parallel pipelines of SLOT and SLOT. DU may accept either VLIW or superscalar dual instruction issue wherein preferably SLOT executes uncacheable or cacheable load or store instructions 32 bit ALU instructions and DCU instructions. SLOT executes uncacheable or cacheable load instructions and 32 bit ALU instructions.

DCU provides a physically tagged multi way and employs a serial tag data lookup. DU operation includes pseudo dual ported supporting simultaneous access from SLOT and SLOT in a 16 way set associative architecture. With a 32 KB capacity and 32 bit line size DU may be shared among all six threads T T. Moreover DU provides a not recently used replacement policy together with an inter thread non blocking operation. With page configurable write through and write back and cache locking by individual lines DU provides an 8 KB tightly coupled memory TCM in one embodiment.

The following description details certain further features and functions of DCU . DU executes cache instructions for managing data cache functions of DCU . Cache instructions allow specific cache lines to be locked and unlocked invalidated and allocated to a GRF specified cache line. There is also an instruction to globally invalidate the cache store retrieve instructions. These instructions are pipelined similar to the load and store instructions. For loads and stores to cacheable locations that miss the data cache and for uncacheable accesses DU presents requests to BIU . Load misses present a line fill request. Uncacheable loads present a read request. Store hits misses and uncacheable stores present a write request.

DU tracks outstanding read and line fill requests to BIU . BIU directs the data received for these request to DU . DU provides non blocking inter thread operations that allow accesses by other threads while one or more threads are blocked pending completion of outstanding load requests. Since DSP is an IMT machine. The usual load use pipeline distance criticality of single thread pipelined machines does not apply. If a load updates the GRF the earliest next use may be six cycles later in order to satisfy the shortest possible load use requirements. Therefore DCU performs a serial tag look up followed by data access. This saves energy by only accessing the cache line that hit in the data array.

DU in other words receives up to two decoded instructions per cycle including immediate operands from CU via decode and issue circuit . In RF pipeline stage DU receives GRF and or CRF source operands from the appropriate thread specific registers. The GRF operand is received from the GRF . In EX pipeline stage DU generates the effective address EA of a load or store memory instruction. EA signals are presented to the MMU which performs the virtual to physical address translation and page level permissions checking and provides page level attributes. For accesses to cacheable locations DU looks up the data cache tag in EX pipeline stage with the physical address. If the access hits DU performs the data array access EX pipeline stage .

For cacheable loads the data read out of DCU is aligned by the appropriate access size zero sign extended as specified and driven to the CU in WB pipeline stage . Thereupon the data may be written into the instruction specified GRF file. For cacheable stores the data to be stored is read out of the thread specific register in the CU in EX pipeline stage and written into the data cache array on a hit in EX pipeline stage . For both loads and stores auto incremented addresses are generated in the EX pipeline stage and EX pipeline stage . Then loads and stores may be driven to CU in EX pipeline stage further to be written into the instruction specified GRF file.

When executing ALU instructions DU receives a GRF operand in RF pipeline stage and a GRF operand in EX pipeline stage . ALU arithmetic and compare instructions operate in EX pipeline stage and the results go to CU in EX pipeline stage for writeback. ALU instructions involving byte shifting and sign zero extension and use the load aligner in WB pipeline stage . ALU subsequently sends the result to CU .

Although some instructions require two register reads e.g. store instructions and certain some ALU instructions or two register writes e.g. auto incremented load instructions CU only provides a single register read and a single register write port for DU per thread. Since DSP provides an IMT machine the single read port may be time multiplexed for reads in RF pipeline stage and EX pipeline stage and the single write port may be time multiplexed for writes in EX pipeline stage and WB pipeline stage .

The serial tag data access of DCU also enables stores to access the data array in EX pipeline stage as loads since hit location is resolved in the prior EX pipeline stage . Thus the usual single thread store pipelining solutions such as separate store ports store buffering or store to load forwarding are not required here for stall free pipelining of stores.

The relaxed timing constraint on the hit determination also allows SRAM data array of DCU to be highly e.g. way set associative which is suited to a six way IMT machine. This minimizes cache conflicts among private data of each thread. SRAM data array is shared among all the threads to leverage capacity efficiency of shared data. In addition the shared SRAM data array cache also removes the need for cache coherence mechanisms to maintain coherence among multiple shared copies.

Within the operation of DCU the disclosed subject matter provides for selective pre charge of CAM tag array the match lines to save both static and dynamic power consumption. The present embodiment does not pre charge the match lines when it is known that the associated set within CAM tag array is not being accessed. The match lines are also not pre charged when it is known that a particular set within SRAM data array does not contain a valid line.

Thus the present embodiment provides a selective pre charge based on two factors. The first factor is set selection and the second factor is valid state. In addition to CAM tag array DCU provides separate SRAM state array which maintains an index address. Each entry contains the valid invalid flag for every set of CAM tag array . Each match line is only pre charged in the event that the associated set is selected and the set is valid.

By not pre charging the match line of CAM tag array leakage is eliminated during static operation. In addition dynamic power is also saved because for traditional content addressable memories the pre charge is gated with the operation of the clock cycles. With the disclosed subject matter the clock is gated so that it only sees loading from the pre charged match lines. Consequently dynamic loading of the DSP and the associated power or energy consumption is also substantially reduced.

With continuing reference to the follow details the pipeline operation of DCU during CAM tag array and related operations. Serial tag data access operations enable stores to access SRAM data array in the same EX pipeline stage as loads since hit location is resolved in EX pipeline stage . Thus the usual single thread store pipelining solutions such as separate store ports store buffering or store to load forwarding are not required here for stall free pipelining of stores. The relaxed timing constraint on the hit determination also allows DCU to be highly e.g. 16 way set associative.

Match line therefore is kept low during DSP operation except when the set is been selected for access. This is accomplished using the inverse of the Set vld signal from inverter as input to NFET which pulls down match line . NFET may be small size in order to pull down match line .

The present embodiment provides dummy match line which generates a trigger signal of the same value as match line . Holder circuit protects the trigger signal from fading before the evaluation of CAM tag array . Fading may occur when Set vld transitions from low to high too soon resulting in a minimum delay or when the Set vld signal transitions from high to low too soon causing dummy match line to be pulled low. On the other hand there is the need to shut off the pull down of the match line as soon as the Set vld rises. This prevents power contention with the pre charge circuit. As such this requires turning off the NFET pull down voltage as quickly as possible with the rise of Set vld. However it is preferable to a delay in turning on NFET . Therefore the input to NFET pull down is an output of NOR gate with one input of the NOR gate being delayed.

CAM tag array includes match lines one for each set selectable by Set vld circuitry. With the present embodiment the transition from low to high voltage occurs to pre charge match line high only just before the search line is driven. Delay path from match line to PFET pre charge will turn off PFET but the smaller PFET will keep match line high.

Now a special case may occur when Set vld signal stays high for the evaluation of CAM tag array . Accordingly it is preferable to pre charge only in the event that Set vld is high for two consecutive cycles. On the other hand the pre charge should start as soon as Set vld rises. The pre charge should also terminate as soon as match line is at Vto prevent contention with the CAM tag array pulldown. To satisfy all these requirements data cache match line circuit includes buffer circuits and NOR gate . The delay of buffer circuit equals the pre charge time e.g. 200 300 ps while the delay of buffer circuit may be close to phase durations.

Since match line is discharged low no leakage occurs from any of the 40 960 40 64 array size 16 match lines traditional leaking paths. Also dynamic power losses are reduced since there is no clock loading for addressing sets within CAM tag array . Instead the clock is used only in the dummy row to gate the trigger signal. One embodiment of the disclosed subject matter may eliminate this clock function if the clock is used on the wordline driver.

At rising clock RAM wordline voltage and RAM bitline pre charge voltage go high. RAM wordline voltage remains for 250 ps. Nominally 400 ps after RAM wordline voltage goes high saen voltage goes high. In response to sean voltage rising RAM wordline voltage goes low. Also at falling clock saen voltage and RAM bitline pre charge voltage return low. A second rise of search voltage occurs during the fall of RAM wordline voltage and saen voltage . Thus after two clock cycles search line voltage remains high and all other voltages return to low voltage.

The disclosed subject matter therefore maintains match line at a low or zero voltage. Only when data from a CAM tag array cache set is needed will match line be charged. Once match line is charged DCU and thus DSP obtains the desired data. Then match line voltage returns to low voltage. The result is a reduction or elimination of dynamic power losses by reducing clock loading to zero. That is there is no need to obtain a signal load from the clock signal. The decode logic that determines the cache block from which data or instructions are sought provides the selective pre charge for match line . The result is that the selective pre charge of the present embodiment provides both the decode signal for selecting the specific cache block as well as the match line charge signal for charging the match line.

The processing features and functions described herein can be implemented in various manners. For example not only may DSP perform the above described operations but also the present embodiments may be implemented in an application specific integrated circuit ASIC a microcontroller a microprocessor or other electronic circuits designed to perform the functions described herein. The foregoing description of the preferred embodiments therefore is provided to enable any person skilled in the art to make or use the claimed subject matter. Various modifications to these embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments without the use of the innovative faculty. Thus the claimed subject matter is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.

