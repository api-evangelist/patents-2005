---

title: System and method for flexible physical-logical mapping raid arrays
abstract: A system, method and computer program for allocating physical memory from a group of N memory devices to logical volumes. A group of N memory devices are partitioned into a plurality of bands, each of the group of N memory devices sharing a portion of each of the plurality of bands. A cluster map for each of the plurality of bands is generated. The cluster maps indicate the physical address for each of a plurality of clusters. Each of the plurality of clusters are distributed equally over two or more of the N memory devices to ensure a specified level of redundancy for each of the plurality of bands. Each of the N memory devices share an approximately equal number of clusters. Available bands are determined and are allocated to a logical volume.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07694072&OS=07694072&RS=07694072
owner: Xyratex Technology Limited
number: 07694072
owner_city: Havant, Hampshire
owner_country: GB
publication_date: 20050922
---
This application claims the benefit of U.S. Provisional Application Ser. No. 60 611 802 filed Sep. 22 2004 in the U.S. Patent and Trademark Office the entire content of which is incorporated by reference herein.

The present invention relates to allocation of physical resources for logical volumes in redundant arrays of inexpensive disk RAID arrays. Specifically a system and method for assigning physical address space to logical data blocks is presented wherein data space availability and system management flexibility are increased.

Currently redundant arrays of inexpensive disk RAID arrays are the principle storage architecture for large networked computer storage systems. RAID architecture was first documented in 1987 when Patterson Gibson and Katz published a paper entitled A Case for Redundant Arrays of Inexpensive Disks RAID University of California Berkeley . Fundamentally RAID architecture combines multiple small inexpensive disk drives into an array of disk drives that yields performance exceeding that of a Single Large Expensive Drive SLED . Additionally the array of drives appears as a single logical storage unit LSU or drive. Five types of array architectures designated as RAID 1 through RAID 5 were defined by the Berkeley paper each type providing disk fault tolerance and offering different trade offs in features and performance. In addition to the five redundant array architectures a non redundant array of disk drives is referred to as a RAID 0 array. RAID controllers provide data integrity through redundant data mechanisms high speed through streamlined algorithms and accessibility to stored data for users and administrators.

A networking technique that is fundamental to the various RAID levels is striping a method of concatenating multiple drives into one logical storage unit. Striping involves partitioning each drive s storage space into stripes which may be as small as one sector 512 bytes or as large as several megabytes. These stripes are then interleaved in round robin style so that the combined space is composed alternately of stripes from each drive. In effect the storage space of the drives is shuffled like a deck of cards. The type of application environment I O or data intensive determines whether large or small stripes should be used. The choice of stripe size is application dependant and affects the real time performance of data acquisition and storage in mass storage networks. In data intensive environments and single user systems which access large records small stripes typically one 512 byte sector in length can be used so that each record will span across all the drives in the array each drive storing part of the data from the record. This causes long record accesses to be performed faster because the data transfer occurs in parallel on multiple drives. Applications such as on demand video audio medical imaging and data acquisition which utilize long record accesses will achieve optimum performance with small stripe arrays.

Hosts A through N are representative of any computer systems or terminals that are capable of communicating over a network. Communication means is representative of any type of electronic network that uses a protocol such as Ethernet. RAID controllers A through N are representative of any storage controller devices that process commands from hosts A through N and based on those commands control memory devices A through N. RAID controllers A through N also provide data redundancy based on system administrator programmed RAID levels. Redundancy methods include data mirroring parity generation and or data regeneration from parity after a device failure. Communication means is any type of storage controller network such as iSCSI or fibre channel. Memory devices A through N may be any type of storage device such as for example tape drives disk drives non volatile memory or solid state devices. Although most RAID architectures use disk drives as the main storage devices it should be clear to one skilled in the art that the invention embodiments described herein apply to any type of memory devices.

In operation host A for example generates a read or a write request for a specific volume e.g. volume 1 to which it has been assigned access rights. The request is sent through communication means to the host ports of RAID controllers A through N. The command is stored in local cache in RAID controller B for example because RAID controller B is programmed to respond to any commands that request volume 1 access. RAID controller B processes the request from host A and determines from mapping tables the first physical memory device A through N address from which to read data or to write new data. If volume 1 is a RAID 5 volume and the command is a write request RAID controller B generates new parity stores the new parity to a parity memory device A through N via communication means sends a done signal to host A via communication means and writes the new host A data through communication means to corresponding memory devices A through N. As a result data is less susceptible to loss from memory device A through N failures and generally can be restored from parity and or functional memory devices A through N in the event of a failure. Any one of RAID controllers A through N also have the ability to take over control for a failed RAID controller A through N such that system performance is unaffected or the effects are limited.

The operation of most standard RAID controllers is set at the Application Programming Interface API level. Typically Original Equipment Manufactures OEMs bundle RAID networks and sell these memory systems to end users for network storage. OEMs bear the burden of customization of a RAID network and tune the network performance through an API. However the degree to which a RAID system can be optimized through the API is limited API does not adequately handle the unique performance requirements of various dissimilar data storage applications. Additionally API does not provide an easily modifiable and secure format for proprietary OEM RAID configurations.

There is therefore a need for a RAID controller that has the capability to be adequately programmed for unique performance and data storage requirements. Furthermore the RAID controller configuration should be easily modifiable by a user or system administrator. The general functions of the RAID controller such as volume allocation should be optimized to use fewer processing resources in order to increase overall system performance. Finally the RAID controller needs to allocate physical storage space to logical volumes in such a way that the majority of the storage capacity is utilized.

An example RAID controller with a mapping function for allocating physical disk space to logical volumes is described in U.S. Patent Application Publication No. 2003 0028727. The 727 application entitled RAID Apparatus for Storing a Plurality of Same Logical Volumes on Different Disk Units describes a RAID apparatus that has a plurality of same logical volumes allocated on a real volume. The real volume is designed so that a plurality of same logical volumes are respectively allocated on different physical disk units and a combination of a plurality of logical volumes allocated on each physical disk unit differs from one physical disk unit to another. This structure prevents uneven loading on the real volume from occurring because of uneven loads on the logical volumes.

The 727 application identifies the problem of physical disk device load balancing in a RAID architecture and offers a solution allocating physical disk space such that equivalent logical volumes reside on separate physical disks for load balancing optimization. However the 727 application fails to provide an effective means to allocate volumes to physical storage devices such that there is greater flexibility in system design. Furthermore the 727 application does not provide a means for mapping logical volumes to physical storage space with fewer processing cycle requirements. Finally the 727 application does not provide a means for utilizing a greater amount of available space of each storage device as compared to conventional methods.

It is therefore an object of this invention to provide a system and method for assigning physical storage space in a RAID array such that maximum system flexibility is available to the administrator s .

It is another object of the invention to provide a system and method for assigning physical storage space in a RAID array such that fewer processing cycles are needed to maintain mapping information when a new volume is created.

It is yet another object of this invention to provide a system and method for assigning physical storage space in a RAID array such that more data storage capacity is available.

The present invention provides a method and a computer program are provided for allocating physical memory from a group of N memory devices to logical volumes. The method and program include the step of partitioning the group of N memory devices into a plurality of bands each of the group of N memory devices sharing a portion of each of the plurality of bands. A cluster map for each of the plurality of bands is generated. The cluster maps indicate the physical address for each of a plurality of clusters. Each of the plurality of clusters are distributed equally over two or more of the N memory devices to ensure a specified level of redundancy for each of the plurality of bands. Each of the N memory devices share an approximately equal number of clusters. Available bands are determined and are allocated to a logical volume.

The present invention also provides a system for allocating physical memory to logical volumes. The system includes a group of N memory devices partitioned into a plurality of bands. Each of the group of N memory devices share a portion of each of the plurality of bands. Each of the plurality of bands has a cluster map. Each cluster map indicates the physical address for each of a plurality of clusters. Each of the plurality of clusters are equally distributed over two or more of the N memory devices to ensure a specified level of redundancy for each of the plurality of bands. Each of the N memory devices share an approximately equal number of clusters. An array controller is also configured to determine if a band from the plurality of bands is available and to allocate an available band to a logical volume.

These and other aspects of the invention will be more clearly recognized from the following detailed description of the invention which is provided in connection with the accompanying drawings.

The present invention is a method of allocating physical storage space to logical unit numbers LUNs or volumes that use a RAID controller. The method provides greater flexibility to the system administrator through the RAID controller by systematically assigning various portions of physical space to single or multiple logical device groups. Each device group has specific rules for data usage and allocation. Each device group is further categorized into single or multiple sub device groups. A special algorithm in the RAID controller arranges physical storage device space into logical units or bands that are readily allocated with little metadata overhead per system administrator commands. The physical space is allocated to logical volumes according to system administrator specifications.

GUI is a software application used to input personality attributes for RAID controllers . GUI runs on PC . RAID controllers are representative of RAID storage controller devices that process commands from hosts A through N and based on those commands control memory devices A through N see . As shown in RAID controllers are an exemplary embodiment of the invention however other implementations of controllers may be envisioned here by those skilled in the art. RAID controllers provide data redundancy based on system administrator programmed RAID levels. This includes data mirroring parity generation and or data regeneration from parity after a device failure. RAID controller hardware is the physical processor platform of RAID controllers that executes all RAID controller software applications and consists of a microprocessor memory and all other electronic devices necessary for RAID control. Operating system is an industry standard software platform such as Linux for example upon which software applications can run. Operating system delivers other benefits to RAID controllers . Operating system contains utilities such as a file system that provide a way for RAID controllers to store and transfer files. Software applications include algorithms and logic necessary for the RAID controllers and are divided into those needed for initialization and those that operate at run time. Initialization software applications consist of the following software functional blocks CIMOM which is a module that instantiates all objects in software applications with the personality attributes entered SAL which is the application layer upon which the run time modules execute and LAL a library of low level hardware commands used by a RAID transaction processor.

Software applications that operate at run time include the following software functional blocks system manager a module that carries out the run time executive SWD a module that provides software supervision function for fault management PDM a module that handles the personality data within software applications EM a task scheduler that launches software applications under conditional execution and BBU a module that handles power bus management for battery backup.

SM is responsible for allocating physical space to newly requested volumes and adding physical space to existing volumes when new devices are added to the system. SM tales commands from the system administrator e.g. assigning new volumes or creating new sub device groups and executes those commands. Commands that cannot be processed because of lack of space available for example are returned as error messages to the system administrator. The volume allocation function of SM is described in more detail in .

A sub device group may include from one to sixteen physical devices however all devices must be the same class of storage. The class of storage is defined by the system administrator. It may be based on the types of devices in sub device group such as fibre channel or serial ATA or based on physical characteristics such as rotation speed or size or based on logical considerations such as function department or user. At system installation SM defaults all physical devices to the same class of storage. After installation the system administrator may define new classes of storage.

SM further divides each storage sub device group into bands which are the smallest unit of logical storage assigned to a logical volume . By categorizing the storage area in such a manner the granularity of each storage unit allows more physical space to be utilized. Table 1 shows an example of bands that stripe across all the devices within a sub device group . There are n number of bands in sub device group depending on the capacity of each device.

Each band may be assigned to RAID 0 or RAID 5. There are three band formats master volume data mirror volume data and snap volume data. A band may be assigned to contain master volume data mirror volume data or snap volume data as defined below.

The master volume data band format is used when space is allocated to a master volume e.g. volume . The master volume may include one or more bands however all bands in that volume must be in the same sub device group e.g. . The amount of user space within a band varies depending on the RAID level. The data band may be configured for either RAID level 0 or 5.

When space is allocated as a mirror to a master volume the mirror band format is used. A mirror volume may include one or more bands but all mirror bands associated with a master volume must be in a different sub device group e.g. sub device group than the bands used for the master volume. The amount of user space within a band varies depending on the RAID level. The mirror band may be configured for either RAID level 0 or 5 and is not required to be the same RAID level as the master volume.

The snap band format is used when space is allocated for a point in time copy of a master volume. The snap volume may include one or more bands and all snap bands associated with a master volume may be in the same or different sub device group. The amount of user space within a band varies depending on the RAID level. The snap band may be configured for either RAID level 0 or 5 and is not required to be the same RAID level as the master volume.

Bands are expanded through the addition of devices to the sub device group in which the bands reside. At anytime after sub device group is created it may be expanded through addition of one or more devices to sub device group . After the devices are added SM migrates the existing bands to use the added devices. When the migration is complete sub device group will include additional bands that may then be allocated to new or existing logical volumes .

Table 2 shows an example of a redundancy group RGrp mapping for various numbers integer power of two only of devices in a sub device group for RAID 0 no parity device is required for a single band. Each band is further sub divided into a plurality of RGrps depending on the type of RAID level defined by the system administrator and the number of devices within a sub device group . RGrp describes the RAID level stripe size number of devices device path used and location of the data within sub device group . The number of RGrps assigned to sub device group must be an integer power of two for RAID 0 and an integer power of two plus one additional device for RAID 5 for parity data .

Table 3 shows an example of an RGrp mapping of RGrps for integer power of two plus one sub device groups for RAID 5 for parity data for a single band in sub device group . The number of RGrps assigned to sub device group must be an integer power of two plus one additional device for RAID 5 for parity data .

Table 4 shows an example of an RGrp mapping of RGrps for a RAID 0 band in sub device group that does not include an integer power of two number of devices.

In this example rotating RGrps RGrp1 RGrp2 RGrp3 RGrp4 RGrp5 RGrp6 and RGrp7 are used to map band. The number of RGrps required to map the entire band is equal to the number of devices within any sub device group . For example in Table 4 there are seven RGrps required to map a RAID 0 band in sub device group that includes seven devices. Each RGrp is striped across the devices such that there is an integer power of two number of devices e.g. 2 4 8 and so on for RAID 0 with a specific RGrp and no device has two stripes of the same RGrp. For example the seven disk sub device group in Table 4 cannot use eight devices for rotating a specific RGrp because Device would contain two stripes of RGrp1. The next available choice is four integer power of 2 which satisfies the RGrp assignment rules by rotating onto four devices RGrp1 before beginning a new RGrp RGrp2 .

Table 5 shows an example of a rotating RGrp mapping for sub device groups in RAID 5 band that do not equal integer powers of two plus one devices for parity e.g. 3 5 9 and so on .

Table 5 outlines the process for band RGrp mapping in a RAID 5 level that does not include an integer power of two number of devices plus a parity device in sub device groups . As in the previous example the number of RGrps e.g. RGrp1 RGrp2 RGrp3 for example is equal to the number of devices in each of sub device groups . Therefore there are four RGrps in the four device sub device group namely RGrp1 RGrp2 RGrp3 and RGrp4 six RGrps in the six device sub device group namely RGrp1 RGrp2 RGrp3 RGrp4 RGrp5 and RGrp6 and eight RGrps in the eight device sub device group namely RGrp1 RGrp2 RGrp3 RGrp4 RGrp5 RGrp6 RGrp7 and RGrp8. The number of devices an RGrp will stripe across is equal to an integer power of two plus one for the next lower integer power of two plus one multiple. For example in the eight disk sub device group the next lower integer power of two plus one is four plus one which is five. Therefore each RGrp RGrp1 8 stripes across five devices in an eight disk sub device group . Similarly the next lower integer power of two plus one for the six disk sub device group is also four plus one which is five. In the four disk sub device group band the next lower integer power of two plus one multiple is two plus one which is three. Thus four RGrps RGrps1 4 stripe across three disks in a sub device group .

Each enumerated RGrp striped across multiple devices 2RAID level 0 logical volumes or 2 1 RAID level 5 logical volumes is a cluster see Tables 6 and 7 . Thus in Table 5 the RGrp1 sections together combine into a single cluster. Likewise RGrp2 sections are another cluster and so on. Thus there are eight clusters in the eight disk sub device group six clusters in the six disk sub device group and four clusters in the four disk sub device group.

A cluster is a configurable value that is used to manage user data within a sub device group. It is not used for managing parity data for RAID 5 volumes. The minimum cluster size is 1 MB and must be an integer power of two. The cluster size is set before any device groups or volumes are created and that size is used by all device groups within RAID controller .

Table 6 shows an example of a cluster map that includes clusters of a single band in an eight disk sub device group that is configured for a RAID 0 level.

The band is in an eight disk sub device group at RAID level 0 and includes n 10 clusters which are all mapped to RGrp1. Since eight is an integer power of two rotating RGrps are not required and therefore the band can use the same RGrp in this case RGrp1. For this configuration only one RGrp is required to map all the clusters in the band.

Table 7 illustrates an example of a RAID 5 cluster map in which rotating redundancy is required because the number of disks is not equal to an integer power of two plus one. Therefore eight RGrps are required to map across all of the disks in sub device group . This translates to eight clusters for that stripe. The RGrp rotation repeats for the next stripe which translates into another group of eight clusters that has an offset of six RGrp1 starts 6 stripes up from the first stripe . The third group of RGrps maps to a third set of eight clusters with an offset of eleven RGrp1 starts again 11 stripes from the first stripe and so on.

Groups of eight clusters are mapped by eight RGrps and each set is identified by a specific offset in the map. The top of the band has space available to map six clusters only because a single RGrp for example RGrp7 must span five disks in this example and there is not enough space to map RGrp7 or RGrp8 so the map completes at the end of RGrp6 which spans the required five disks.

In this step SM calculates the number of memory devices A through N in each sub device group. Based on this value SM calculates the number of redundancy groups that are required to map the sub device group for RAID 0 and again for RAID 5. For example in an eight disk sub device group the number of redundancy groups that are required to map clusters for RAID 0 is one integer power of two true and eight redundancy groups are required to map clusters for RAID 5 integer power of two plus one false . Method proceeds to step .

In this step SM compares the RAID 0 redundancy group map to the RAID 5 redundancy group map for a particular sub device group and determines a common 1 MB boundary where a full redundancy group rotation ends. This marks a band boundary where either RAID 0 or RAID 5 may be assigned to the band. For example in the redundancy group maps of Tables 6 and 7 common band boundaries could be formed at multiples of every five plus one stripes. The known principle of using multiples of the least common denominator may be used to find common band boundaries. Method proceeds to step .

In this step SM calculates the cluster maps for each of the bands as the band boundaries have already been defined in the previous steps for each sub device group and redundancy groups that have been calculated for each band for both RAID 0 and RAID 5. Cluster maps for rotating redundancy are in a slightly different format from cluster maps for which a single redundancy group maps all of the clusters in a band as shown in Tables 6 and 7 respectively. Method proceeds to step .

In this decision step SM receives a request for a new volume creation including information about the size of the requested volume the desired sub device group and its RAID level. SM analyzes the sub device group for bands that are free and bypasses bands that are already allocated to other volumes. SM checks whether there are any free bands left for allocation in the requested sub device group. If yes method proceeds to step if no method proceeds to step .

In this step SM allocates to the new volume the first available band that meets the requirements for the requested volume and assigns the requested RAID type to the band. SM continues to scan for free bands until the entire requested volume size has been satisfied with enough allocated bands from the sub device groups. However if there are not enough free bands to allocate to the new volume SM generates a message to the system administrator when the space allocated to the volume begins to reach capacity and informs the system administrator that data should be migrated to other volumes or that more memory devices A through N should be added to the sub device group. Method proceeds to step .

In this step SM sets the state of the allocated bands from free to allocated and brings the new volume online by allowing host access. Method ends.

In this step SM generates an error message to the system administrator that indicates that there are no free bands in the desired sub device group with which to allocate the newly requested volume. Method ends.

By defining bands and creating cluster maps for each RAID type during initialization rather than when a volume request is made the RAID controller s processor has more throughput available for other system resources and thereby increases overall system performance over that of conventional networked storage systems. This method of allocation also allows more user flexibility in designing the system for various data storage needs because the pre mapped bands are assigned to a new volume as defined by the user rather than by the RAID controller that allocates volumes according to internal algorithms with little or no user input. Finally this allocation method allows more memory device capacity to be utilized because the bands align on the nearest megabyte boundaries and the way the clusters are laid out results in very little unused space on the devices. The only space that is not available to the user is the Meta Data area and a portion at the end of the device. The unmapped space at the end of the device is used for reassigning clusters during error recovery.

Although the present invention has been described in relation to particular embodiments thereof many other variations and modifications and other uses will become apparent to those skilled in the art. Therefore the present invention is to be limited not by the specific disclosure herein but only by the appended claims.

