---

title: Video enhancement
abstract: Video enhancement enable visually-apparent improvements to image frames of a video sequence. In an example implementation, stabilized or other image frames include missing image areas that are filled using a video completion technique. The missing image areas are filled in using a video completion technique that involves motion inpainting. Local motion data of a motion map from known image areas is propagated into the missing image areas to produce an extended motion map. Pixels of the missing image areas are then filled in by warping pixels from neighboring image frames into the missing image areas responsive to the extended motion map. In another example implementation, video deblurring reduces the blurriness of a current image frame. Sharper image pixels from neighboring image frames are transferred so as to replace blurry pixels of the current image frame. In yet another example implementation, video completion with motion inpainting enables amelioration of visual artifacts.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07548659&OS=07548659&RS=07548659
owner: Microsoft Corporation
number: 07548659
owner_city: Redmond
owner_country: US
publication_date: 20050513
---
This disclosure relates in general to enhancing video and in particular by way of example but not limitation to improving the visual appearance of one or more image frames of a video sequence.

Creating home videos is a popular use of consumer video cameras. People take videos of sporting events family gatherings first time activities of their children and other special events. Unfortunately home videos often suffer from moderate to severe shakes. These shakes tend to arise from the recording habits of consumers. Such habits include both walking with the camera and shooting long video shots with no tripod or other support. The resulting videos are generally too long unedited lacking in good composition devoid of scripting and shaky.

Consequently video enhancement has been steadily gaining in importance. Video enhancement can improve videos in real time or after the recording is completed. An important aspect of video enhancement is video stabilization which is the process of generating a new compensated video sequence when undesirable image motion caused by camera jittering is removed. Unfortunately existing approaches to video stabilization produce noticeably degraded video sequences.

Video enhancement enables visually apparent improvements to image frames of a video sequence. In a described example implementation stabilized or other image frames include missing image areas that are filled using a video completion technique. The missing image areas are filled in using a video completion technique that involves motion inpainting. Local motion data of a motion map from known image areas is propagated into the missing image areas to produce an extended motion map that also encompasses the missing image areas. Pixels of the missing image areas can then be filled in by warping pixels from neighboring image frames into the missing image areas responsive to the extended motion map.

In another described example implementation video deblurring reduces the blurriness of a current image frame. Sharper image pixels from neighboring image frames are transferred so as to replace blurry pixels of the current image frame. In yet another described example implementation video completion with motion inpainting enables amelioration of visual artifacts.

Other method system approach apparatus device media procedure API arrangement etc. implementations are described herein.

As explained above an important aspect of video enhancement is video stabilization which can generate a new compensated video sequence that reduces or eliminates undesirable image motion caused by camera jittering. Unfortunately a major problem of current video stabilizers is that missing image areas appear in the stabilized video due to the compensation of the motion path.

The missing image areas have been conventionally addressed with one of two options trimming the video to obtain only the portion that appears in all frames or constructing straightforward image mosaics by accumulating from neighboring frames to fill up the missing image areas. The former approach is problematic because it reduces the original video resolution when the trimmed images are scaled. Moreover sometimes due to severe camera shake there is no common area among neighboring frames. The latter approach can work satisfactorily for static and planar scenes but it produces visible artifacts for dynamic or non planar scenes.

Accordingly there is a need for schemes mechanisms techniques etc. that can generate full frame stabilized videos with good visual quality in a practical and or robust manner. In example described implementations the motion of image frames is estimated the estimated motion of the image frames is smoothed the image frames having the smoothed motion are completed and the completed image frames are deblurred. Visual artifacts if any may also be identified removed and patched in the image frames. More specifically video completion may be accomplished using motion inpainting which involves propagating local motion data to naturally stitch image mosaics. Also the image deblurring may be accomplished using sharp pixels from neighboring frames which are selected using a relative blurriness measure.

In an example implementation video completion that is based on motion inpainting is utilized. Instead of relying on color intensity as in existing image inpainting motion inpainting entails propagating local motion into missing image areas. The propagated motion is then used to naturally fill up missing image areas even for scene regions that are non planar and or dynamic. Using the propagated local motion as a guide image data from neighboring frames are locally warped to maintain spatial and temporal continuities for the stitched images. Image warping based on local motion has been used for de ghosting. However the approaches described herein propagate the local motion into area s where the local motion cannot be directly computed in order to fill up missing image areas.

In another example implementation motion deblurring is employed to address the problem of motion blur that appears in stabilized videos. Although motion blur in original videos may look natural it becomes annoying noise in stabilized videos because it does not match the compensated camera motion. Furthermore mosaicing without appropriate deblurring can result in inconsistent stitching of blurry images with sharp images. To address this issue a deblurring approach that does not require accurate point spread functions PSFs which are often hard to obtain is described.

Thus instead of estimating PSFs motion deblurring as described herein involves the transfer of sharper pixels from one or more image frames to corresponding blurry pixels in another image frame. This can increase the overall sharpness as well as generate a video of consistent sharpness. A described implementation of motion deblurring entails transferring pixels from sharper image frames to replace less sharp pixels of a current image frame using a weighted interpolation that is responsive to relative blurriness and alignment error.

This written description is separated into three additional sections. A first section references and is entitled Example Qualitative Implementations for Video Enhancement . A second section references and is entitled Example Quantitative Implementations for Video Enhancement . There is also a third section that references and is entitled Example Operating Environment for Computer or Other Device .

Stabilized image sequence can undergo an image enhancement or . Thus for the illustrated image enhancement of image enhancement is applied after the image stabilization. However image enhancement may alternatively be considered to embrace or include image stabilization.

Three enhanced image frames and are illustrated in the bottom half of . The left image frame represents an example of a conventional trimming and scaling approach . As is apparent the motion of the camera causes only a central image square to remain as the common picture area after the trimming. After scaling the central image square to the original picture size not shown the image resolution is consequently decreased. Additionally blockiness may result and or the aspect ratio may be corrupted.

The middle image frame represents an example of a conventional straightforward mosaicing approach . As is apparent particularly from an upper segment of the image frame especially in the region around the adult s head straightforward mosaicing can result in effects that are very visually disruptive. For instance the top of the head of the adult is misaligned with respect to the bottom of the adult s head.

The right image frame represents an example of one or more described video enhancement approaches . As illustrated in for the described approach image enhancement includes both video completion and image frame deblurring. However a given image frame and or video sequence may be subjected to one but not the other. Neither trimming nor scaling as in image enhancement is effectively necessary using the described video completion approaches . Moreover the misalignment from straightforward mosaicing as in image enhancement is significantly reduced if not eliminated by the described video completion approaches .

The actions of flow diagram may be performed in many possible environments and with a variety of hardware firmware and software combinations. For example they may be performed i by a mobile device e.g. a mobile phone personal digital assistant PDA etc. by a camcorder by a personal computer etc. and ii in real time or off line. Also some of the processing may be performed in real time e.g. by a device acquiring the video while other parts of the processing are reserved for offline performance e.g. by a more powerful computing device . Additional example environments and devices are described herein below especially in the section entitled Example Operating Environment for Computer or Other Device .

At block a shaky video having multiple image frames is input. For example the shaky video may be input to a device a module a process or some other component. At block the motion of the multiple image frames is estimated. At block the estimated motion of the multiple image frames is smoothed. Smoothing the motion of the multiple image frames produces a stabilized image sequence.

At block the multiple image frames of the stabilized image sequence are completed with motion inpainting. At block each image frame of the multiple completed image frames are deblurred with sharp pixels from other image frames. In any given implementation video completion or image deblurring or both may be applied to stabilized image frames.

At block visual artifacts if any may be identified removed and patched i.e. corrected in the image frames. The action s of this optional block may be utilized to ameliorate such artifacts as for example overlaid alphanumeric characters a network logo marks resulting from smudges scratches on a camera lens or defects of a CCD and so forth. At block the enhanced image frames are output as enhanced video.

The actions of blocks and are described further below in this section with particular reference to the following respective subsections Qualitative Motion Estimation and that relates thereto Qualitative Motion Smoothing and that relates thereto Qualitative Video Completion and that relates thereto Qualitative Image Deblurring and that relates thereto and Qualitative Visual Artifact Amelioration and that relates thereto .

Global motion pertains to the transformation between consecutive image frames. The inter frame motion estimation is accomplished by aligning consecutive frames using a global transformation. The transformation model may be for example translation plus rotation affine planar projective perspective some combination thereof and so forth. The frame alignment may be effectuated in multi resolution and iteratively. In a described implementation the computation is performed at each step in coarse to fine fashion using a Laplacian image pyramid but other approaches may alternatively be employed.

An original position O of the current image frame is shown. The local displacement of the current image frame is smoothed as indicated by the motion compensated position MC of the current image frame. Smoothing the estimated motion based on local displacement can avoid accumulation errors that can be prevalent with pre existing approaches.

At block pixels from neighboring image frames are warped to missing image areas of a current image frame. For example for a predetermined number of neighboring image frames a pixel is warped to the current image frame from a corresponding pixel location.

At block it is determined if the warped pixels collectively meet a consistency constraint. For example a consistency value e.g. a variance is determined based on the warped pixel values with the consistency value reflecting how much the warped pixels from the different neighboring image frames are similar. The determined consistency value is compared to a threshold consistency value. If the determined consistency value exceeds the threshold consistency value the method continues at block . If the determined consistency value fails to exceed the threshold consistency value the method continues at block .

At block the value of the warped neighborhood pixels from a compatible frame is accepted. For example the warped neighborhood pixels of the most compatible frame may be stored as the completed pixel value for the missing image area. The most compatible frame can be determined based on for example the frame that is the most temporally proximate to the current frame the frame with the best inter frame alignment error with respect to the current frame some combination thereof and so forth. The median value of warped neighborhood pixels may alternatively be used. The actions of blocks through are repeated for each of the pixels in the missing image area. When the missing image area has been analyzed with regard to warping multiple neighborhood image pixels and the consistency constraint the method continues at block as indicated by the dashed arrow line extending from block .

At block local motion is computed with respect to a neighboring frame. The neighboring frame may be selected for example based on inter frame alignment error. The local motion may be computed to produce local motion data as described herein above in the subsection entitled Qualitative Motion Estimation and below in the associated quantitative subsection.

At block local motion data is propagated from known image areas to missing image areas. For example the local motion data of a motion map may be propagated from the boundary of known image areas into the missing image areas. The propagation of the local motion data produces a e.g. optical flow field or motion map in the missing image areas by extending the flow field or motion map from known image areas into missing image areas. Thus the local motion data of a motion map can be produced over all of the area of the new stabilized frame including those areas that are not covered by the motion map in the original frame.

At block pixels are locally warped from the selected neighboring image frame responsive to the extended motion map. For example both the global transformation matrix between the two image frames and the local motion map are applied to a pixel in the selected neighboring image frame to produce a corresponding pixel for the current image frame.

At block it is determined if all pixels are filled in for the current image frame. If not then the method continues at block so as to repeat the actions of blocks with a different e.g. the next neighboring image frame. When all the pixels are filled in video completion for the current image is completed at block . Small numbers of remaining unfilled pixels may also be filled in using another technique such as filter blurring diffusion and so forth.

At block a relative blurriness measure between a current image frame and each neighboring image frame of multiple neighboring image frames is ascertained. An example relative blurriness measure is described herein below in the subsection entitled Quantitative Image Deblurring however other relative or absolute blurriness sharpness measures may alternatively be employed.

At block those neighboring image frames that are relatively sharper than the current image frame are selected. For example if 12 neighboring image frames are considered e.g. six in each of the forward and backward directions seven of the 12 neighboring image frames may be sharper than the current image frame so those seven are selected.

At block the current image frame is sharpened by transferring and interpolating corresponding pixels from the selected sharper neighboring image frames to the current image frame. For example corresponding pixels from those seven less blurry neighboring image frames may be transferred with a weighting that is responsive to an alignment error and or to the relative blurriness measure.

At block at least one visual artifact is identified in the image frames of a video sequence. For example the visual artifact may be identified manually e.g. by direct user indication or automatically e.g. based on an optional video enhancement setting of a program or a user menu keyboard command . If identified manually it may be marked with a pointer icon under the control of a mouse or similar input device. If identified automatically it may be identified by detecting that pixels at a fixed pixel location across multiple image frames i have a constant color or intensity value and or ii have a value that is different from nearby pixels at a constant delta while other pixels in the image frames have i changing values and or ii values with varying deltas respectively. Other pixel characteristics may alternatively be used with an automatic identification of the visual artifact.

At block the identified visual artifact is removed from an image frame to reveal or create a missing image area. At block the pixels of the revealed missing image area are filled in using a video completion technique that involves motion inpainting. Motion inpainting is described herein at the subsections entitled Qualitative Video Completion above and Quantitative Video Completion below .

Hence pixels of the missing image area which is created upon the removal of the identified visual artifact are replaced with pixels warped from a neighboring image frame responsive to an extended motion map with the extended motion map being created by propagating motion data from known image areas to unknown image areas. At block the actions of blocks and are repeated for each affected image frame of the video sequence.

The description in this section is directed to example quantitative implementations for video enhancement. Although some examples are relatively specific for the sake of increased clarity actual implementations may be implemented in many different manners. This quantitative section includes the following subsections Quantitative Motion Estimation Quantitative Motion Smoothing Quantitative Video Completion Quantitative Image Deblurring Video Completion Results Quantitative Visual Artifact Amelioration and Quantitative Conclusion .

This subsection describes motion estimation methods that can be used in the described deblurring and completion mechanisms. First a method to estimate interframe image transformation or global motion is described. Local motion that deviates from the global motion is estimated separately and described second. The global motion is used for two purposes stabilization and image deblurring. The local motion is used for video completion. The following subsection entitled Quantitative Motion Smoothing describes a motion smoothing algorithm which is relevant to stabilizing global motion.

We first explain a method of estimating global motion between consecutive images. In the case that a geometric transformation between two images can be described by a homography or 2D perspective transformation the relationship between two overlapping images I p and I p can be written by p Tp . p x y 1 and p x y 1 are pixel locations in projective coordinates and indicates equality up to scale since the e.g. 3 3 matrix T is invariant to scaling.

Global motion estimation is performed by aligning pair wise adjacent frames assuming a geometric transformation. In a described method an affine model is assumed between consecutive images. By way of example only we employ the hierarchical motion estimation framework proposed by Bergen et al. J. R. Bergen P. Anandan K. J. Hanna and R. Hingorani Hierarchical model based motion estimation in 21992 pp. 237 252. . By applying the parameter estimation for every pair of adjacent frames a global transformation chain is obtained.

Throughout this quantitative section we denote the pixel location in the image coordinate Ias p. The subscript t indicates the index of the frame. We also denote the global transformation Tto represent the coordinate transform from frame i to j. Thus the transformation of image Ito the Icoordinate can be described as I Tp . The transformation T describes the coordinate transform hence I Tp has the pixel values of frame t 1 in the coordinates of frame t.

Local motion describes the motion which deviates from the global motion model. For example local motion relates to the motion of moving objects image motion due to non planar scenes some combination thereof and so forth. In a described implementation local motion is estimated by computing optical flow between frames after applying a global transformation however the computed optical flow is derived from the common coverage areas between the frames.

By way of example only a pyramidal version of the Lucas Kanade optical flow computation J. Y. Bouguet Pyramidal implementation of the Lucas Kanade feature tracker description of the algorithm 2000. is applied to obtain the optical flow field. The optical flow field F p u p v p F p represents an optical flow from frame I P to I Tp . The vector components u and v represent the flow vector along the x and y direction respectively in pcoordinates.

This subsection describes how the undesired motion is removed. A stabilized motion path is obtained by removing the undesired motion fluctuation. It is assumed that the intentional motion in the video is usually slow and smooth. We therefore define the high frequency component in the global motion chain as the unintentional motion.

Previous motion smoothing methods attempt to smooth out the transformation chain itself or the cumulative transformation chain with an anchoring frame. In a described implementation on the other hand the motion of image frames is smoothed by smoothing local displacement.

With conventional approaches when smoothing is applied to the original transformation chain T. . . Tas it is done in pre existing works a smoothed transformation chain tilde over T . . . tilde over T is obtained. In this case a motion compensated frame I is obtained by transforming Iwith Ttilde over T . Unfortunately the resulting cascade of the original and smoothed transformation chain often generates accumulation error.

In contrast implementations as described herein are relatively free from accumulative error because described techniques locally smooth displacement from a current frame to neighboring frames. Instead of smoothing out the original transformation chain along the video frame sequence we directly compute the transformation S from one frame to the corresponding motion compensated frame using neighboring transformation matrices.

As is visibly apparent abrupt displacements which are considered to be unwanted camera motion are well reduced by the described motion smoothing. The smoothness of the new camera motion path can be controlled by changing k with a larger k yielding a smoother result. Experiments indicate that annoying high frequency motion is well removed by setting k 6 e.g. about 0.5 seconds with NTSC . However the variable k can be increased when a smoother video is preferred or decreased e.g. for reduced computational demands and or for a faster response in a real time implementation .

In a described implementation for video completion image mosaics are locally adjusted using a local motion field in order to obtain relatively seamless stitching of the mosaics in the missing image areas. Specifically motion inpainting is utilized to propagate the local motion field into the missing image areas where local motion cannot be directly computed. An underlying assumption is that the local motion in the missing image areas is similar to that of adjoining image areas.

At phase local motion from a neighboring frame is estimated over the common coverage image area. For example local motion may be computed between a current image frame and a neighboring image frame for at least approximately overlapping region s . At phase the local motion field is then propagated into missing image areas to effectuate motion inpainting. Thus instead of propagating color as in pre existing image inpainting the local motion is propagated for motion inpainting. At phase the propagated local motion is used as a guide to locally warp image mosaics. This can achieve a smooth stitching of the image mosaics.

An example of this video completion algorithm is described somewhat more rigorously in the following paragraphs of this subsection. The following terminology is used Let Mbe the missing pixels or undefined image pixels in the frame I. We wish to complete Mfor every frame t while maintaining visually plausible video quality.

In a preliminary or initial phase of video completion we attempt to cover the static and planar part of the missing image area by mosaicing in conjunction with an evaluation of the validity of the mosaic results. When the global transformation is correct and the scene in the missing image area is static and planar mosaics generated by warping from different neighboring frames should be consistent with each other in the missing area. Therefore it is possible to evaluate the validity of the mosaic by testing the consistency of the multiple mosaics that cover the same pixels but that are derived from multiple corresponding neighboring frames.

We use the variance of the mosaic pixel values to measure the consistency when the variance is high the mosaic is less reliable at a given pixel. For each pixel pin the missing image area M the variance of the mosaic pixel values is evaluated by

In this first phase each neighboring frame I is assigned a priority to be processed based on its alignment error. Usually it is observed that the nearer frame shows a smaller alignment error. The nearer frame therefore typically has a higher processing priority. The alignment error is computed using the common coverage area of I p and I Tp by

Local motion may be estimated as described herein above in the subsection entitled Quantitative Motion Estimation or by another local motion estimating technique.

In this second phase of video completion the local motion data in the known image areas is propagated into the missing image areas. The propagation starts at pixels on the boundary of the missing image area. Using motion values of neighboring known pixels motion values in the missing image area M at the boundary are defined and the boundary therefore gradually advances into the missing area M until it is completely filled in.

Suppose pis a pixel in a missing area M. Let H p be the pixels of the neighborhood of p that already have a defined motion value by either the initial local motion computation or prior extrapolation of motion data. The neighborhood pixels of a given pixel pmay be considered to be the eight adjacent pixels or some other definition may alternatively be used. The motion value for pixel pis generated by a weighted average of the motion vectors of the pixels H p 

As illustrated in q are first located in the neighboring image Iand their local motion. Using the geometric relationship between qand p p are tentatively determined in I. Using p we measure the color similarity by w p q 1 ColorDistance I p I q where is a small value for avoiding division by zero. In this way the weight factor is computed using the color similarity and the motion value computed by Eq. 7 is assigned to p. The l norm for the color difference in RGB space has been used for the sake of computational speed but a different measure could alternatively be used.

In a described implementation the actual scanning and composition in the missing area M is done using the Fast Marching Method. The FMM is introduced by J. A. Sethian J. A. Sethian Cambridge Univ. Press 1996. and an example of the FMM being applied in the context of image inpainting is given by A. Telea A. Telea An image inpainting technique based on the fast marching method 9 1 23 34 2004. .

Let M be the group of boundary pixels of the missing image area M e.g. let M be pixels that have a defined neighbor . Using FMM we are able to visit each undefined pixel only once starting with pixels of M and advancing the boundary between defined and undefined pixels inside the missing image area M until the undefined pixels are assigned motion values see . The pixels are processed in ascending distance order from the initial boundary M such that pixels close to the known area are filled first. The result of this process is a smooth extrapolation of the local motion flow to the undefined area in a manner that preserves object boundaries with color similarity measure.

Other mechanisms instead of the FMM may alternatively be used to propagate the local motion data into the missing image areas. For example diffusion may also be used. With diffusion however a given pixel may be visited more than once before a motion value is assigned thereto.

In this third phase of video completion after the optical flow field in the missing image area Mis obtained with motion inpainting or more generally after a motion map is extended into the missing image area we use the optical flow field F as a guide to locally warp I in order to generate a smooth mosaic. Moving objects may also be included as part of the smooth mosaic. . 8 If some missing pixels still exist in I the algorithm returns to the first phase and uses the next neighboring frame to compute local motion over common areas.

After a second looping through of the first through the third phases the missing pixels are usually all filled. However it is possible that there still remain missing image pixels that are not covered by constrained mosaics of the preliminary initial phase or warped mosaics of the first third phases . Such image areas are considered to be generally small therefore we simply apply a blur filter to fill up these small areas. Alternatively richer methods such as non parametric sampling or diffusion can be used to produce higher quality results than simple blurring for the final pixel completion but the richer methods involve additional computational cost.

After video stabilization motion blur which is not associated with the new motion of the video frame sequence becomes a noticeable noise. Removing that noise can improve the visible appearance of the video. As noted herein above it is usually difficult to obtain accurate PSFs from a free motion camera consequently image deblurring using deconvolution is unsuitable for our purposes. In order to sharpen blurry frames without using PSFs we developed and describe herein a new interpolation based deblurring technique. In accordance with a described implementation of the interpolation based deblurring technique sharper image pixels are transferred from neighboring frames to corresponding blurry image pixels of a current image frame.

First the relative blurriness of the image is evaluated. Relative blurriness represents how much of the high frequency component has been removed from the frame in comparison to the neighboring frames. Image sharpness which is the inverse of blurriness has been investigated in the field of microscopic imaging where a premium is usually placed on accurate focus.

We use the inverse of the sum of squared gradient measure to evaluate the relative blurriness because of its robustness to image alignment error and its computational efficiency. By denoting two derivative filters along the x and y directions by fand f respectively the blurriness measure is defined by

This blurriness measure does not give an absolute evaluation of image blurriness but it yields relative image blurriness among similar images when compared to the blurriness of other images. Hence for this particular blurriness measure we restrict the blurriness measure to be used in a limited number of neighboring frames where significant scene change is not observed. Also the blurriness measure is computed using a common coverage area which is observed in the neighboring frames under consideration. Relatively blurry frames are determined by examining b b t N. When for example b b is larger than 1 frame I is considered to be sharper than frame I. Inversely when b b is less than 1 frame I is considered to be less sharp or blurrier than frame I.

Once relative blurriness is determined blurry frames are sharpened by transferring and interpolating corresponding pixels from sharper frames. To reduce reliance on pixels where a moving object is observed a weight factor which is computed by a pixel wise alignment error E from I to Iis used . 10 

High alignment error is caused by either moving objects or error in the global transformation. Using the inverse of pixel wise alignment error E as a weight factor for the interpolation blurry pixels are replaced by interpolating sharper pixels. The deblurring can be described by

It is apparent that the stabilized result of the middle row contains a significant amount of missing image areas. In the experiment that produces the results of a 5 5 size filter h is used to perform motion inpainting. As shown in the bottom row the missing image areas of the stabilized middle row sequence are naturally filled in using an implementation of the described video completion technique.

In addition to further enhancing a stabilized video sequence implementations of the video completion and deblurring algorithms described above can also be used in a range of other video enhancement applications. Two example applications are specifically described here i sensor dust removal from a video sequence in which the visual artifact is caused by dirt spots on the video lens or a damaged CCD and ii overlaid text logo removal. They can both be envisioned as a problem of filling up specific image areas that are indicated or marked as missing.

By way of example only this can naturally be applied to the removal of a time stamp from a video. In particular when a stabilizing process is applied to a video that originally includes a time stamp it can become desirable to remove the time stamp artifact from the stabilized video because the time stamp becomes shaky in the final stabilized video.

In the examples described below with particular reference to the artifacts were manually marked as missing image areas. However the artifacts can also be automatically ascertained by detecting any pixels at a static pixel location that remain visually constant throughout a video sequence while other pixels in the image frames change. The missing image areas are then filled up by an implementation of the described video completion technique.

The third row of shows the result of the video completion process. The resulting video image frame has the text removed. In fact the example result looks almost identical to the original video frame image because the missing image areas are naturally filled up. The bottom row shows the absolute intensity difference between the original image frame and the resulting image frame. The resulting image frame sequence is not identical to the original image frame sequence as identifiable by the slight color differential in the area around the text overlay. However the difference is small and more importantly visual appearance is well preserved.

Described herein are example implementations of video completion and deblurring algorithms for generating full frame stabilized videos. The video completion algorithm is based on motion inpainting. Motion inpainting propagates motion into missing image areas and the propagated motion field is then used to relatively seamlessly stitch image mosaics. The described completion method implicitly enforces spatial and temporal consistency supported by motion inpainting. Spatial smoothness of the constructed mosaics is indirectly ensured by the smoothness of the extrapolated optical flow. Also temporal consistency on both static and dynamic areas is given by optical flow from the neighboring frames. The described deblurring algorithm transfers and interpolates sharper pixels from neighboring frames. These enumerated properties and abilities make the resulting videos look natural and coherent. Additionally the applicability of the described video completion algorithm to practical video enhancement has been evidenced by showing the results of both sensor dust removal and text removal.

The devices actions aspects features functions procedures modules data structures images components etc. of are illustrated in diagrams that are divided into multiple blocks. However the order interconnections interrelationships layout etc. in which are described and or shown are not intended to be construed as a limitations and any number of the blocks can be modified combined rearranged augmented omitted etc. in any manner to implement one or more systems methods devices procedures media apparatuses APIs arrangements etc. for video enhancement. Furthermore although the description herein includes references to specific implementations including a general device of the illustrated and or described implementations can be implemented in any suitable hardware software firmware or combination thereof and using any suitable motion estimation algorithm s motion smoothing procedure s consistency constraint s relative blurriness measure s alignment error formulation s and or weighting factor s and so forth.

Example operating environment is only one example of an environment and is not intended to suggest any limitation as to the scope of use or functionality of the applicable device including computer network node entertainment device mobile appliance general electronic device etc. architectures. Neither should operating environment or the devices thereof be interpreted as having any dependency or requirement relating to any one or to any combination of components as illustrated in .

Additionally implementations for video enhancement may be realized with numerous other general purpose or special purpose device including computing system environments or configurations. Examples of well known devices systems environments and or configurations that may be suitable for use include but are not limited to personal computers server computers thin clients thick clients personal digital assistants PDAs or mobile telephones watches hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics video game machines game consoles portable or handheld gaming units network PCs videoconferencing equipment minicomputers mainframe computers network nodes distributed or multi processing computing environments that include any of the above systems or devices some combination thereof and so forth.

Implementations for video enhancement may be described in the general context of processor executable instructions. Generally processor executable instructions include routines programs protocols objects functions interfaces components data structures etc. that perform and or enable particular tasks and or implement particular abstract data types. Realizations of video enhancement as described in certain implementations herein may also be practiced in distributed processing environments where tasks are performed by remotely linked processing devices that are connected through a communications link and or network. Especially but not exclusively in a distributed computing environment processor executable instructions may be located in separate storage media executed by different processors and or propagated over transmission media.

Example operating environment includes a general purpose computing device in the form of a computer which may comprise any e.g. electronic device with computing processing capabilities. The components of computer may include but are not limited to one or more processors or processing units a system memory and a system bus that couples various system components including processor to system memory .

Processors are not limited by the materials from which they are formed or the processing mechanisms employed therein. For example processors may be comprised of semiconductor s and or transistors e.g. electronic integrated circuits ICs . In such a context processor executable instructions may be electronically executable instructions. Alternatively the mechanisms of or for processors and thus of or for computer may include but are not limited to quantum computing optical computing mechanical computing e.g. using nanotechnology and so forth.

System bus represents one or more of any of many types of wired or wireless bus structures including a memory bus or memory controller a point to point connection a switching fabric a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. By way of example such architectures may include an Industry Standard Architecture ISA bus a Micro Channel Architecture MCA bus an Enhanced ISA EISA bus a Video Electronics Standards Association VESA local bus a Peripheral Component Interconnects PCI bus also known as a Mezzanine bus some combination thereof and so forth.

Computer typically includes a variety of processor accessible media. Such media may be any available media that is accessible by computer or another e.g. electronic device and it includes both volatile and non volatile media removable and non removable media and storage and transmission media.

System memory includes processor accessible storage media in the form of volatile memory such as random access memory RAM and or non volatile memory such as read only memory ROM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules instructions that are immediately accessible to and or that are being presently operated on by processing unit .

Computer may also include other removable non removable and or volatile non volatile storage media. By way of example illustrates a hard disk drive or disk drive array for reading from and writing to a typically non removable non volatile magnetic media not separately shown a magnetic disk drive for reading from and writing to a typically removable non volatile magnetic disk e.g. a floppy disk and an optical disk drive for reading from and or writing to a typically removable non volatile optical disk such as a CD DVD or other optical media. Hard disk drive magnetic disk drive and optical disk drive are each connected to system bus by one or more storage media interfaces . Alternatively hard disk drive magnetic disk drive and optical disk drive may be connected to system bus by one or more other separate or combined interfaces not shown .

The disk drives and their associated processor accessible media provide non volatile storage of processor executable instructions such as data structures program modules and other data for computer . Although example computer illustrates a hard disk a removable magnetic disk and a removable optical disk it is to be appreciated that other types of processor accessible media may store instructions that are accessible by a device such as magnetic cassettes or other magnetic storage devices flash memory compact disks CDs digital versatile disks DVDs or other optical storage RAM ROM electrically erasable programmable read only memories EEPROM and so forth. Such media may also include so called special purpose or hard wired IC chips. In other words any processor accessible media may be utilized to realize the storage media of the example operating environment .

Any number of program modules or other units or sets of processor executable instructions may be stored on hard disk magnetic disk optical disk ROM and or RAM including by way of general example an operating system one or more application programs other program modules and program data . These processor executable instructions may include for example one or more of a motion estimator a motion smoother a video image frame completer a video image frame deblurrer a visual artifact corrector some combination thereof and so forth.

A user may enter commands and or information into computer via input devices such as a keyboard and a pointing device e.g. a mouse . Other input devices not shown specifically may include a microphone joystick game pad satellite dish serial port video camera scanner and or the like. These and other input devices are connected to processing unit via input output interfaces that are coupled to system bus . However input devices and or output devices may instead be connected by other interface and bus structures such as a parallel port a game port a universal serial bus USB port an infrared port an IEEE 1394 Firewire interface an IEEE 802.11 wireless interface a Bluetooth wireless interface and so forth.

A monitor view screen or other type of display device may also be connected to system bus via an interface such as a video adapter . Video adapter or another component may be or may include a graphics card for processing graphics intensive calculations and for handling demanding display requirements. Typically a graphics card includes a graphics processing unit GPU video RAM VRAM etc. to facilitate the expeditious display of graphics and performance of graphics operations. In addition to monitor other output peripheral devices may include components such as speakers not shown and a printer which may be connected to computer via input output interfaces .

Computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computing device . By way of example remote computing device may be a peripheral device a personal computer a portable computer e.g. laptop computer tablet computer PDA mobile station etc. a palm or pocket sized computer a watch a gaming device a server a router a network computer a peer device another network node or another device type as listed above and so forth. However remote computing device is illustrated as a portable computer that may include many or all of the elements and features described herein with respect to computer .

Logical connections between computer and remote computer are depicted as a local area network LAN and a general wide area network WAN . Such networking environments are commonplace in offices enterprise wide computer networks intranets the Internet fixed and mobile telephone networks ad hoc and infrastructure wireless networks mesh networks other wireless networks gaming networks some combination thereof and so forth. Such networks and logical and physical communications connections are additional examples of transmission media.

When implemented in a LAN networking environment computer is usually connected to LAN via a network interface or adapter . When implemented in a WAN networking environment computer typically includes a modem or other component for establishing communications over WAN . Modem which may be internal or external to computer may be connected to system bus via input output interfaces or any other appropriate mechanism s . It is to be appreciated that the illustrated network connections are examples and that other manners for establishing communication link s between computers and may be employed.

In a networked environment such as that illustrated with operating environment program modules or other instructions that are depicted relative to computer or portions thereof may be fully or partially stored in a remote media storage device. By way of example remote application programs reside on a memory component of remote computer but may be usable or otherwise accessible via computer . Also for purposes of illustration application programs and other processor executable instructions such as operating system are illustrated herein as discrete blocks but it is recognized that such programs components and other instructions reside at various times in different storage components of computing device and or remote computing device and are executed by processor s of computer and or those of remote computing device .

Although systems media devices methods procedures apparatuses techniques schemes approaches procedures arrangements and other implementations have been described in language specific to structural logical algorithmic and functional features and or diagrams it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features or diagrams described. Rather the specific features and diagrams are disclosed as exemplary forms of implementing the claimed invention.

