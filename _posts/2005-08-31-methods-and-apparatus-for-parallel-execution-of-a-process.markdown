---

title: Methods and apparatus for parallel execution of a process
abstract: In one embodiment, a process may be performed in parallel on a parallel server by defining a data type that may be used to reference data stored on the parallel server and overloading a previously-defined operation, such that when the overloaded operation is called, a command is sent to the parallel server to manipulate the data stored on the parallel server. In some embodiments, the previously-defined operation that is overloaded may be an operation of an operating system. Further, in some embodiments, when the data stored on the parallel server is no longer needed, a command may be sent to the parallel server to reallocate the memory used to store the data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07814462&OS=07814462&RS=07814462
owner: The Regents of the University of California
number: 07814462
owner_city: Oakland
owner_country: US
publication_date: 20050831
---
This application claims the benefit under 35 U.S.C. 119 e of co pending U.S. Provisional Application Ser. No. 60 623 682 filed Oct. 29 2004 entitled A Method And System For An Interactive Parallel Programming Environment by Long Yin Choy et. al. the contents of which are incorporated herein by reference.

This invention was made with Government support under Contract Nos. F19628 00 C 0002 and F30602 02 1 0181 awarded by the U.S. Air Force under Contract Nos. CCR9404326 and DMS9971591 awarded by the NSF and under Contract Nos. DE FG02 04ER25631 and DE FG02 04ER25632 awarded by DOE. The Government has certain rights in this invention.

Parallel computing is the concurrent use of multiple processors to solve a computational problem. Large problems may take significant time to solve using only a single processor. Thus such problems may be divided among multiple processors each of which solves a portion of the problem. However writing program code to solve a computational problem in parallel may present challenges. For example a programmer may have to devise a complex algorithm to determine an efficient way in which to divide the problem up among the multiple processors how memory should be allocated and shared among these processors and how messages should be passed between processors.

The programmer may also wish to employ pre existing parallel computing software packages such as for example parallel virtual machine PVM or message passing interface MPI packages so that routines for sharing data between processors spawning additional processes and other general parallel computing tasks need not be coded from scratch. Thus the programmer may have to be familiar with such software packages.

One aspect of the invention is directed to a method of enabling performance of a process in parallel. The method comprises acts of a defining a data type capable of referencing data stored on a parallel server b defining an overloaded operation that overloads a previously defined operation wherein the overloaded operation is defined to operate on an instance of the data type and to cause a process to be performed in parallel on data stored in memory on the parallel server that is referenced by the instance of the data type and wherein the previously defined operation is defined to cause the process to be performed serially and c defining a routine that when the data stored on the parallel server is no longer needed causes at least a portion of the memory on the parallel server that stores such data to be reallocated. Another aspect is directed to at least one computer readable medium encoded with instructions that when executed on a computer system perform the above described method.

A further aspect is directed to a computer in a computer system comprising the computer a parallel server and a communication link that couples the computer and the parallel server. The computer comprises an input and at least one controller coupled to the output that a receives via the input a definition of a data type capable of referencing data stored on the parallel server b receives via the input a definition of an overloaded operation that overloads a previously defined operation wherein the overloaded operation is defined to operate on an instance of the data type and to cause a process to be performed in parallel on data stored in memory on the parallel server that is referenced by the instance of the data type and wherein the previously defined operation is defined to cause the process to be performed serially and c receives via the input a definition of a routine that when the data stored on the parallel server is no longer needed causes at least a portion of the memory on the parallel server that stores the data to be reallocated.

Another aspect is directed to a method of performing a process in parallel on a parallel computer. The method comprises acts of a receiving at the parallel computer a first command to perform the process wherein generation of the first command is caused by performance of an overloaded operation in a programming language wherein the overloaded operation overloads a previously defined operation and wherein the previously defined operation is defined to cause the process to be performed serially b performing the process in parallel on the parallel computer to generate a result c storing the result in memory on the parallel computer and d receiving at the parallel computer a second command to reallocate the memory on the parallel computer that stores the result. A further embodiment is directed to at least one computer readable medium encoded with instructions that when executed on a computer system perform the above described method.

Another aspect is directed to a parallel computer comprising an input and at least one controller coupled to the input that a receives via the input a first command to perform a process wherein generation of the first command is caused by performance of an overloaded operation in a programming language wherein the overloaded operation overloads a previously defined operation and wherein the previously defined operation is defined to cause the process to be performed serially b performs the process in parallel to generate a result stores the result in memory on the parallel computer and c receives via the input a second command to reallocate the memory on the parallel computer that stores the result.

A further aspect is directed to a method of enabling performance of a process in parallel on a parallel server. The method comprises an act of defining an overloaded operation that overloads a previously defined operation of an operating system wherein the overloaded operation is defined to cause a process to be performed in parallel on data stored in memory on the parallel server and wherein the previously defined operation is defined to cause the process to be performed serially. Another aspect is directed to at least one computer readable medium encoded with instructions that when executed on a computer system perform the above described method.

A further aspect is directed to a computer in a computer system comprising the computer a parallel server and a communication link coupling the computer and the parallel server. The computer comprises an input and at least one controller coupled to the input that receives a definition of an overloaded operation that overloads a previously defined operation of an operating system wherein the overloaded operation is defined to cause a process to be performed in parallel on data stored in memory on the parallel server and wherein the previously defined operation is defined to cause the process to be performed serially.

Scientific computing software applications are widely used in the science and engineering community as computing and modeling tools. As used herein the term scientific computing software application refers to a software application capable of solving computational problems that are input by a user. Examples of commercially available scientific computing software applications include MATLAB produced and sold by The MathWorks Natick Mass. Maple produced and sold by Maplesoft Waterloo Ontario Canada and Mathematica produced and sold by Wolfram Research Champaign Ill. These software applications are useful in a variety of disciplines including for example engineering mathematics physics life sciences chemistry chemical engineering social sciences finance economics and others. It should be appreciated that while referred to as a scientific computing software application scientific software applications are not limited to solving problems or performing calculations of a scientific nature as any suitable type of problems or calculations may be processed by the scientific computing software application.

Because of the breadth of the fields in which these applications are used their users are often not professional software developers but rather may have little programming experience and skill. Thus the creators of such software applications usually provide a high level programming language and a user friendly interface that allows users with little programming experience to employ the software application to perform computational and modeling tasks. Such scientific computing software applications typically include a significant amount of built in intelligence which often results in less code being written to solve a problem than to solve the same problem in a traditional programming language such as for example C C or FORTRAN. Further the syntax of a scientific computing software application programming language is typically far less strict than a traditional programming language and low level tasks such as memory allocation and management are typically handled by the application and are out of the control of the programmer. These scientific computing software application programming languages take control away from the programmer and additional parsing and interpreting performed by these applications may require additional processor cycles and may result in less efficient execution of a program making them unsuitable for some applications. However the time and level of skill required to write effective programs are often reduced making these applications usable by less experienced and less skilled programmers and even skilled programmers may save a lot of time.

For example the programming language of a scientific computing software application may have a built in data type for matrices and built in matrix operations such that when a mathematical operator e.g. plus is used with operators that are of the matrix data type a matrix operation is automatically performed. An example of a program for adding two 2 2 matrices represented by the variables A and B in a scientific computing software application is shown in Table 1.

By contrast performing the same matrix addition in a traditional programming language such as C may require more code and may involve operating on each individual matrix element. An example of a program for performing the same task in the C programming language is shown in Table 2.

As can be seen from the above example even a simple task such as adding two very small matrices many more lines of code in C in the example four times the number of lines than it does in the programming language of a typical scientific computing software application.

While these scientific computing programming languages are well suited for allowing inexperienced programmers to solve computational problems relatively easily their usefulness is limited in solving very large or very complex problems. That is large and or complex problems may take a significant and or unacceptable amount of time to solve using a single processor and it may be desirable instead to solve the problem in parallel e.g. by dividing the problem up amongst multiple processors . However some scientific computing software applications do not include built in parallel computing capability. Others that do provide parallel computing capability or are capable of operating in conjunction with third party add on software packages that provide parallel computing capability may be limited in their capabilities and may require the programmer to have detailed knowledge of parallel processing techniques such as devising and coding a complex parallel algorithm controlling memory allocation and management and or managing interprocessor communication. Thus the parallel computing capabilities available in connection with a scientific computing software application may be insufficient to perform the desired task and or may be too complex for the level of skill of the typical user of the software application.

Thus Applicants have appreciated that it may be desirable to provide parallel computing capability in a scientific computing application that is easy to use does not differ much from the user s perspective from the serial i.e. non parallel use of the scientific computing application and does not require a lot of learning on the users part. The embodiments described below may provide some all or none of the benefits listed above and it should be appreciated that the invention is not limited to providing all or any of these benefits.

In one embodiment a technique referred to herein as parallelism through polymorphism may be employed to provide a parallel computing capability. Parallelism through polymorphism allows a programmer to use the same or what he perceives to be the same operators and method calls that he is familiar with but causes the operations performed by these operators and method calls to be performed in parallel.

In object oriented programming OOP terminology polymorphism refers to a characteristic of a programming language whereby operators and methods have different functionalities depending on the data types of their parameters. Many programming languages allow users to create new data types sometimes referred to as classes that have specific properties. For example a programmer may define a data type or class called Triangle that has properties defining a triangle such as the length of each side the measure of each angle and the height.

A technique referred to as overloading may be employed when implementing polymorphism. As used herein the term overloading refers to having two or more objects or methods that have the same name but have different meanings or functionalities depending on the context in which the object or method is used.

For example which method is called may depend on the number of parameters with which the method is called or the data type of parameters with which the method is called. That is a programmer may define two methods each named Area. The first Area method may take as a parameter a user defined data type called Triangle and computes the area of a triangle. The second Area method may take as a parameter a user defined data type called Circle and computes the area of a circle. Thus simply from the line of code B Area A it cannot be determined which Area method is to be called. That is if the variable A is of the data type Triangle the first Area method is called while if the variable A is of the data type Circle the second Area method is called.

As another example from the programmer s perspective the method call for the two or more methods that share the same name may be identical. That is the method name the number of parameters and the type of parameters are the same for both or all of the methods and the decision as to which of the two or methods is called is not made by the programmer at the time of programming. For example the line of code x y where both x and y are variables that represent matrices of integers may have different functionality depending on the context in which the line of code is written. Thus for example if both x and y are small matrices an addition routine that performs a normal matrix addition may be called. However if either x or y represents a matrix larger than a certain size an addition routine that performs the matrix addition in parallel may be called. The decision as to which addition routine is to be called may be made in any suitable way as the invention is not limited in this respect. For example the decision may be made either at compile time at run time or at any other suitable time. Further the decision may be made by any suitable entity such as the compiler or interpreter of the programming language or an outside software program though the compiler interpreter or outside software program may be manually configured e.g. by a programmer to make these decisions in a certain way .

As in the example above operators may also be overloaded. For example the function of the operator in the line of code C A B may be different depending on the data types of the operands A and B. That is if the operands A and B are integers the operator may be defined to perform a integer addition. However if the operands A and B are arrays or matrices the operator may be defined by the programmer to perform a matrix addition.

In one embodiment operators and or methods of a program of scientific computing software application may be overloaded. The overloaded methods and or operators may be defined to pass a message to communication software requesting that a certain operation or process be performed in parallel. Communication software in response to receiving the message may then send a command to server software to perform the process in parallel. This may be done in any suitable way as the invention is not limited in this respect. For example a new data type or class may be defined and operators and or methods may be overloaded such that when the operators and or methods are called with a parameter or operand of the newly defined data type the overloaded method is called.

As an example the rand function in the programming language of a scientific computing software application may be a built in method of scientific computing software application that creates an n by n matrix of randomly valued entries where n is an integer value specified in the parameter of the method. Thus the line of code in Table 3 creates a 100 by 100 matrix of random numbers and stores the result in the variable X. The server software may then pass out tasks to a plurality of processors to complete the task or tasks require by the method.

However the rand method may be overloaded so that if the parameter provided is of the new data type i.e. as opposed to an integer or scalar then the overloaded rand method is called rather than the built in rand method. For example in the line of code in Table 4 the parameter provided to the rand method is 100 p. The variable p may be an object of the new data type or class which for the purposes of this example is called the dlayout class. The operator may be the built in multiplication operator of scientific computing software application which takes two operands. However the operator may be overloaded such that when one or both of its operands are objects of the dlayout class the overloaded operator is called. The overloaded operator may return an object of the dlayout class.

Thus the operation 100 p in Table 4 may return a dlayout object. The rand method may also be overloaded so that when the parameter provided is a dlayout object instead of an integer or scalar the overloaded rand method is called. The overloaded rand method may call communication software which sends a command to server software to create a distributed 100 by 100 matrix. The overloaded rand method may return an object of a user defined data type or class that may be used as a name or handle to reference the matrix created on the parallel server. For example the overloaded rand method may return an object of the user defined ddense class which is stored in the variable X. Thus the distributed matrix on the parallel server may be accessed and manipulated using the variable X. For example as shown in Table 5 a method for computing the eigenvalues of a matrix may be performed on the distributed matrix.

That is the built in method eig of scientific computing software application may take a matrix or an array as its parameter. However the eig function may be overloaded so that if the parameter provided to the method is an object of the ddense class the overloaded method is called. Like the overloaded rand method the overloaded eig method when called may call communication software which may send a command to server software to calculate the eigenvalues of the distributed matrix X. The overloaded eig method may also return an object of the ddense class which is stored in the variable Y and may be used as a handle to access the matrix of eigenvalues on the parallel server.

Because the overloaded methods and or operators used to contact the parallel server as well as the parallel algorithms that execute on the parallel server may be provided for the user of the scientific computing software application i.e. without the user having to code them the user need not have detailed knowledge of parallel programming. Rather much of the parallel processing capability is transparent to the user.

For example continuing with the examples provided above from the user s perspective the only difference in the program code needed to create a 100 by 100 matrix of random numbers and compute its eigenvalues is the addition of the tag p in the parameter of the rand method as shown above in Table 4. However the p tag causes a distributed matrix to be created on the parallel server and future operations on the matrix to be performed in parallel without the user having to devise or code an parallel algorithms. In the examples above the rand and eig methods and the operator were overloaded to perform certain operations in parallel such as creating a distributed matrix on a parallel server and computing its eigenvalues in parallel. However the invention is not limited in this respect as any suitable built in method or operator of the scientific computing software application may be overloaded e.g. to cause its functionality to be performed in parallel .

It should be appreciated that although in the examples above certain data types classes methods and or operators are described as being user defined these data types classes methods and or operators need not be defined by the ultimate end user of the scientific computing software application. Indeed the term user defined is merely used to distinguish between those classes methods and operators that are built in to the scientific computing software application programming language by its publisher s specification and those that are added in by an external programmer. Thus in one embodiment the overloaded methods operators and user defined classes may be provided in a software package that also includes communication software and server software . The software package when installed may work in conjunction with scientific computing software application to provide the above described parallel processing functionality.

Further in the examples above the built in methods and operators were only overloaded with a single additional method or operator. However the invention is not limited in this respect as a method or operator may be overloaded with any suitable number of methods or operators. That is in the example provided above the operator was overloaded with a operator that takes an object of the dlayout class as one of its operands and returns a dlayout object. However the operator could also be overloaded with an operator that takes two ddense objects each of which references a distributed matrix on the parallel server as its operands causes a matrix multiplication of the two distributed matrices to be performed in parallel and returns a ddense object that serves as reference to the distributed matrix that results from the matrix multiplication. Thus in the example above the operator may perform one of three different operations depending on the data types of its operands.

Additionally in the examples above two data types or classes were defined i.e. dlayout and ddense . It should be appreciated that these class names are provided as examples and any suitable class names or data type names may be used. Moreover while in the examples above two user defined data types or classes were used in connection with providing parallel computing capability in scientific computing software application the invention is not limited in this respect as any suitable number of user defined data types or classes may be used. Further any suitable data and methods may be encapsulated in these user defined data types as the invention is not limited in this respect.

In one embodiment scientific computing software application and communication software may execute on the processor of a client computer while server software may execute on multiple processors of one or more parallel servers. However the invention is not limited in this respect as scientific software application and communication software may execute on any suitable processor or processors. For example scientific software application may execute on one of the processors of the parallel server that executes server software . In such embodiments the processor that executes scientific software application and or communication software may be viewed as the client while the remaining processors in parallel server individually and in collections of one or more of them may be viewed as the parallel server. It should be appreciated that the processor that executes scientific software application and or communication software may also execute a portion of server software . Thus this processor may be viewed as the client as well as part of the parallel server.

The parallel server that executes server software may be any suitable type of parallel server as the invention is not limited in this respect. For example the parallel server may be a symmetric multiprocessing SMP system a massively parallel processor MPP system or may be a Beowulf cluster. In this respect it should be appreciated that parallel server may include a plurality of separate machines that are linked together so that they may operate on a single problem in parallel. A system having such an architecture may be viewed as a single parallel server despite being made up of many individual computers that are also capable of operating independently of each other.

In the examples above overloaded methods and operators are used in scientific computing software application to call and or pass messages to communication software which in turn sends a command to parallel server . These software entities may communicate in any suitable way as the invention is not limited in this respect. For example in one embodiment subroutines of communication software may be linked either statically or dynamically into scientific computing software application so that the overloaded methods and or operators of scientific computing software application may directly call these subroutines. In another embodiment interprocess communication may be used to communicate between scientific computing software application and communication software . Any suitable form of interprocess communication may be used such as for example pipes signals message queues semaphores shared memory sockets or any other suitable form as the invention is not limited in this respect.

Because scientific computing software applications are intended primarily for computing and modeling the programming languages of these software applications typically do not provide the capability to perform and or handle communications over a network. Thus in one embodiment communication software may be written in a language e.g. C or Fortran that provides network communication capability. However in embodiments of the invention in which the programming language of scientific computing software application allows a programmer to perform network communications communication software need not be used and the overloaded methods and operators of scientific computing software application may programmed to send commands to server software i.e. without using communication software as an intermediary .

In embodiments wherein scientific computing software application and communication software reside on a separate machine from server software communication software or in embodiments where communication software is not used scientific computing software application may communicate with server software over a network. Any suitable network protocol or protocols may be used as the invention is not limited in this respect. For example in some embodiments the TCP IP protocol may be used. In embodiments wherein scientific computing software application and communication software reside on a processor of the parallel server that executes server software any suitable method of communication between communication software or in some embodiments scientific computing software application and parallel server may be used. For example network based communication may be used even though the communications are not necessarily traveling over a network interprocess communication may be used or any other suitable form of communication may be used as the invention is not limited in this respect.

Applicants have appreciated that solving problems in parallel often involves operating on very large data sets. Consequently passing results between the client and the parallel server each time they are computed may decrease performance. Thus in one embodiment data may remain on the parallel server unless results are explicitly requested to be sent to the client. For example as discussed in above the example line of code in Table 4 causes a command to be sent to the parallel server to create a 100 by 100 matrix distributed matrix of random numbers. The parallel server may create the 100 by 100 matrix in response to the command. If the result of this computation i.e. the 100 by 100 matrix were to be sent back to the client rather than keeping the data on the parallel server 10 000 numbers may have to be sent back to the client. Further future operations on the matrix to be performed by the parallel server may involve sending the matrix data back to the parallel server from the client so that the parallel server may operate on the data. This sequence may increase network traffic latency and overall processing time.

However if the data remains on the parallel server the data need not be transferred unless the user of the scientific computing software application explicitly requests to view the data. For example the 100 by 100 matrix created on the parallel server using the line of code in Table 4 may remain on the parallel server and only a reference to that data may be returned to the scientific computing software application. In the example in Table 4 this reference is stored in the ddense variable X. Thus if it is desired to perform future operations on the matrix the matrix data can be identified and located on the parallel server using this reference.

If a user of a scientific software computing application desires to view a portion of the resulting data the user may request that this data be returned from the parallel server and displayed. This may be done in any suitable way as the invention is not limited in this respect. In some embodiments method and or operator overloading may be used to request that data stored on the parallel server be returned to the client. For example the line of code shown in Table 6 may be used to retrieve and display the first element of the distributed matrix created by the line of code in Table 4. Scientific computing software application may provide for example a method called subsref that is used to access an array or matrix element and invoked by the notation A R C where A is a variable that represents the array or matrix and R C refers to the row and column of the requested matrix element. The subsref function may be overloaded so that if called with a ddense variable as its parameter instead of an array or matrix a command may be sent to the parallel server to return the requested element of the matrix that is referenced by the ddense variable.

Server software may operate in any suitable way to receive commands from the client and perform operations in parallel as the invention is not limited in this respect. is but one example of a suitable configuration of server software for at least some embodiments. In parallel server includes a plurality of processors . . . and a memory . Each of processors includes four software modules client managers . . . server managers . . . library managers . . . and data managers . . . . Memory of parallel server stores a plurality of software libraries . . . that include routines for performing various operations.

Client managers interface with clients and provides routines for reading commands and arguments from a client and sending results and other data back to the client. Server managers handle communications between server processes executing on different processors of parallel server . That is server managers manage the transmission of data between the processors of parallel server and collect results and error codes from the processors. Library managers are responsible for maintaining a list of available software libraries and the routines provided by them. When instructed by a server manager library manager may perform a call to a routine of one of the libraries . Data managers include routines for creating deleting and changing data stored on the parallel server. Data managers maintain a mapping between the references and or identifiers used by the client to identify the data and the actual storage location of the data.

In one embodiment when server software is initialized one processor may be designated the head processor and all or some of the remaining processors may be designated slave processors that operate under the control of the head processor. The head processor may serve as the central processor and may receive the commands from the client. Thus in one embodiment only the client manager module on the head processor is used. When the head processor receives a command from the client the server manager on the head processor may send messages to the slave processors to perform the desired operation. The library managers on each of the processors may call the appropriate routine in libraries .

The server managers on the processors may communicate with each other in any suitable way as the invention is not limited in this respect. In one embodiment the message passage interface MPI application programming interface API may be used. Alternatively the parallel virtual machine PVM API or any other suitable from of communication may be used.

In the example of parallel server is shown having a memory which stores software libraries . Memory may be assembled from any suitable type of memory or memories and may include for example non volatile storage media such as magnetic disk optical disc or tape and or volatile memory such as random access memory RAM . Further in parallel server is shown having a single memory that is shared by all processors. However the invention is not limited in this respect as each processor may have a separate memory or clusters of processors may share memory. For example in embodiments in which parallel server is implemented as a Beowulf cluster each processor may have a separate memory and a separate copy of libraries or parts thereof . Further in some embodiments in which processors share a memory portions of the memory may be allocated to each processor for exclusive use by that processor. Thus each processor may maintain a separate copy of libraries in the portion of the memory allocated to it.

In the example of parallel server includes processors . It should be appreciated that the parallel server may have any suitable number of processors and the invention is not limited in this respect. Similarly memory may store any suitable number of software libraries. Examples of software libraries that may be used are ScaLAPACK and the Parallel Basic Linear Algebra Subprograms PBLAS library both of which include parallel linear algebra routines.

In the example of server software is implemented in part as four software modules on each processor. It should be appreciated that the invention is not limited in this respect as server software may be implemented in any suitable way.

Further it should be understood that the four modules in the above described examples are intended to illustrate functionally how server software may operate. However the four modules need not be implemented as separate computer programs and may be implemented in any suitable way as the invention is not limited in this respect.

During execution of a program in which the above described parallel computing capabilities are used memory may be dynamically allocated on both the client and the parallel server. As is well known this may result particularly in an object oriented programming language in memory being allocated to objects and processors no longer in use. Thus garbage collection techniques may be used on both the client and the parallel server to free up such memory that has been dynamically allocated. Some programming languages provide automatic garbage collection some provide for manual garbage collection and some provide no garbage collection at all. In a programming language that provides an automatic garbage collector the programmer may define a destructor method in a class definition. When an object of the class is instantiated memory is dynamically allocated to store the object. The garbage collector automatically determines when the object is no longer needed i.e. goes out of scope and calls the object s destructor method. An example of a programming language that provides automatic garbage collection is the JAVA programming language. In programming languages that provide for manual garbage collection a destructor method may be called explicitly by the programmer i.e. in the program . An example of a programming language that provides for manual garbage collection is the C programming language.

In some embodiments of the invention the programming language of scientific computing software application may provide automatic garbage collection. However the automatic garbage collector only frees memory allocated on the client but does not free memory allocated on the parallel server. For example the line of code in Table 4 creates a ddense object X on the client that serves as reference to a distributed 100 by 100 matrix of random numbers that is created on the parallel server. Thus it is desired to free the memory used by the object X on the client and to free the memory used by the distributed matrix referenced by the object X on the parallel server. is a flow chart that shows an example of a process to accomplish these tasks.

In at act the object X goes out of scope as computer scientists say. That is the object is no longer needed. This condition is ascertained by the garbage collector and at act the garbage collector may call the object s destructor method. The object s destructor method may be defined e.g. in the class definition to call a routine of communication software which sends a command to the server software to free the memory on the parallel server used to store the distributed matrix. Thus the process continues to act where the object s destructor calls communication software . The destructor may pass the reference information stored in the object that identifies the matrix on the parallel server. The process next continues to act where the garbage collector frees the memory on the client used to store the object. At act communication software may send a command to the parallel server to free the memory on parallel server used to store the matrix referenced by the object X. The command may include a reference to the matrix that the parallel server may use to identify the matrix and the memory locations that should be freed. The process then continues to act where after receiving the request to free the memory parallel server frees the memory used to store the distributed matrix.

After receiving the command from communication software the parallel server may free the memory in any suitable way as the invention is not limited in this respect. For example the command from communication software may be received by the head processor of the parallel server. The head processor may then send a message to each slave processor to free the memory used to store the portion of the distributed matrix controlled by the respective slave processor.

In the example of memory on the client used to store an object that referenced a distributed matrix and memory on the parallel server used to store the distributed matrix were freed. However it should be appreciated that the garbage collection process described in is not limited to use with freeing memory that stores objects which reference distributed matrices i.e. on the client and or distributed matrices themselves i.e. on the parallel server . Indeed memory that stores any suitable type of data may be freed. The data need not be an object or a matrix but can be any type of data structure mathematical construct or any other type of data as the invention is not limited in this respect.

In some other embodiments of the invention the programming language of scientific computing software application does not provide a garbage collector. However scientific computing software application includes an interpreter or virtual machine of a different programming language that does provide automatic garbage collection. Thus scientific computing software application allows a programmer not only to create objects and run code in the programming language of scientific computing software application but also to create objects and run code in the additional programming language. An example of a scientific computing software application that includes an interpreter or virtual machine of a different programming language is MATLAB from The MathWorks Natick Mass. MATLAB includes a JAVA Virtual Machine JVM and allows the programmer inter alia to construct and use JAVA objects and call JAVA objects and methods.

Thus as shown in code may be input to scientific computing software application by a user through the user interface of the scientific computing software application. The code may be processed either by the interpreter of scientific computing software application or the interpreter or virtual machine of the secondary programming language based on whether the code is code in the programming language of scientific computing software application or code in the secondary programming language.

In some embodiments the garbage collector of a secondary programming language virtual machine may be used in freeing on a parallel server memory that stores data that is no longer needed. This may be done in any suitable way as the invention is not limited in this respect. For example the programming language scientific computing software application does not provide for destructor methods that are automatically called when an object goes out of scope as this programming language does not provide automatic garbage collection. Thus in one embodiment an object of the secondary programming language may be embedded in the object of scientific computing software application programming language.

For example the ddense class described above may be defined in the scientific computing software application programming language. The class definition may include an object of the secondary programming language. Thus each time an object of the ddense class is instantiated an object of the secondary programming language may also be instantiated and the instantiated object of the ddense class e.g. that serves as a reference to a distributed matrix on the parallel server may have a reference to the object of the secondary programming language. Because the secondary programming language provides for automatic garbage collection the object of secondary programming language may include a destructor method that is called whenever the object goes out of scope. Because the object of the secondary programming language is referenced by the ddense object of the scientific computing software application the object of the secondary programming language may not go out of scope until the ddense object has gone out of scope. Thus the garbage collector will not call the destructor of the object of the secondary programming language until after the ddense object has gone out of scope.

In some embodiments the destructor method of the object of the secondary programming language may include a call to communication software indicating that the memory referenced by the ddense object on the parallel server may be freed. In response to this call communication software may send a command to server software to free the memory used by the matrix that is referenced by the ddense object. In some embodiments the destructor method of the object of the secondary programming language may not call communication software to instruct parallel server to free the memory but rather may add an identifier that identifies the data on the parallel server to be garbage collected to a list of identifiers. A routine of the programming language of scientific computing software application may subsequently pass each identifier in the list to communication software which may in response send a command to parallel server to free the memory associated with each identifier.

In some embodiments the garbage collector of the secondary programming language may be explicitly invoked in addition to being automatically invoked by the interpreter or virtual machine. For some scientific computing software applications that include an interpreter or virtual machine of a secondary programming language it may sometimes be unclear when the garbage collector of the secondary programming language is automatically invoked. Thus determining when and how frequently garbage collection is performed may present challenges. Sometimes the garbage collector may be invoked explicitly at intervals. This may be done in any suitable way as the invention is not limited in this respect.

An example of a process for explicitly invoking the garbage collector at intervals is shown in . In at act an overloaded method is called. Each time an overloaded method that is used in providing parallelism through polymorphism as discussed above is called a counter may be incremented. This may be done in any suitable way as the invention is not limited in this respect. For example the overloaded method may include a call to a method with increments the counter. Thus the process continues to act where the counter is incremented. The counter may be implemented in any suitable way as the invention is not limited in this respect. For example the counter may be implemented as a global variable or in any other suitable way.

The process then continues to act where it is determined whether the counter meets or exceeds a predefined threshold. In the example of the predefined threshold is however any suitable threshold may be used as the invention is not limited in this respect. If the counter does not meet or exceed the predefined threshold the process returns to the start i.e. until another overloaded method is called . If the counter does meet or exceed the threshold the process continues to act where the counter is reset e.g. to zero and the garbage collector of the secondary programming language is explicitly invoked. The garbage collector may be explicitly invoked in any suitable way as the invention is not limited in this respect. In one embodiment wherein the secondary programming language is the JAVA programming language the line of code in Table 7 may be used to explicitly invoke the garbage collector.

When the garbage collector is invoked processing continues to act where the garbage collector calls the destructor methods of objects of the secondary programming language that have gone out of scope. The process then continues to act wherein each destructor method that has been called adds to a list an identifier that identifies the data on the parallel server to be garbage collected. The list may be implemented in any suitable way as the invention is not limited in this respect. In one embodiment the list may be implemented as a data structure in scientific computing software application . Alternatively the list may be implemented as a file that is external to scientific computing software application .

As discussed above a separate routine may subsequently pass each identifier in the list to communication software which may send a command to server software to free the appropriate memory.

In the examples above when an object is garbage collected all the memory on the parallel server used to store the object s data is reallocated. As used herein the term reallocated refers to indicating that memory previously allocated to one or more processes to store data is no longer allocated for that purpose and is available for other use.

It should be appreciated that when an object is garbage collected all of the memory used to store the object and its data need not be reallocated at once as the invention is not limited in this respect. For example as shown in parallel server includes four processors i.e. processors and . Each of the processors may control a portion of the parallel server s memory and an object stored on the parallel server may be distributed such that a portion of the object s data may be stored in each of the processor s memory though it should be appreciated that object need not be distributed across all of the processors in the parallel server and may be distributed across only a portion of the processors as the invention is not limited in this respect .

Thus in one embodiment when a command is received by the parallel server to garbage collect a particular object and the command is sent to each processor each processor may individually determine when to reallocate the memory used to store the portion of the object s data under its control. Thus for example processor may reallocate the memory used to store its portion of the data immediately upon receiving the garbage collection command while processor may reallocate the memory used to store its portion of the data several hours later. Processor may reallocate this memory at a later time for any suitable reason as the invention is not limited in this respect. For example processor may be busy performing another operation and may wait until its processing load has decreased before reallocating the memory. As another example processor may not need to re use this memory in the near future as it has plenty of free memory available. Thus processor may wait to free this memory until its amount of free memory has dropped below a particular threshold.

In some embodiments rather than receiving a command from the client to garbage collect a particular object after it has gone out of scope each processor in the parallel server may individually determine when an object for which it stores all or a portion of the data has gone out of scope and may reallocate the memory used to store the object or the portion of the object when it has determined that the object is out of scope. This may be done in any suitable way as the invention is not limited in this respect.

In one embodiment a copy of the program to be executed may be sent to each processor. Further in addition to a scientific software computing application executing on the client each processor in the parallel server may execute the scientific software computing application. Thus each processor has a full interpreter to process the program. Each processor may use the interpreter i.e. of the scientific computing software application and the copy of the program to determine when objects or variables have gone out of scope. When a processor has determined that an object has gone out of scope it may free the memory used to store its portion of the object.

In some of the example above routines of a scientific computing software application were overloaded to enable performance of these routines on a parallel server. However the invention is not limited to use with a scientific software application as the above described techniques for enabling performance of a routine or process in parallel may be employed with any suitable software entity such as other types of software application programs drivers operating systems or other software programs.

In some embodiments routines of the operating system may be overloaded so that functionality provided by the operating system may be performed in parallel. As used herein the term operating system refers to one or more software programs that control and manage hardware resources and basic system operations. Examples of some operating systems include the Windows operating system produced and sold by Microsoft Corporation Redmond Wash. the MAC OS operating system produced and sold by Apple Computer Inc. Cupertino Calif. the Solaris operating system produced and sold by Sun Microsystems Inc. Santa Clara Calif. and the Linux operating system produced and sold by various vendors.

As shown in application programs . . . and operating system may execute on a client machine. Operating system may communicate with parallel server to enable performance of some operating system functionality in parallel. It should be appreciated that the client machine may be a machine separate from the parallel server or may be one of the processors of parallel server . Routines of operating system may be overloaded such that when the overloaded routine is called a message is sent to parallel server to perform the process of the routine in parallel. This may be done in any suitable way as the invention is not limited in this respect.

Additionally any suitable operating system routines may be overloaded to enable any suitable process to be performed in parallel as the invention is not limited in this respect. In one embodiment memory management routines may be overloaded. For example memory management routine s of the operating system may be overloaded to enable allocation and destruction of memory on the parallel server. When an application program requests that the operating system allocate memory to it the operating system s memory allocation routine s may be overloaded so that instead of allocating memory on the local processor memory is allocated on the distributed processor. Similarly the operating system s memory destruction routine s may be overloaded such that when an application program indicates to the operating system that it no longer needs the memory allocated to it the memory allocated on the parallel server is reallocated.

Operating systems may provide a variety of other services in addition to memory management and the number of services that operating systems provide to application programs continues to expand. Applicants have appreciated that it may be desirable to perform some of these services in parallel. Thus the operating system s routines that perform a service may be overloaded using the above described techniques to enable parallel performance of the service.

In some embodiments the operating system may make a service available to application programs through an application programming interface API . An API is a set of function definitions e.g. function name and parameters that one software program may call to interact with another software program. Thus for example by calling the routines for a particular service an application program may indicate to the operating system to perform the service. In some embodiments to enable parallel performance of an operating system service the functions or routines defined in the API may be overloaded. However the invention is not limited in this respect as any suitable function or routine may be overloaded to enable parallel performance.

The above described embodiments of the present invention can be implemented in any of numerous ways. For example the embodiments may be implemented using hardware software or a combination thereof. When implemented in software the software code can be executed on any suitable processor or collection of processors whether provided in a single computer or distributed among multiple computers. It should be appreciated that any component or collection of components that perform the functions described above can be generically considered as one or more controllers that control the above discussed functions. The one or more controllers can be implemented in numerous ways such as with dedicated hardware or with general purpose hardware e.g. one or more processors that is programmed using microcode or software to perform the functions recited above.

In this respect it should be appreciated that one implementation of the present invention comprises at least one computer readable medium e.g. a computer memory a floppy disk a compact disk a tape etc. encoded with a computer program i.e. a plurality of instructions which when executed on a processor performs the above discussed functions of the embodiments of the present invention. The computer readable medium can be transportable such that the program stored thereon can be loaded onto any computer environment resource to implement the aspects of the present invention discussed herein. In addition it should be appreciated that the reference to a computer program which when executed performs the above discussed functions is not limited to an application program running on a host computer. Rather the term computer program is used herein in a generic sense to reference any type of computer code e.g. software or microcode that can be employed to program a processor to implement the above discussed aspects of the present invention.

It should be appreciated that in accordance with several embodiments of the present invention wherein processes are implemented in a computer readable medium the computer implemented processes may during the course of their execution receive input manually e.g. from a user and or via external electrical connection .

The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of including comprising having containing involving and variations thereof is meant to encompass the items listed thereafter and additional items.

Having described several embodiments of the invention in detail various modifications and improvements will readily occur to those skilled in the art. Such modifications and improvements are intended to be within the spirit and scope of the invention. Accordingly the foregoing description is by way of example only and is not intended as limiting. The invention is limited only as defined by the following claims and the equivalents thereto.

