---

title: Scalable method and system for streaming high-resolution media
abstract: A system and method for distributing data (e.g., imaging data such as pixels, or 3D graphics data such as points, lines, or polygons) from a single or a small number of data sources to a plurality of graphical processing units (graphics processors) for processing and display is presented. The system and method provide a pipelined and multithreaded approach that prioritizes movement of the data through a high-speed multiprocessor system (or a high-speed system of networked computers), according to the system topology. Multiple threads running on multiple processors in shared memory move the data from a storage device (e.g., a disk array), through the high-speed multiprocessor system, to graphics processor memory for display and optional processing through fragment programming. The data can also be moved in the reverse direction, back through the high-speed multiprocessor system, for storage on the disk array.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07460126&OS=07460126&RS=07460126
owner: Silicon Graphics, Inc.
number: 07460126
owner_city: Sunnyvale
owner_country: US
publication_date: 20050824
---
This application claims the benefit of U.S. Provisional Application No. 60 603 721 entitled METHOD AND SYSTEM FOR USING SCALABLE GRAPHICS TO STREAM HIGH RESOLUTION IMAGERY FOR DISPLAY OR PIXEL PROCESSING filed Aug. 24 2004 which is incorporated by reference herein in its entirety.

Conventional shared memory multiprocessor systems can be too slow for processing and display of high resolution and large scale imagery or graphics such as images captured by satellites deep space telescopes or on board space vehicle cameras as well as high resolution movies. Thus what is needed is a method and system for efficiently distributing high resolution and large scale image and graphics data with associated audio data through a shared memory multiprocessor system for processing and or display.

A system and method for distributing data e.g. imaging data such as pixels or 3D graphics data such as points lines or polygons from a single or a small number of data sources to a plurality of graphical processing units graphics processors for processing and display is presented. The system and method provide a pipelined and multithreaded approach that prioritizes movement of the data through a high speed multiprocessor system or a system of high speed networked computers according to the system topology. Multiple threads running on multiple processors in shared memory move the data from a storage device e.g. a disk array through the high speed multiprocessor system to graphics processor memory for display and optional processing through fragment programming. The data can also be moved in the reverse direction back through the high speed multiprocessor system for storage on the disk array.

In one embodiment a pipelined multithreaded system for prioritizing movement of data includes one or more central processing units CPUs each CPU executing one or more threads a plurality of media processors and a shared memory. The threads include a read controller thread that controls one or more reader threads a plurality of processing controller threads each processing controller thread generating a plurality of processing threads and a rendering controller thread that controls a plurality of renderer threads. Each reader thread receives a piece of source image data and loads the piece into the shared memory. Each processing controller thread receives a piece from a corresponding reader thread. Each processing thread extracts a plurality of sub pieces from each piece and moves the sub pieces through the system. Each renderer thread receives a sub piece from a corresponding processing thread and downloads the sub piece to an appropriate media processor for synchronized output of the sub pieces.

In another embodiment the threads include a read controller thread that controls a plurality of reader threads and a rendering controller thread that controls a plurality of renderer threads. Each reader thread receives a piece of source image data and loads the piece into the shared memory. Each renderer thread receives a piece from a corresponding reader thread and downloads the piece to an appropriate media processor for synchronized output of the pieces.

In yet another embodiment a pipelined multithreaded method for prioritizing movement of data includes receiving source image data as one or more pieces processing the pieces on appropriate CPUs downloading the sub pieces to corresponding media processors and outputting synchronized sub pieces. For each piece a reader thread is generated that loads the piece into a shared memory. One or more processing controller threads receive the pieces from corresponding reader threads. Each processing controller thread generates a plurality of processing threads that extract sub pieces for each piece. The processing threads move the sub pieces to appropriate rendering nodes and renderer threads executing on the CPUs receive the sub pieces from corresponding processing threads.

In a further embodiment a pipelined multithreaded method for prioritizing movement of data includes receiving source image data as a plurality of pieces moving the pieces to appropriate rendering nodes downloading the pieces to appropriate media processors and outputting synchronized pieces. For each piece a reader thread is generated that loads the piece into a shared memory and a plurality of renderer threads that each receive the pieces from corresponding reader threads.

In a still further embodiment a pipelined multithreaded method for prioritizing movement of data includes receiving source image data as one or more pieces processing the pieces on appropriate CPUs downloading a quantity of sub pieces to an equal quantity of graphics processors processing the sub pieces on the graphics processors and repeating the downloading and processing steps until a total quantity of sub pieces is processed. For each piece a reader thread is generated that loads the piece into a shared memory. One or more processing controller threads receive the pieces from corresponding reader threads. Each processing controller thread generates a plurality of processing threads that extract a plurality of sub pieces for each piece. The processing threads move the sub pieces to appropriate rendering nodes and a plurality of renderer threads receive the sub pieces from corresponding processing threads. In this embodiment the total quantity of sub pieces exceeds the quantity of graphics processors.

This specification discloses one or more embodiments that incorporate the features of this invention. The embodiment s described and references in the specification to one embodiment an embodiment an example embodiment etc. indicate that the embodiment s described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is understood that it is within the knowledge of one skilled in the art to effect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

Embodiments of the invention may be implemented in hardware firmware software or any combination thereof. Embodiments of the invention may also be implemented as instructions stored on a machine readable medium which may be read and executed by one or more processors. A machine readable medium may include any mechanism for storing or transmitting information in a form readable by a machine e.g. a computing device . For example a machine readable medium may include read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices electrical optical acoustical or other forms of propagated signals e.g. carrier waves infrared signals digital signals etc. and others. Further firmware software routines instructions may be described herein as performing certain actions. However it should be appreciated that such descriptions are merely for convenience and that such actions in fact result from computing devices processors controllers or other devices executing the firmware software routines instructions etc.

In one embodiment system includes a single system image computer e.g. a computer workstation equipped for high performance graphics having a redundant array of inexpensive disks RAID storage subsystem for storage device a non uniform memory access system architecture NUMA memory for shared memory graphics processors for media processors and multiple digital video interactive DVI outputs for media outputs . A multithreaded program allows system to display a single image stream e.g. a movie to DVI outputs .

The multithreaded program reads a single image data file from storage device by dividing the larger single image into smaller image pieces and loads the pieces into shared memory . The multithreaded program processes the pieces on CPUs optionally divides each of the image pieces into smaller sub pieces and moves the pieces or sub pieces to graphics processors . Each graphics processor processes a different piece or sub piece. Processing by graphics processors may include for example performing convolution color correction smoothing edge detection and orientation changes among other processing techniques.

After graphics processors process the pieces or sub pieces the image stream can be output to associated media outputs in various ways. For example a power wall of multiple displays each displaying a different image piece or sub piece can be used to view a composite image stream such as in a command and control military or rescue operations center. Also a projector having multiple inputs can be used to combine the image pieces or sub pieces and project the image stream as a single larger image. After graphics processors process the image pieces or sub pieces the processed pieces and sub pieces can be optionally transferred back into shared memory re combined and stored as a single larger image on storage device .

By converting pixel data in multiple threads the systems and methods described herein also optimize image download to OpenGL or OpenML . OpenGL is an Application Programming Interface API developed by Silicon Graphics Inc. Mountain View Calif. for the development of 2D and 3D graphics applications. For more detailed information about OpenGL see The OpenGL Graphics System A Specification Version 2.0 October 2004 by Mark Segal and Kurt Akeley available from Silicon Graphics Inc. and also available from http www.opengl.org which is incorporated herein by reference. OpenML is an Application Programming Interface API also developed by Silicon Graphics for the development of video and audio applications. For more detailed information about OpenML see OpenML V1.0 Specification Jul. 19 2001 edited by Steve Howell available from Khronos Group at http www.khronos.org which is incorporated herein by reference.

Two example implementations of a shared memory multiprocessor system for streaming high resolution media are described below followed by a discussion of example pipelined multithreaded methods for prioritizing movement of high resolution media through a shared memory multiprocessor system.

The system and method described above for prioritizing movement of data through a shared memory multiprocessor system can be used for streaming image processing and in particular high resolution movie playback in real time. An example implementation for playback of a 4096 by 3112 pixel movie is described below in conjunction with . This implementation streams a series of 4096 by 3112 pixel images i.e. frames from a storage device through a scalable system to sixteen synchronized media outputs which output video. The term 4K is used for several different resolutions in this example it denotes 4096 by 3112. Each pixel contains three 10 bit components packed into a 32 bit quantity and thus a complete image requires a little more than 50 MB of storage. Thus to display 24 frames per second approximately 1.23 GB per second must be streamed from the disk to the output device.

A block diagram of a system in accordance with the first example implementation is shown in . System has a datacentric pipelined multithreaded design that prioritizes the movement of data through the system. System includes storage device which stores a source image i.e. a 4K movie CPUs shown in shared memory shown in graphics processors shown in and media outputs .

System is implemented as a pipeline with three stages a reading stage a processing stage and a rendering stage . Each stage in the pipeline consists of multiple threads each thread performing that stage s operation on one piece called a stripe or sub piece called a tile of source image . Each of the threads in a stage is connected to an appropriate thread in the next stage with a thread safe queue. Each of the threads in a stage is also locked to a CPU appropriate to that thread s data locality requirements and memory used by that thread may be allocated on the node containing the CPU executing that thread or on the node containing the CPU executing the thread for the prior or subsequent pipeline stage.

Each image stripe or tile passing through the pipeline carries an image number used for synchronization when necessary. In this example the tile placement in a composite output image is implied by the ordering of the output device which is a 4 input tiled video projector. In a more general implementation tiles could include a destination location and a size relative to the composite output image.

In this implementation each stage operates on a current target image number defined by a global clock and a stage specific time offset. This offset is defined in a text configuration file and can be configured to tune the buffering in the pipeline to reduce sensitivity to performance of the individual threads .

Each of the stages is described in detail below with respect to a system having sixteen graphics processors . The number of threads and image regions in system as well as CPUs used to execute threads are configured using a text file and can be retargeted for smaller or larger images in this implementation. A different implementation might use an automatic configuration based on discovery of the source image and destination output characteristics.

The first stage in the pipeline is reading stage shown in more detail in . As illustrated in the example of reading stage contains five threads one read controller thread and four reader threads and . Read controller thread controls reader threads . Read controller thread identifies the most appropriate image to read for the current time and synchronizes reader threads so that one thread does not get significantly out of step with the others.

Reader threads read corresponding stripes and of 4096 by 3112 source image file from disk and load the stripes into memory shown in . In this example stripes have the same width as source image and one quarter the height i.e. 4096 by 778 pixels . Once stripes are loaded into memory a pointer to each stripe is placed in a corresponding stripe queue and to the next stage i.e. processing stage . For example as shown in stripe queue has in its queue stripe for frame of source image a stripe for frame of source image and a stripe for frame of source image .

Reading stage has high bandwidth storage capable of serving read requests for a part of source image file to each reader thread . The read requests are a little more than 12.5 MB each and can be sequential. In this example reading stage is implemented with XFS over fiber channel to one or more appropriately sized RAIDs. XFS which is available from Silicon Graphics Inc. SGI Mountain View Calif. is the native file system for SGI computers and is fully supported by the open source community. In other implementations reader threads might receive source image data from a network device such as gigabit Ethernet or 10 gigabit Ethernet or from other CPUs creating image data such as a high resolution ray tracer or from a video input device such as a composite video input PCI card.

In this implementation multiple threads are also slaved to each reader thread using asynchronous I O. Block data copies to the buffer cache are also avoided by using direct I O.

The second stage in the pipeline is processing stage shown in more detail in . Processing stage includes four processing controller threads and and sixteen processing threads . Processing controller threads each receive corresponding stripes from reader threads . Then each processing controller thread generates four processing threads to extract tiles from each stripe . For example as shown in processing controller thread generates four processing threads and to extract four tiles and from stripe of frame of source image . In this example a tile has the same height as a stripe and one quarter the width i.e. 1024 by 778 pixels .

Each processing thread copies a tile out of a source stripe and queues the tile for the next stage i.e. rendering stage in a tile queue. For example as shown in processing thread copies tile out of stripe and queues tile in a tile queue . Tile queue has in its queue the following tiles tile of stripe for frame of source image a tile of stripe for frame of source image and a tile of stripe for frame of source image .

Each processing thread performs a memcpy operation for each tile row out of each stripe row. In another implementation processing threads could use NUMA operations to block copy or Direct Memory Access DMA the row. In another implementation processing threads can also convert the pixels in a tile between pixel types and formats or perform other operations such as filtering or decimation.

In this example processing threads are primarily responsible for moving data as fast as possible from the reader node to a renderer node. In this way each processing thread makes its tile local to the OpenGL context that is loading the tile pixels into a texture. Each processing thread also makes the output image contiguous so that the FGL DYNAMIC TEXIMAGE extension can be used to skip one copy of the image data internal to OpenGL and increase texture download speed.

The third stage in the pipeline is rendering stage shown in more detail in . Rendering stage includes one rendering controller thread and sixteen renderer threads . Renderer threads receive tiles synchronize among themselves so that the composite image formed from the tiles is consistent choose the most correct tile for the current adjusted time and download the tile data to texture e.g. using glTexSubImage2D and render a textured quad. For example as shown in renderer thread receives tile at rendering node downloads tile data to texture using glTexSubImage2D and renders a textured quad.

The output portion of rendering stage includes sixteen graphics processors display pipes each having an associated media output which is a video display in this example. Each display shows one tile from source image . For example as shown in graphics processor processes tile for output on display .

Display pipes must show tiles from the same source image or visible tearing will occur at tile boundaries. Thus logic in rendering controller thread determines the most correct image number for the current time queries reader threads to determine which threads have been copied to rendering nodes and chooses the most recent complete image to render that is not more recent than this correct image number. Each renderer thread opens its own OpenGL context so that OpenGL memory is local to that node.

Buffer swaps can be either locked to the video signal s vertical blanking interval e.g. using  GL SYNC TO VBLANK or xsetmon or can be internally timed to float very close to 24 Hz. The video timing can be configured for 48 Hz thus every other video frame would start at the correct time. Graphics pipes can also be genlocked using an ImageSync card available from Silicon Graphics for fully locked video and frame synchronization.

Buffering is required because each stage of system can run out of sync with the real 24 Hz frame clock causing the playback to appear to stutter. The queue lengths and clock offsets can be configured during initialization however so the latencies can be set to buffer this stuttering.

As described above the goal of this first example implementation is to run all pipeline stages at 24 frames per second all the time. For example if reader threads run too slowly a frame will be skipped and eventually a newer frame will be read. When the newer frame appears at the rendering stage the renderers will skip from the most recent complete frame to the newer frame. Likewise if renderer threads run too slowly they will discard every frame in the pipeline except the newest. At worst system will at least keep up with the wall clock even if it does not show every frame in the movie.

Each pipeline stage and is connected to the next via a thread safe queue not shown containing some number of slots e.g. 32 slots for image tiles. The buffering system is made thread safe using UNIX semaphores and locking with pthreads mutex objects. A pipeline stage can enqueue tiles into its output queue until the queue is full. Thus we say the pipeline stages are buffered so that a stall at one stage does not cause the previous stage to stall as well.

In this example the runtime configuration is driven from two text files. The first file describes the movie frames. The second file describes an object for each thread in the running system including the depth of queues the frame time offsets the CPU assigned to execute the thread and whether that CPU is to be excluded from other threads that are not assigned to a particular CPU e.g. kernel system threads GUI etc. . Each individual thread object can lock itself to a CPU e.g. using the pthread setrunon np call and isolate the processor on which it runs e.g. using sysmp . Thus in this way system performs a mapping of stages and to CPUs and emits the thread configuration file which can then be customized by a user depending on the topology of the installed system.

A test implementation showed that a 32 processor 16 pipe Onyx4 system can reliably render a 4096 by 3112 pixel movie at 24 frames per second. The Onyx4 UltimateVision system for advanced visual computing is available from Silicon Graphics .

In the first example implementation described above there are forty three threads one main thread not shown in one read controller thread four reader threads four processing controller threads sixteen processing threads one rendering controller thread and sixteen renderer threads . The main thread controller threads and and reader threads do not utilize one hundred percent of a CPU i.e. they mostly calculate frame numbers collate results and wait on semaphores . Thus in an example configuration the main thread controller threads and and reader threads are spread out and locked down on even numbered CPUs sharing time with processing threads which are also locked down on even numbered CPUs . Renderer threads are locked on odd numbered CPUs .

Another similar configuration showed that a 28 processor 12 pipe Onyx4 system can render a 4096 by 2180 pixel movie with even more CPU cycles free. Renderer threads are sleeping much of the time waiting on the completion of DMA from main memory to the graphics card. Thus in one example configuration processing threads are on the same CPUs as renderer threads successfully using the CPU to copy tile data while renderer threads were waiting on DMA to complete.

While this test implementation was performed using an Onyx4 system the example streaming architecture for playback of a 4K movie can also be implemented using other systems.

Second Example Implementation Playback of a 3840 by 2160 Pixel 4K Movie Using 4 Graphics Processors and 1 Audio Processor

A second example implementation is described below in conjunction with . This implementation streams a series of 3840 by 2160 pixel images from a RAID storage device through a scalable system to five synchronized OpenML devices four of which output video and one of which outputs audio. As described above the term 4K is used for several different resolutions in this example it denotes 3840 by 2160. On disk each pair of pixels is represented by four 10 bit components and each component is packed into memory directly after the previous component with no padding. Thus in this example a complete image requires a little more than 20 MB of storage. Thus to display 24 frames per second approximately 480 MB per second must be streamed from the disk to the output device.

As in the previous example a read controller thread synchronizes and directs reader threads to issue reads from source image . Each block read is executed in a separate subthread in order to perform asynchronous I O. Each subthread also uses direct I O in order to avoid a redundant copy into the buffer cache not shown .

Each renderer thread video renderer threads and audio renderer thread opens a corresponding OpenML device video devices and audio device as specified in the text configuration file. Renderer threads then synchronize with a sixth media player management thread not shown . If the management thread calculates that the five rendering threads did not start at the same time as described below then management thread instructs renderer threads to close and reopen their corresponding OpenML devices until the devices are synchronized.

Like the first example implementation described above the second example implementation uses a buffering system between pipeline stages and . However this implementation does not attempt to keep up with any wall clock. Instead in this implementation it is assumed that reading stage can deliver each frame more than fast enough on average renderer threads can download image data more than fast enough on average and output devices and remain synchronized with each other and output one frame of data per time allocated to a single frame. At 24 frames per second these assumptions imply that the reading and rendering operations must complete in 41 milliseconds or less.

Synchronization in the output devices is ensured by a master clock which drives the internal clocks of audio device and video devices . Thus the scan out of four video frames and the output of the beginning of a frame s worth of audio data start at the same time down to the resolution of the master clock. The five output devices are opened with blank data and the time at which they output the first frame of blank data is recorded. If these times are not coincident with the same master clock pulse then the devices are closed and opened again based on the assumption that the devices will eventually be synchronized .

Reading stage is configured with its per stage time 48 frames in the future e.g. 2 seconds at 24 frames per second thus reader threads can read and enqueue at least 48 frames before renderer threads begin feeding those images to display devices . As mentioned above reading stage and rendering stage have to complete their operation for a frame in less than approximately 41 milliseconds. As long as this rate is sustained the queues become full and remain full.

Because the queue between reading stage and rendering stage contains 48 frames when the queue is full system can recover from up to two seconds of delays in reading stage . In this implementation when the buffer becomes empty the program simply stops.

The examples of streaming architecture for playback of a 4K movie described above can be adapted to the display of many individual image streams arranged for example to cover the room of a planetarium. The input stream can also be from a series of individual frames i.e. a movie from a collection of several movies or can be the collated output of a large number of CPUs creating 2D imagery. A further adaptation of the streaming architecture is to mix various movie streams i.e. image fusion data fusion or simply fusion . While the example streaming architectures described above did not incorporate read back e.g. glReadPixels another adaptation can download data perform pixel processing read back the data and store the results on the same or another disk array or other storage device .

In step source image data is received as one or more pieces by generating a reader thread for each piece that loads the piece into a shared memory. For example as shown in four reader threads receive pieces i.e. stripes of source image which is a frame of a 4K movie file stored on storage device . Stripes are loaded into shared memory shown in .

In step the pieces are processed on appropriate CPUs by using one or more processing controller threads to receive the pieces from corresponding reader threads generating a plurality of processing threads to extract a plurality of sub pieces for each piece and moving the sub pieces to appropriate rendering nodes. For example as shown in processing controller threads receive stripes from reading stage . Each processing controller thread generates four processing threads to extract four tiles from each stripe and move the tiles to rendering stage .

In step the sub pieces are downloaded to appropriate media processors using a plurality of renderer threads to receive the sub pieces from corresponding processing threads. For example as shown in renderer threads receive tiles from processing stage and download the tiles to graphics processors .

In step synchronized sub pieces are output via media outputs associated with the media processors. For example as shown in each graphics processor has an associated video display . Each video display displays a tile forming a composite output image stream.

In step source image data is received as a plurality of pieces by generating a reader thread to receive each piece. For example as shown in four reader threads receive pieces of source image .

In step the pieces are moved to appropriate rendering nodes. For example as shown in reading stage moves the pieces to rendering stage .

In step the pieces are downloaded to media processors by using a plurality of renderer threads to receive pieces from a corresponding reader threads. For example as shown in renderer threads receive the pieces from corresponding reader threads and download the pieces to media processors having associated media outputs and . In the example of renderer threads are video renderer threads that download the source image pieces to graphics processors. An additional renderer thread is an audio renderer thread that receives audio data associated with source image data and downloads the audio data to an audio processor.

In step synchronized pieces are output via media outputs associated with the media processors. For example as shown in each video output displays a piece of the source image data forming a composite output image stream. In the example of the display of the source image pieces via video outputs is synchronized with playback of the audio data via an audio output .

In step one or more pieces of a source image are received by generating reader threads to receive the pieces and load the pieces into a shared memory. For example in reader threads receive stripes of source image and load the stripes into shared memory shown in .

In step the pieces are processed on appropriate CPUs by generating processing controller threads to receive the pieces from corresponding reader threads generating processing threads to extract a plurality of sub pieces for each piece and moving the sub pieces to appropriate rendering nodes. For example as shown in processing controller threads receive stripes from reading stage . Each processing controller thread generates four processing threads to extract four tiles from each stripe and move the tiles to rendering stage .

In step a quantity of sub pieces are downloaded to an equal quantity of graphics processors by using renderer threads to receive the sub pieces from corresponding processing threads. According to process a total quantity of sub pieces exceeds the quantity of graphics processors. For example as shown in renderer threads receive tiles from processing stage and download the tiles to graphics processors .

If in step the total quantity of sub pieces has been processed then process proceeds to step . In step the processed sub pieces are transferred back into the shared memory and recombined for storage as a single image. If in step the total quantity of sub pieces has not been processed then process returns to step . Because the sub pieces are not being outputted to media outputs there is no need to process all of the sub pieces at the same time to maintain synchronization. Accordingly in process the quantity of sub pieces can exceed the quantity of graphics processors. Sub pieces are downloaded to the graphics processors for processing in a round robin fashion until all of the sub pieces have been downloaded and processed.

While various embodiments of the present invention have been described above it should be understood that they have been presented by way of example only and not limitation. It will be apparent to persons skilled in the relevant art that various changes in form and detail can be made therein without departing from the spirit and scope of the invention.

