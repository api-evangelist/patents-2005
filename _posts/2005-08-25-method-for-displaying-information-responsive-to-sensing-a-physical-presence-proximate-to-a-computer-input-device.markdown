---

title: Method for displaying information responsive to sensing a physical presence proximate to a computer input device
abstract: In a computer system, a touch sensitive input device having touch sensitive auxiliary controls system can be used to anticipate a user's action. When a user's hand approaches a touch sensitive input device, feedback can be displayed on a display screen. A user can receive feedback without activating the input device. The feedback may take the form of status information related to the feature controlled by the input device and can vary depending upon the application open. Likewise, when the hand of a user is moved away from the touch sensitive input device, the feedback brought on by sensing the user's hand may disappear.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07602382&OS=07602382&RS=07602382
owner: Microsoft Corporation
number: 07602382
owner_city: Redmond
owner_country: US
publication_date: 20050825
---
This application is a divisional application of prior U.S. patent application Ser. No. 09 804 496 filed Mar. 13 2001 which application is a continuation in part of commonly assigned U.S. patent application entitled Proximity Sensor in a Computer Input Device to Hinckley et al. filed Sep. 14 1998 and assigned Ser. No. 09 152 434 now U.S. Pat. No. 6 456 275 issued Sep. 24 2002 which is herein incorporated by reference. Also prior U.S. patent application Ser. No. 09 804 496 filed Mar. 13 2001 is a continuation in part of commonly assigned U.S. patent application entitled A Technique For Implementing a Two Handed Desktop Use Interface For a Computer to Hinckley filed Sep. 14 1998 and assigned Ser. No. 09 152 432 now abandoned which is herein incorporated by reference. Further prior U.S. patent application Ser. No. 09 804 496 filed Mar. 13 2001 is a continuation in part of commonly assigned U.S. patent application entitled Method of Interacting With a Computer Using a Proximity Sensor in a Computer Input Device to Hinckley et al. filed Sep. 14 1998 and assigned Ser. No. 09 152 443 now U.S. Pat. No. 6 396 477 issued May 28 2002 which is herein incorporated by reference.

In addition prior U.S. patent application Ser. No. 09 804 496 filed Mar. 13 2001 is a continuation in part of commonly assigned copending U.S. patent application entitled A Technique For Implementing an On Demand Display Widget Through Controlled Fading Initiated By User Contact With a Touch Sensitive Input Device to Hinckley filed Nov. 25 1998 and assigned Ser. No. 09 200 325 now U.S. Pat. No. 6 333 753 issued Dec. 25 2001 which is herein incorporated by reference and which claims priority to U.S. provisional patent application entitled Toolglass Improvements On Demand Tool Sheet Gesturing Through Tool Sheets filed Sep. 14 1998 and assigned Ser. No. 60 100 261. The prior U.S. patent application Ser. No. 09 804 496 filed Mar. 13 2001 is also a continuation in part of commonly assigned copending U.S. patent application entitled A Technique For Implementing an On Demand Tool Glass For Use in a Desktop User Interface to Hinckley filed Nov. 25 1998 and assigned Ser. No. 09 200 321 now U.S. Pat. No. 6 232 957 issued May 15 2001 which is herein incorporated by reference and which claims priority to U.S. provisional patent application entitled Toolglass Improvements On Demand Tool Sheet Gesturing Through Tool Sheets filed Sep. 14 1998 and assigned Ser. No. 60 100 261.

The prior U.S. patent application Ser. No. 09 804 496 filed Mar. 13 2001 is related to commonly assigned copending U.S. patent application entitled Method of Interacting With a Computer Using a Proximity Sensor in a Computer Input Device to Hinckley et al. filed Apr. 5 2000 and assigned Ser. No. 09 543 723 now U.S. Pat. No. 6 559 830 issued May 6 2003 which is herein incorporated by reference and which is a continuation of U.S. patent application Ser. No. 09 152 443 now U.S. Pat. No. 6 396 477 issued May 28 2002 identified above.

The present invention relates to input devices for computer systems. More particularly the present invention relates to the use of touch sensitive input devices in computer systems.

Input devices for computerized systems such as keyboards touchpads trackballs game controllers and mice often include one or more auxiliary controls such as joysticks touchpads trackballs headsets microphones buttons knobs rocker switches triggers sliders wheels biometric sensors etc. Auxiliary controls may be used alone or in conjunction with input devices and other auxiliary controls.

Most input devices create input signals using transducers or switches. Switches are typically found in the buttons of mice joysticks and game controllers as well as in the keys of keyboards. Transducers are found in mice and trackballs and create electrical signals based on the movement of balls in those devices or by optically detecting movement of the device across a surface. Transducers are also found in headsets where they convert speech signals into electrical signals. Touchpads create input signals using sensors that provide an electrical signal when the user contacts the touchpad that signal including the location within the touchpad where contact is made.

Although it is desirable to increase the amount of information that an input device and its auxiliary controls can provide to the computer the number of transducers and switches that can be added to an input device is limited by the user s ability to remember all of the functions that a particular transducer or switch performs and by the practicalities of the available or at least feasibly available real estate for the switches and transducers on the input device e.g. keyboard or mouse . The ability of a user to determine the functionality of each auxiliary control is also generally limited to a static label on or near the auxiliary control activating each auxiliary control clicking a button to select a menu option to request help file or reading a user manual. Furthermore the functionality of an auxiliary control may vary from one game or application to another game or application. Thus a static label on an auxiliary control is of little value when the function and status of the input device and its auxiliary controls change from application to application. In this regard even if a user learns the assignment of the buttons and other controls in one game or application another game or application may assign the same buttons or controls different features. Similarly the status of the feature controlled by the auxiliary control generally can only be determined by activating the control.

Thus there is a need to provide a user with the ability to better determine the functionality and status of auxiliary controls regardless of their context e.g. active application or game .

To address this need indicators such as LEDs or small LCDs may be integrated directly with the input device and its auxiliary controls to provide feedback or state information. However association of LEDs or LCDs with each auxiliary control increases both the amount of power consumed and the cost of the input device. Furthermore LEDs and LCDs have very limited output forms and are separated from the main display e.g. a computer monitor causing the user to constantly have to look away from the screen and at the auxiliary control to determine the functionality or status of the control. Such constant shifts of attention can mentally tire the user as he or she is forced to repetitively reacquire the current context. Accordingly there is a need to provide a more efficient convenient and or cost effective way to determine the functionality and or status of auxiliary controls of input devices in a computer system.

The present invention overcomes many of the shortcomings of existing input devices by providing touch sensitive input devices which have touch sensitive auxiliary controls that sense a physical presence and provide visual feedback on an on screen display or on the control itself acoustic feedback e.g. voice or music or tactile feedback e.g. vibration .

In an aspect of the present invention keys buttons knobs rocker switches or other auxiliary controls of an input device such as a keyboard touchpad trackball game controller monitor joystick steering wheel headset or mouse can be augmented with sensors that detect contact or extreme proximity of a user s hand. According to another aspect a sensed signal of the auxiliary control can provide the user with an on screen display of status state information tool tips help text or other feedback relevant to the control the user has touched without the user having activated e.g. depress turn roll or otherwise activate the control. Alternatively acoustic or tactile feedback may be provided to the user instead of or in addition to the feedback provided by the on screen display. Hence a user can simply touch different buttons or other controls to explore their function assignment or status or obtain other feedback. Such feedback can allow the user to better understand the consequences of their action should he or she subsequently chooses to activate the control. Also a user may quickly and casually be able to view status information.

Aspects of the invention include providing context sensitive feedback for an auxiliary control in an on screen display such as a display widget e.g. graphical user interface GUI responsive to detection of a user s hand. Hence touch sensitive controls can be used to predict the context of a user s action. The context of the situation can be established by preparatory actions by the user such as grabbing touching or approaching a control. Knowing the context of the action the computer can begin to execute and predict the will of the user.

According to another aspect of the invention contexts of an auxiliary control may include but are not limited to different types of applications such as games utility and productivity applications. Also contexts may change within various portions of an application or game.

In another aspect of the invention when a GUI is displayed responsive to detection of a physical presence proximate to or contacting an auxiliary control a user may interact with the GUI using another auxiliary control or an input device including but not limited to a mouse touchpad or keypad. For example if a volume control GUI is displayed a user may adjust the volume with a mouse.

In another aspect of the invention input devices including their auxiliary controls may have a touch sensor which directly detects when a physical presence e.g. user touches the device or its controls so as to provide display and dismissal of feedback on an on demand basis e.g. whenever the user establishes or breaks physical hand contact with the device.

For example a transition in a touch indication provided by the device reflective of the user then making contact with the device or control such as by touching the device with a finger may cause a tool tip to be displayed. A transition indicative of a user breaking physical contact with the device such as by lifting his finger off the device can cause the tool tip to be dismissed from the display. In one aspect of the invention to prevent user distraction these detected transitions initiate corresponding predefined animation sequences that occur over preset time intervals in which the feedback either begins to fade into view typically from an invisible i.e. totally transparent state to eventually a predefined semi transparent state or non transparent state as soon as user contact begins and then begins to fade out from view i.e. eventually back to its invisible state as soon as user contact with the device is broken e.g. as soon as the user lifts his or her hand away from the device.

In another aspect of the invention toolbars scrollbars and the like may only be displayed on a display screen in response to detection of a physical presence. Illustratively touching and not activating an auxiliary control may cause on screen display of a toolbar and breaking contact with the control may cause dismissing of the toolbar. In one aspect of the invention a user may interface with the toolbar using an auxiliary control or input device as desired with one hand while continuing to touch the auxiliary control which caused the on screen display of the toolbar. According to this aspect display clutter can be reduced and available application screen area increased at appropriate times during program execution consistent with and governed by user action but without imposing any significant cognitive burden on the user to do so. This in turn is likely to significantly improve the user experience .

These and other novel advantages details embodiments features and objects of the present invention will be apparent to those skilled in the art from following the detailed description of the invention the attached claims and accompanying drawings listed herein which are useful in explaining the invention.

In accordance with illustrative embodiments of the present invention an auxiliary control of an input device may be configured to detect when a physical presence e.g. user s hand stylus touches the control so as to provide display and dismissal of a display widget on an on demand basis. The display widget may be a graphical user interface GUI and include information such as functionality e.g. tool tips and or status information on an on demand basis. An exemplary list of display widgets includes toolbars tool glass sheet scroll bars window frames and window decorations title bars floating tool palettes modal and non modal dialog boxes list or combo boxes setting controls buttons text entry areas etc.

A transition in a touch indication provided by the input device reflective of the user then making physical contact with an auxiliary control such as by touching the control with a finger may cause a display widget to be displayed. A transition indicative of a user breaking contact with the auxiliary control such as by lifting his finger off the control causes the display widget to be dismissed. To prevent user distraction these detected transitions can initiate corresponding predefined animation sequences that occur over preset time intervals in which the display widget either begins to fade into view typically from an invisible i.e. totally transparent state to eventually a predefined semi transparent state as soon as user contact begins and then begins to fade out from view i.e. eventually back to its invisible state as soon as user contact with the auxiliary control is broken for example as soon as the user lifts his or her finger from the control. Furthermore depending on a specific touch sensitive auxiliary control used it is sometimes preferable to begin the fading after a brief time delay or cooling period occurs. For example the user may reach the edge of the touchpad and reclutch his or her hand e.g. briefly lift up his or her finger and then re center it on the pad to continue pointer motion. It might be annoying for the display widget to begin fading immediately when this happens. A brief time delay e.g. approximately 0.5 to 1.0 seconds coupled with continued contact sensing of the finger prior to the start of the fading allows brief reclutchings of this sort without any changes to the screen display. Similarly if a user has not touched the auxiliary control for a sufficiently long period of time a similar time delay coupled with sensing for a release of user contact prior to the start of a fade in might be used to prevent a short inadvertent contact from causing the display widget from fading in and then out and otherwise annoying the user.

Furthermore according to the present invention touch induced display and dismissal can also readily be used to display and dismiss a display widget e.g. toolbar on an on demand basis by sensing contact between a user s preferred hand and another touch sensitive auxiliary control or input device such as e.g. a touch sensitive mouse. Displaying a display widget in response to detection of touch specifically when a user grabs an auxiliary control or input device can exploit a user s desire to utilize the display widget implicit in the user s action in reaching for and grabbing the auxiliary control or input device. The display widget would be dismissed when the user releases the auxiliary control or input device for example when touch contact is broken. Displaying and dismissing a display widget in this fashion advantageously places little if any additional cognitive burden on the user.

With reference to an exemplary system for implementing the invention includes an illustrative computer system with a conventional personal computer including a processing unit CPU a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. The system memory includes read only memory ROM and random access memory RAM . A basic input output BIOS containing the basic routine that helps to transfer information between elements within the personal computer such as during start up is stored in ROM . The personal computer further includes a hard disk drive for reading from and writing to a hard disk not shown a magnetic disk drive for reading from or writing to removable magnetic disk and an optical disk drive for reading from or writing to a removable optical disk such as a CD ROM or other optical media. The hard disk drive magnetic disk drive and optical disk drive are connected to the system bus by a hard disk drive interface magnetic disk drive interface and an optical drive interface respectively. The drives and the associated computer readable media provide nonvolatile storage of computer readable instructions data structures program modules and other data for the personal computer .

Although the exemplary environment described herein employs the hard disk the removable magnetic disk and the removable optical disk it should be appreciated by those skilled in the art that other types of computer readable media which can store data that is accessible by a computer such as magnetic cassettes flash memory cards digital video disks Bernoulli cartridges random access memories RAMs read only memory ROM and the like may also be used in the exemplary operating environment.

A number of program modules may be stored on the hard disk magnetic disk optical disk ROM or RAM including an operating system one or more application programs other program modules program data and device drivers . The device drivers processes commands and information entered by a user through input devices which can include a keyboard mouse game controller trackball touchpad. The input devices also can have auxiliary controls such as a joystick game pad touchpad trackball key headset monitor microphone button knob rocker switch trigger slider wheel lever touch strip biometric sensor etc. The input devices may be wired or wirelessly coupled to the personal computer .

According to an exemplary embodiment of the present invention at least one of the input devices includes a touch sensor and an input device such as a mouse may have both a touch sensor and a movement transducer . Touch sensor is capable of generating a signal that indicates when a physical presence such as a user s hand is touching one of the input devices itself or an auxiliary control thereof. Movement transducer is capable of generating a signal that indicates when a user causes part of the input device to move. The signals generated by touch sensor and movement transducer can be passed along a conductor connected to the processing unit through a serial port interface that is coupled to the system bus but may be connected by other interfaces such as a sound card a parallel port a game port or a universal serial bus USB .

A monitor or other type of display device may also be connected to the system bus via an interface such as a video adapter . In addition to the monitor personal computers may typically include other peripheral output devices such as a speaker and printers not shown .

The personal computer may operate in a networked environment using logic connections to one or more remote computers such as a remote computer . The remote computer may be another personal computer a hand held device a server a router a network PC a peer device or other network node and typically includes many or all of the elements described above relative to the personal computer although only a memory storage device has been illustrated in . The logic connections depicted in include a local area network LAN and a wide area network WAN . Such networking environments are commonplace in offices enterprise wide computer network intranets and the Internet.

When used in a LAN networking environment the personal computer is connected to the local area network through a network interface or adapter . When used in a WAN networking environment the personal computer typically includes a modem or other means for establishing communications over the wide area network such as the Internet. The modem which may be internal or external is connected to the system bus via the serial port interface . In a network environment program modules depicted relative to the personal computer or portions thereof may be stored in the remote memory storage devices. It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used. For example a wireless communication link may be established between one or more portions of the network.

In some contact sensor embodiments a touch sensor includes a conductive film that has a capacitance that changes when it is touched. This sensor also includes a capacitive measuring circuit that generates an electrical signal based on the change in capacitance of the conductive film. Those skilled in the art will recognize that other contact sensor technologies are available such as photodiodes piezoelectric materials and capacitive pressure sensors. Any of these sensors may be used within the context of the present invention. In one proximity sensor embodiment the touch sensor uses reflected light from an LED to detect when the user is proximate the sensor. A chip used to drive the LED and sense the reflected light according to this illustrative embodiment may be produced by Hamamatsu Corporation of Bridgewater N.J. Other proximity sensor embodiments use changes in electric or magnetic fields near the input device to determine when the user is proximate to the device.

In embodiments of the present invention the touch sensors may provide the same information regardless of where on the touch sensor the user touches the input device or the portion of the sensor to which the user is proximate. Thus these touch sensors decouple touch data from position data. provides an illustrative input device with this type of touch sensor.

In other embodiments for example with touchpads touch screens and touch tablets a given touch sensor may provide location information that would indicate where the user made contact within that touch sensor or where the user came closest to the touch sensor within the touch sensor. In these devices one cannot specify positional data without touching the device nor can one touch the device without specifying a position. Hence touch sensing and position sensing are tightly coupled in these devices.

Referring to analog to digital converter and multiplexer converts the analog electrical signals found on conductors and into digital values carried on a line . Line is connected to microcontroller which controls multiplexer to selectively monitor the state of the four touch sensors. Microcontroller also receives inputs from various other sensors on the input device. For simplicity these inputs are shown collectively as input . Those skilled in the art will recognize that different input devices and auxiliary controls provide different input signals depending on the types of motion sensors in the input device. Examples of motion sensors include switches which provide signals indicative of the motion needed to close a switch microphones which provide signals indicative of air movement created by an audio signal encoder wheels which provide signals indicative of the motion of a mouse ball trackball or mouse wheel and resistance wipers which provide electrical signals indicative of the movements of a joystick. Each of these motion sensors acts as an input generator that is capable of generating input information to be sent to the computer system. Based on the particular input generator this input information can include a depressible key s state a depressible button s state sound information or movement information.

Those skilled in the art will also recognize that the number of input lines tied to microcontroller depends on the number of sensors on the input device and the configuration of the input device. For example for a keyboard the microcontroller uses input lines to determine if any of the auxiliary controls have been activated. The micro controller accomplishes this using a multiplexer not shown to sequentially test the state of each auxiliary control on the keyboard. The techniques used to detect these auxiliary control states closures are well known in the keyboard art.

In a mouse or trackball input lines include lines for detecting the closure of switches and lines for detecting the rotation of encoder wheels. The switches are located beneath buttons on the mouse or trackball. The encoder wheels track the movement of the mouse ball or trackball. Typically one encoder wheel tracks movement in the X direction and another encoder wheel tracks movement in the Y direction. In most embodiments each encoder wheel has its own associated input line into microcontroller . In some mice an additional encoder wheel tracks the rotation of a wheel located on top of the mouse.

In some mice the X and Y movement of the mouse is tracked by a separate optics microcontroller that is connected to microcontroller through lines . The optics microcontroller uses optical data to determine movement of the mouse. The optical microcontroller converts this optical data into movement values that are transmitted to microcontroller along input lines .

In a game controller such as a game pad input lines include lines for detecting the closure of multiple switches on the game pad as well as lines for detecting the rotation of wheels on the game pad. In joysticks input lines can include lines connected to resistance wipers on the joystick as well as switches on the joystick. In headsets lines include multiple lines that carry multi bit digital values indicative of the magnitude of the analog electrical signal generated by the microphone. An analog to digital converter typically produces these digital values. To reduce the weight of the headset the analog to digital converter and microcontroller can be found on a soundboard located within the computer. To further reduce the weight of the headset multiplexer and A to D converter of can also be implemented on the soundboard.

Microcontroller produces an output which is provided to serial port interface of . Typically output is a serial digital value that indicates which motion sensor or touch sensor has been activated. For keyboards the digital values include scan codes that uniquely identify the key other auxiliary control or touch sensor on the keyboard that has been activated. For mice the digital values include a mouse packet that describes the current state of each switch and each touch sensor on the mouse as well as the distances that the mouse wheel and mouse ball have moved since the last mouse packet was sent.

Output line carries signals from microphone and from touch sensor . In some embodiments headset is connected to a computer system that includes a speech recognition system. In these embodiments the speech recognition system is inactive unless touch sensor indicates that a user is touching headset . The activation of the speech recognition system can include loading the speech recognition system into random access memory when the user first touches headset . It can also include prompting a speech recognition system that resides in random access memory so that it can process input speech signals. In either case by only activating the speech recognition system when headset indicates that the user is touching the headset the present invention reduces the likelihood that extraneous speech will be processed by the speech recognition system.

Thus a mouse according to exemplary embodiments of the present invention is able to detect when certain areas of the mouse are being touched and when portions of the mouse or the entire mouse are being moved. Specifically the conductive films on the palm rest left button and side areas and indicate when the user is touching these areas. Note that even if the user does not move the mouse or press a button the sensors associated with the conductive films of will generate an electrical signal when the user touches the mouse. Encoder wheels and generate a separate electrical signal when the user moves the mouse and switches and generate separate electrical signals when the user depresses buttons and and wheel respectively. Thus an exemplary mouse according to the present invention adds functionality without increasing dexterity needed to manipulate the controls thereof.

In alternative embodiments of the present invention trackball and encoder wheels and are replaced by a solid state position tracking device that collects images of the surface that the mouse travels over to determine changes in the position of the mouse. According to these exemplary embodiments the mouse typically includes a light source used to illuminate the surface an optics system used to collect images of the surface and a processor used to compare the various images to determine if the mouse has moved and if so in what direction. Since the solid state position tracking device converts movement into an electrical signal it can be considered to be a sophisticated transducer or motion sensor.

Note that all of the exemplary embodiments of and can be practiced with a sensor located on the palm rest and or a sensor located on the left button and or a sensor located on the right button.

Although various exemplary embodiments have been described with particularity with respect to touch sensor location in A D A B A B A C A H A B A B A D and A D it should be noted that sensors may be included in other locations. For example it is possible to combine some or all of the touch sensors illustrated in one embodiment with some or all of the touch sensors illustrated in one embodiment with some or all of the touch sensors illustrated in another embodiment. Further it should be understood that many of the touch sensor locations including but not restricted to those on the sides of the mice in A D A B A B A C A H A B A B A D and A D may be conductive films on top of auxiliary controls where the controls can be activated to perform a function when pressed. In this instance one set of electrical signals is generated when the control is touched and a second set of electrical signals is generated when the control is activated. In certain illustrative embodiments of the invention various mice including their auxiliary controls e.g. buttons wheels can detect a physical presence e.g. a finger contacting a touch sensitive surface thereof the physical presence representing an explicit user request to display a display widget on a display screen and or to generate other feedback e.g. acoustic tactile . The feedback can provide information to the user such as status information control functionality and help text. The information may vary from application to application. In some embodiments of the invention the position of the display widget on the display screen may track the movement of the physical presence across the touch sensitive surface.

In illustrative embodiments of the invention game controllers such as those of including their auxiliary controls e.g. buttons joystick trigger can detect a physical presence e.g. a finger contacting a touch sensitive surface thereof the physical presence representing an explicit user request to display a display widget on a display screen and or to generate other feedback e.g. acoustic tactile . The feedback can provide information to the user such as status information control functionality and help text. The information may vary from application to application. In some embodiments of the invention the position of the display widget on the display screen may track the movement of the physical presence across the touch sensitive surface.

Additional touch sensors can be located on keyboard casing at portions and below space bar at portion below arrow keys and at a portion below keypad . Arrow keys are typically used by the user to move a cursor across the display. Note that although keyboard is shown with touch sensors on the keys and touch sensors on portions and other exemplary embodiments of the invention may only have touch sensors on the keys or only on one of the portions and . In other exemplary embodiments different combinations of these touch sensors can be found on the keyboard . In addition some or all of the touch sensors on portions and may be proximity sensors. Touch sensors and may represent many types of auxiliary controls including but not limited to a joystick game pad touchpad trackball button knob rocker switch trigger slider wheel lever etc. The proximity sensors can detect the user s hand when it is near the sensor without requiring the hand to actually contact the sensor. Generally one signal is provided when the user touches but does not activate e.g. depress turn roll the auxiliary control and a second signal is provided when the user activates the auxiliary control. In exemplary embodiments of the invention a keyboard including its auxiliary controls can detect a physical presence e.g. a finger contacting a touch sensitive surface thereof the physical presence representing an explicit user request to display a display widget on a display screen and or to generate other feedback e.g. acoustic tactile . The feedback can provide information to the user such as status information control functionality and help text. The information may vary from application to application. In some embodiments of the invention the position of the display widget on the display screen may track the movement of the physical presence across the touch sensitive surface.

As has been described keys buttons knobs rocker switches or other auxiliary controls of an input device can be augmented with sensors that detect contact or extreme proximity of a physical presence including but not limited to a user s hand e.g. finger palm or a stylus. Illustrative input devices have been shown and described previously including mice trackballs game controllers keyboards and touchpads . According to embodiments of the present invention such sensed signals can be used to provide the user with on screen displays of status state information or other feedback relevant to the control that the user has touched.

Serial interface converts the serial binary signal from input device into parallel multi bit values that are passed to device driver . In many embodiments of the present invention device driver can be implemented as a software routine that is executed by CPU of . In these embodiments device driver can be input device specific and designed to interact with a particular input device and its auxiliary controls based on a designated protocol. Thus if input device is a mouse device driver is a mouse driver that is designed to receive mouse packets generated by the mouse using a mouse packet protocol. If input device is a keyboard device driver is a keyboard driver designed to receive keyboard scan codes indicative of a key being depressed or a touch sensor being touched.

Based on the designated protocol device driver converts the multi bit values into device messages that are passed to operating system . These device messages indicate what events have taken place on the input device. For example if a touch sensor on a mouse has been touched the message indicates that the particular sensor is being touched. When the touch sensor is released a separate message is generated by device driver to indicate that the touch sensor has been released.

The messages generated by device driver are provided to operating system which controls the routing of these messages. According to illustrative embodiments the device messages are usually sent to a focus application . The focus application is typically the application that has the top most window on the display.

In some illustrative embodiments operating system maintains a list of message hook procedures that have been registered with the operating system. In these illustrative embodiments operating system sequentially passes the device message to each message hook procedure on the list before sending the message to focus application . Such message hook procedures are shown generally as message hook procedures of . Most message hook procedures simply evaluate the device message to determine if some action should be taken. After evaluating the device message the message hook procedure returns a value to operating system indicating that the operating system should pass the device message to the next procedure in the list. Some message hook procedures have the ability to eat a device message by returning a value to operating system that indicates that the operating system should not pass the device message to any other message hook procedures or to the focus application.

The message hook procedures and the focus application use the device messages especially those indicating that a touch sensor has been touched to initiate a variety of functions that are discussed below.

For example depict images of screens displayed by various applications of the present invention that utilize device messages generated based on signals from an input device according to illustrative embodiments of the present invention such as mouse or trackball of respectively. depicts an image of a screen that shows a virtual desktop . Virtual desktop includes images of icons and as well as an open window . Open window is associated with a word processing application known as Microsoft Word offered by Microsoft Corporation of Redmond Wash.

In window a caret is positioned within a sentence of an open document. Caret may be positioned by moving mouse or ball of trackball . In caret appears as a vertical line that extends between two smaller horizontal lines. Those skilled in the art will recognize that caret can have many different shapes and typically appears as an arrow on desktop .

The position of caret within the sentence of window causes a tool tip to appear. Tool tip indicates who entered the word that caret is positioned over.

Window also includes a tool bar that includes drawing tools that can be used to draw pictures in the document of window .

According to exemplary embodiments of the present invention such as shown in caret tool tip and tool bar only appear in window while the user is touching a portion of the input device such as an auxiliary control e.g. button or button combination assigned to provide a tool tip while in the word processing application. If the user is not touching the input device caret tool tip and tool bar disappear. shows an image of display when the user is not touching a portion of the input device. By eliminating tool bar caret and tool tip when the user is not touching the input device the present invention can reduce the clutter found in window and make it easier for the user to read the document shown in window .

Those skilled in the art will recognize that the disappearance of caret tool tip and tool bar when the user is not touching the input device can be controlled independently. Thus the user may customize window such that tool tip and tool bar disappear when the user releases the input device but caret remains visible. In addition the rate at which items disappear and reappear can be controlled. Thus it is possible to fade images off the display and to fade them back onto the display as the user releases and then touches the input device. In some illustrative embodiments of the invention the fade out period is 0.7 to 1.0 seconds to minimize distraction and the fade in period is 0.0 seconds for the caret which appears instantly and 0.3 seconds for toolbars. In certain embodiments the fade out time may be a variable configurable by a user with a preset default period.

In other exemplary embodiments the user may move left and right across menu bar by using the keys representing the numbers 4 and 6 on numeric keypad . As the user moves across menu bar a different pull down menu can be displayed for each respective menu heading. Specifically by touching the key representing the number 4 the user causes a keyboard message to be sent to the application which changes the display so that the menu heading to the left of the current menu heading in header menu is displayed. Thus if the pull down menu for the menu heading Tools is currently displayed in window touching the key representing the number 4 causes a pull down menu associated with the menu heading Insert to be displayed. Similarly the user can cause a pull down menu to appear for a menu heading to the right of the current menu heading by touching the key representing the number 6 on numeric keypad . Thus if the current pull down menu is associated with the menu heading Tools and the user touches the key representing the number 6 the pull down menu associated with the menu heading Format in header menu will be displayed. This is shown in where pull down menu for the menu heading Format is displayed.

By touching the keys representing the numbers 2 and 8 on numeric keypad the user can also move up and down within a pull down menu such as pull down menu . As the user moves through a pull down menu different items within the pull down menu become highlighted. An example of a highlighted entry is entry of which highlights the entry Tabs of pull down window as the current entry. If the user touches the key representing the number 8 while entry is the current entry the application that receives the associated keyboard message highlights entry located above entry as the current entry. If the user touches the key representing the number 2 while entry is the current entry entry below entry is highlighted as the current entry.

In the prior art pull down menu will continue to be displayed even if the caret is positioned outside of the pull down menu itself. The only way to make the pull down menu disappear is to click on an area outside of the menu itself. However according to an illustrative embodiment of the present invention the application that produces the pull down menu removes the pull down menu as soon as it receives a mouse message that indicates that the user released the pointing device. This improves user efficiency by reducing the movements the user must make to close the pull down windows associated with header menu .

Using keyboard a focus application displays radial menu when it receives a keyboard message indicating that a user touched one of the keys in keypad . To highlight a specific entry the user touches a key in keypad that is spatially related to the entry. For example to highlight entry of radial menu the user touches the key representing the number 8 which is located directly above a center key representing the number 5 because the spatial positioning of the 8 key relative to the 5 key is the same as the spatial relationship between entry and cancel button . To select an entry the user depresses the key that causes the entry to be highlighted. To dismiss the radial menu the user depress the 5 key.

To manipulate the radial menu using the touch sensors of button on the mouse of the user simply touches the touch sensor that corresponds to an entry on the radial menu. Simply touching the corresponding touch sensor causes the entry to be highlighted. Depressing button while touching the corresponding touch sensor causes the entry to be selected. The application determines that both events have occurred based on two separate mouse messages. A first mouse message indicates which touch sensor is currently being touched. A second mouse message indicates that the left button has been depressed.

According to illustrative embodiments of the invention the use of touch sensitive controls and devices may be transparent to the user. The context of the situation may be indicated by the user s preparatory actions with the control such as grabbing touching or approaching the control device. Referring to for exemplary purposes assume that the current functionality of control knob is to control the volume of audio associated with media as opposed to controlling the volume for other system generated sounds . In this instance if the user wishes to change the volume the user may approach or touch the control knob with his hand. Before the user activates the control knob by turning the knob a GUI for the volume control including the status of the volume in the form of a screen volume indicator can appear on screen as shown in . The user may then turn the control knob to adjust the volume or move a pointer e.g. caret or arrow into the GUI while contacting but not turning the control knob and change the volume using another control such as a key e.g. arrow key mouse or other pointing device. If the user touches the control knob and then employs a key or pointing device to change the volume while the GUI is visible the change in volume state may be shown instantly in the GUI or elsewhere on the display screen. When the user releases the control knob the system knows the GUI is no longer needed and it can cause the GUI to be dismissed without perceptible delay. In some embodiments the GUI will remain visible as long as the pointing device continues interacting with or the cursor remains over the GUI.

In another illustrative embodiment of the invention when the user approaches or touches the control knob without activating the control a display widget such as a tool tip can instantly be displayed on the display screen and identify the current functionality of the control knob . For example a tool tip may indicate but is not limited to one of the following functions 1 tuning for a variety of different applications including audio and video applications 2 volume control for media applications 3 volume control for system generated sounds and 4 control for numerous features which can have multiple settings e.g. brightness cropping color etc. . In other illustrative embodiments as a user approaches a control such as control knob visual feedback may be provided on the actual control knob by an LED or LCD alone or in addition to the display widget on the display screen. In still further illustrative embodiments acoustic or tactile e.g. vibration feedback may be provided alone or in addition to visual feedback on the display screen input device and or control or to each other. Consequently the input device or control may be able to directly provide feedback e.g. acoustic feedback with or without involving or sharing the information with the host computer. In still further exemplary embodiments one or more portions e.g. top and side of the control such as control knob may be able to independently detect contact or user proximity and generate unique messages for the host computer based on which portion of the control is being touched.

In another illustrative embodiment of the present invention based on the keyboard input device of a touch sensitive rocker switch can be provided. In one context the rocker switch may allow a user to switch between applications similarly to using the combination of the Alt and Tab keys currently used on operating systems such as Windows98 by Microsoft Corporation of Redmond Wash. That is the rocker switch can allow a user to move forward and backward between running applications. Illustratively when the user touches or approaches the rocker switch an on screen display showing an icon for each currently running application may be shown with the application in the foreground at the top of the window stacking order being highlighted as shown in . Pressing the rocker switch can allow a user to move forward or backward between the applications in order to highlight the desired application to bring to the foreground of the display. The order may be determined in a number of ways such as alphabetically or the last time each application was in the foreground of the display. Releasing the rocker switch can cause the highlighted application to be selected and brought to the foreground of the display. It should be understood that the touch sensitive rocker switch may used in numerous other applications including displaying different items responsive to user contact or extreme proximity to the switch and highlighting an item responsive to pressing of the switch and selecting an item from the list by releasing contact or moving away from the switch.

An illustrative implementation of various auxiliary controls for an exemplary keyboard input device and their context sensitive response to touch are listed below in Table 1. It should be understood that the various controls and responses may be applied to other input devices such as game controllers trackballs mice touchpads etc.

Below Table 2 describes illustrative techniques for context sensitive response to touch on an exemplary mouse input device according to the present invention.

In addition to many of the types of visual feedback that may be provided on a screen in response to user contact with a touch sensitive control another possibility is to display a standard tool tip such as those illustrated in when key combinations or other controls are set to perform the copy e.g. Ctrl key c key together and paste e.g. Ctrl key v key together functions respectively. That is when the user touches the Ctrl key and c key together the tool tip Copy displayed in may appear on the screen and when the user touches the Ctrl key v key together the tool tip Paste shown in may appear on the screen.

Typically the on screen display can be placed near the current cursor or pointer e.g. caret position regardless of the input device that the user touches. For example a keyboard tool tip could appear next to the cursor. Popping up the on screen display to the right of and above the current cursor position can be beneficial since the on screen display does not conflict with traditional tool tips which appear when the user dwells with the pointer over an icon tool tip appears to the right of and below the pointer or cursor in the Windows operating system by Microsoft Corporation of Redmond Wash. The tool tip can follow the cursor or pointer as the user moves the cursor or pointer with a pointing device e.g. mouse or it can remain stationary at the point where it initially appears. Maintaining the tool tip where it appears rather than moving the tool tip with the cursor is easier to implement and more efficient and would likely be well accepted by users. According to other exemplary embodiments of the present invention the display widget may be displayed at the center of the screen at the center of the currently active focus application or widget e.g. a text entry box at the bottom of the screen or above the system tray icons.

In another exemplary embodiment of the present invention each hot key may be assigned a text macro where activating the hot key causes a block of text to be inserted for example where the cursor is located on the screen. When used in this context touching a hot key displays at least the beginning or another portion if not all of the text macro assigned to the hot key as shown in . The on screen display window may automatically resize according to the amount of text assigned to the text macro in order to display all the text. This context may also be used in conjunction with scrolling controls such that a user while touching the hot key assigned the text macro may scroll through the text.

As mentioned previously according to some exemplary embodiments of the present invention acoustic or tactile feedback may be employed. Acoustic or tactile feedback can be used in combination with or separately from visual feedback on the display. In some cases visual feedback may not be appropriate or acoustic feedback may be desired because of the currently running application the input device that is currently being used or user preference.

According to certain embodiments of the invention acoustic feedback for example appropriate cue tones or other sounds could be generated as the user touches a control. A desired audio cue tone may be mapped to a specific function or control. Volume pitch and timbre could be adjusted to produce appropriate cues that mimic desired sounds such as voice generated announcement of a control function. Cue sounds may be generated by taking parametric sound event requests and sequencing them using MIDI wavetable synthesizer through audio generator illustratively the Creative Labs AWE64 Gold card sound board see . The specific techniques for establishing correct audio parameters for each audio cue properly synthesizing the audio cues and associating cues with corresponding controls and or functions have been omitted for simplicity as they are all readily apparent to those skilled in the art and do not form part of the present invention.

Acoustic feedback to identify functionality and other information may be particularly useful for gaming applications and products such as the MS Game Voice by Microsoft Corporation of Redmond Wash. For example gaming products may include a headset microphone combination attached to a puck. A puck allows a user to selectively talk to multiple combinations of people and teams with whom he or she is playing. Each person may be automatically assigned a code 1 2 3 . . . and a team A B C . . . . A problem may arise when the player has to remember which code corresponds to which player and team. When the user wants a reminder to whom a button is assigned the user may touch a control e.g. number key and receive acoustic feedback of the name of the person assigned that code via their headset.

In one exemplary embodiment of the present invention identified in Table 1 multiple touch areas on an input device can be used to page backwards and forwards through web pages provided by an Internet browser. Examples of input devices having multiple touch sensitive areas useful in paging backward and forward include among others the mice of D A B A B and C. In touching region and then region initiates a page backward function and touching region and then region initiates a page forward function. In touching region and then region initiates a page backward function and touching region and then region initiates a page forward function. In touching regions and respectively and then regions and respectively initiates page forward functions and touching regions and respectively and then regions and respectively initiates page backward functions. In B and C touching regions and respectively and then touching regions and respectively initiates page forward functions and touching regions and respectively and then touching regions and respectively initiates page backward functions.

Note that a mouse according to an illustrative embodiment of the present invention can be configured so that paging functions are initiated simply by touching one touch sensor instead of touching a sequence of two touch sensors. Thus in touching region can initiate a page forward function and touching region can initiate a page backward function. Similarly touching region of can initiate a page forward function and touching region of can initiate a page backward function. In this context the touch sensors of the present invention provide the functionality of the side switches found in U.S. patent application Ser. No. 09 153 148 filed on Sep. 14 1998 entitled INPUT DEVICE WITH FORWARD BACKWARD CONTROL the inventors of which were under a duty to assign the application to the assignee of the present application.

The paging functions performed using these touch areas are shown in . In display shows an Internet browser window that depicts a current page . A user can page backward to the Internet page that was displayed before current page to display a past page of which is shown in Internet browser window . The user can move forward to a next page shown in browser window of display in using the touch sensor combination described above. In order to be able move forward to next page the user must at some point move backward from next page to current page .

It should be understood that the various responses to making contact with or being in extreme proximity to a touch sensitive control may be used in combination. For example when a user first makes contact with a button the button s function may be displayed. In one implementation if the user maintains contact with the button for more than a prescribed amount of time e.g. five seconds more detailed status information may be displayed or a GUI may be available to the user. Alternatively successive touches of the same button within a predetermined period of time may cause different types of information to be displayed such as the button functionality followed by a GUI. Tactile or acoustic feedback may also be provided following an initial touch with a display of the same or more detailed information following a second touch in a predetermined time period or after touching has been detected for a prescribed time period.

In responsive to detection of a user in contact with or proximate to an input device or auxiliary control of the device the on screen display of a display widget may occur instantaneously. Frequently it may be desirable to delay the appearance of the display widget on the screen slightly so that if a user activates rather than merely touches the control or device to activate a function for example the display widget will not rapidly appear and disappear from the screen. Delaying the appearance of the display widget a predefined amount of time can prevent unnecessarily displaying of a display widget with tool tip information for example to a user who is familiar with the active application and the operation of an input device and its auxiliary controls and who rapidly activates the device or its control e.g. depresses a button to activate a function. Several illustrative techniques according to the invention are described below that address controlling the display.

In one embodiment of the invention a brief time period on the order of 0.3 seconds may be employed. In this instance if a user makes contact with but does not activate a key or other control within the brief time period then the on screen display appears. If the user activates the control before the expiration of the brief time period the on screen display would not be displayed in response to the user contacting the control.

According to some embodiments of the invention activation of the control e.g. turning a knob causes the state of the function controlled e.g. volume to change and may display a GUI e.g. volume setting as the user changes the state e.g. volume setting . In contrast when touching but not activating the control for the entire brief period a GUI representing the existing state e.g. current volume of the function may be displayed without causing the state to be changed.

Similarly according to embodiments of the present invention a brief time period may also be used to control disappearing or fading out of the on screen display when a user ceases contact with a control. In this instance the on screen display remains visible during the brief time period at the expiration of which the on screen display disappears or fades out. If the user again touches the control before the brief time period expires the on screen display remains visible. It should be understood that the same type of time periods may be applied in embodiments of the present invention in which detecting contact of a control by a user causes a display widget to be dismissed i.e. disappear or fade out and ceasing contact of the control causes the display widget to reappear or fade in.

According to illustrative embodiments of the invention when a display widget including a tool tip or other information appears schemes for such display can be based on a number of criteria including the control timeout and other information such as mouse movement. graphically depict three illustrative alternatives as to the timing and conditions involved with a touch sensitive mouse that causes a display widget such as a tool tip to appear on a display screen.

According to the timing diagram shown in of an illustrative embodiment of the invention the button tool tip is not displayed when the user is touching the button not activating and moving the mouse. If the user continues to contact the button and stops moving the mouse for the time out period t the button tip is displayed. As in the tool tip remains visible until the user releases contact with the button.

In the timing diagram depicted in if when the user touches the button and the mouse is still the user continues to touch the button and not move the mouse for the time out period t the tool tip is displayed. As in the tool tip remains visible until the user releases contact with the button.

Another aspect of display schemes for the touch sensitive input control display involves the response of a tool tip or other display widget to user activation of the button or other input control. After the user activates the control the user may no longer desire the feedback of the resulting action or otherwise . For example at the time that a user clicks a button button down event the button tool tip can instantly disappear or fade out . Also the button tool tip may reappear or stay hidden after a button up event occurs if the user maintains contact with the button. The computer system can also display visual feedback to associate the button click with the information displayed in the tool tip before the tool tip disappears. According to an illustrative embodiment as depicted by the timing diagram shown in after a button down event caused by a button click the button tool tip will not reappear until a user releases the button and again touches but does not activate the button for time period t. Stated differently following activation of a button the button tool tip will not be displayed until after the next rising edge of the button touch sensor as shown in . According to another embodiment of the present invention depicted by the timing diagram in after a button down event followed by a button up event the button tool tip reappears after the time out t as long as the user continues to maintain contact with the button.

Other embodiments of the invention involve handling simultaneous contact with multiple controls on an input device. If a user touches one control while maintaining contact with a second control the on screen display or other feedback for the second control should be generated. Alternatively certain functions can be defined by a combination of controls for example key combinations. In this instance the tool tip of the combination e.g. key combination is displayed when both keys are simultaneously touched but not activated which will override the tool tip if any associated with the first key contacted. If the user releases one of the controls the on screen display may be dismissed in its entirety or the feedback for the other touched control can be reinstated.

In another embodiment of the present invention once the on screen display in the form of a GUI has appeared or faded in the GUI may remain visible if the cursor or pointer e.g. by movement of a mouse has been located therein even if the user ceases contact of the control e.g. button of a touch sensitive mouse that caused the display of the GUI. In this instance the GUI may eventually disappear in response to movement of the cursor e.g. movement of a mouse outside of the GUI activation of another control or after a predefined period e.g. 5 seconds of inactivity in which no input is received by computer such as movement of a mouse.

According to illustrative embodiments of the invention an on screen display may be faded in and out using an animated transition. It should be understood that techniques for controlling appearance and disappearance of an on screen display may be used alone or in combination. An embodiment of the invention providing animated transition is described below with respect to .

The touch outputs provided by input devices can be applied as represented by leads and respectively to input interfaces . These interfaces can produce separate signals reflective of whether contact is then being sensed or not by the contact sensor of each touch sensitive component e.g. auxiliary control of each input device. These signals may be routed as represented by lead to input device drivers that form a component within operating system . The device drivers interpret the signals produced by the input devices and in response generate appropriate events. With respect to touch these events can specify the particular input device and the state of the corresponding touch sensor e.g. whether hand contact is detected. These events can be passed by operating system to application and ultimately within the application to GUI process . Within this process the events are processed by fade in fade out animation process . The animation process in response to the occurrence of a state transition of each input device i.e. whether that device has just started sensing hand contact or has just ceased sensing such contact and whether a display widget e.g. Tool Glass or toolbar s is then being displayed or not will generate a predefined animation sequence to either controllably fade in or fade out the display widget.

Bitmap generator stores predefined patterns and typically texture maps for the various display widgets including the Tool Glass sheet predefined toolbar s and or other display widgets respectively. These patterns are typically stored as separate files within the application. As application program is invoked the application during initialization or after any change to the widget downloads these patterns to Graphics API via line which in turn may route these patterns to a graphics accelerator not shown for local storage therein as texture maps. Alternatively this downloading may be managed by a service provided by operating system .

During subsequent display the accelerator will read these maps from its local store polygonally fill these maps as appropriate and render resulting filled patterns on display . Once these maps have been downloaded then for either a controlled fade in fade out operation animation process changes an alpha transparency value at which the graphics accelerator will render a corresponding filled pattern for a display widget. For a toolbar the alpha transparency value is varied across a full range of transparency values i.e. between approximately 0 and 1.0 on a linear scale of 0 1 where 0 is fully transparent and 1.0 is fully opaque . For a Tool Glass sheet the alpha transparency value is varied across a range of typically 0 to approximately 0.7 such that even with the Tool Glass sheet rendered at its maximum opacity underlying document objects are still visible which some obscuration through the sheet.

In essence for a fade in or fade out operation animation process will issue a series of instructions over a predefined interval of time specifically one such instruction for each different display frame to successively change the alpha transparency value with which a particular display widget is to then be displayed. These instructions will be issued as also represented by line to operating system which in turn will pass these instructions to Graphics API and ultimately to a graphics accelerator.

Fade in should occur over a relatively short interval of time such as on the order of approximately 0.3 to 0.5 seconds. However so as not to distract a user fade out should occur over a relatively long interval such as on the order of approximately 0.7 1.0 seconds. During these intervals particularly on fade in the alpha transparency values are generally varied in a non linear fashion. Empirically to utilize a conventional slow in slow out technique commonly used in screen animation has been chosen. With this technique the opacity initially changes rather slowly from being substantially if not totally transparent i.e. essentially invisible to an intermediate value then increases rather quickly to another intermediate value with further increases then slowing once again until a maximum opacity either e.g. 1 or 0.7 for a toolbar or Tool Glass sheet respectively is eventually reached. This results in an approximately S shaped curve for opacity as a function of time. The same transparency variations are used for fading out a display widget though in a reverse direction . Clearly other time based opacity functions such as a linear function can be used to vary the opacity during the fade in and fade out intervals. Ultimately the particular function s chosen with possibly a different function being used for fade in versus fade out and in fact different such functions can be used for different display widgets if desired will likely be empirically determined through appropriate user testing.

Furthermore a human eye exhibits increased sensitivity to certain colors such as e.g. red tones over others such as blue tones for a common luminance. Hence fading in a display widget that contains any of the former colors even at relatively low luminance could be potentially distracting and more so particularly as these colors become brighter. To avoid such distraction the display widget could be represented by several different texture maps of different or varying coloration and luminance particularly for color s to which the human eye is most sensitive until a final map with desired coloration and luminance is displayed. In that regard a monochrome texture map for this widget could be initially displayed with texture maps for the same widget but having desired coloration with increasing luminance or even different coloration for that matter then being rendered at appropriate times during fade in until at the end of the fade in period a texture map having a final coloration and luminance is rendered. Fade out could be accomplished in a similar though reverse fashion. For example a display widget could fade out using a succession of colored texture maps of decreasing luminance to a monochrome map which itself then fades out to total transparency or at the inception of fade out change to a monochrome map of the widget and from there fade out to total transparency.

If a graphics accelerator is not used then as symbolized by line Graphics API can provide graphics output directly to video adapter specifically a standard video card not shown therein which in turn will generate appropriate video signals and apply those signals to display . In this instance the computer system would need to be sufficiently fast to implement the appropriate graphics capabilities that would have been provided by the graphics accelerator in software. Furthermore where fade in and fade out graphics capabilities are not supported the display and dismissal of display widgets could occur through other visual techniques. These techniques include e.g. simply translating the widget by sliding or shuffling it onto the screen from an off screen position instantly and completely displaying or dismissing the widget rotating the widget if e.g. a toolbar is on a 3 D surface that rotates into place and or zooming in or out interface widgets or portions of a document. However with these techniques the display widgets such as the toolbars and the Tool Glass sheet are constrained to being displayed fully opaque. Any of these techniques could also be used along with fading with a graphics accelerator that supports alpha transparency.

Although alpha transparency capability is supported by a wide variety of currently existing graphics accelerators this capability can be readily simulated in software in a well known manner by conventional 2 D two dimensional or 3 D three dimensional graphics APIs such as D3D which is a 3 D graphics API currently produced by Microsoft Corporation as a standard component of a WINDOWS operating system OpenGL which is currently available in the art or GDI which historically is only a 2 D low level graphics processing layer currently produced by Microsoft Corporation and also incorporated as a standard component of a WINDOWS operating system .

Instances can arise where a display screen is to simultaneously show both a toolbar s and a Tool Glass sheet or other combinations of display widgets . In these instances unwanted interactions can arise that would cause both widgets to fade in or out. To prevent these interactions and attendant user frustration an appropriate decision process well within the skill in the art would be incorporated into animation process to then permit only one rather than both of these display widgets to fade in or out. For example if both widgets were being displayed but a user is then manipulating the Touch Mouse then only the Tool Glass would be permitted to fade out while the toolbars remained fully displayed. The specific decision process would be governed by the particular widgets that could be simultaneously displayed a need to continue displaying one or more these widgets based on a current contextual setting of the application including an operation then being performed and relative display prioritization among these widgets.

It should be understood that the present invention may employ other sensing technologies apart from touch sensing to invoke on demand action such as e.g. galvanic skin response non contact proximity sensors pressure sensors events from touch proximity sensors on a keyboard data from a GPS global positioning system receiver position of the user or location of a mobile computer video data from a camera and audio data from a microphone.

While particular embodiments of the present invention have been described and illustrated it should be understood that the invention is not limited thereto since modifications may be made by persons skilled in the art. The present application contemplates any and all modifications that fall within the spirit and scope of the underlying invention disclosed and claimed herein.

