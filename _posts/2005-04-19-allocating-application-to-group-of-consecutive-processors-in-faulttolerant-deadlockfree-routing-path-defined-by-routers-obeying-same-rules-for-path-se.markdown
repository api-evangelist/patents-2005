---

title: Allocating application to group of consecutive processors in fault-tolerant deadlock-free routing path defined by routers obeying same rules for path selection
abstract: In a multiple processor computing apparatus, directional routing restrictions and a logical channel construct permit fault tolerant, deadlock-free routing. Processor allocation can be performed by creating a linear ordering of the processors based on routing rules used for routing communications between the processors. The linear ordering can assume a loop configuration, and bin-packing is applied to this loop configuration. The interconnection of the processors can be conceptualized as a generally rectangular 3-dimensional grid, and the MC allocation algorithm is applied with respect to the 3-dimensional grid.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07565657&OS=07565657&RS=07565657
owner: Sandia Corporation
number: 07565657
owner_city: Albuquerque
owner_country: US
publication_date: 20050419
---
This invention was developed under Contract DE AC04 94AL8500 between Sandia Corporation and the U.S. Department of Energy. The U.S. Government has certain rights in this invention.

This application discloses subject matter that is related to subject matter disclosed in co pending U.S. Ser. No. 11 110 466 entitled DISTRIBUTED COMPUTE PROCESSOR ALLOCATOR ARCHITECTURE FOR MULTIPLE PROCESSOR COMPUTING APPARATUS and U.S. Ser. No. 11 110 344 entitled SCALABLE MULTIPLE PROCESSOR COMPUTING APPARATUS WHICH SUPPORTS SECURE PHYSICAL PARTITIONING AND HETEROGENEOUS PROGRAMMING ACROSS OPERATING SYSTEM PARTITIONS now U.S. Pat. No. 7 246 217 both filed concurrently herewith.

The invention relates generally to multiple processor computing and more particularly to communication routing and processor allocation in multiple processor computing.

In a multiple processor computing apparatus such as a supercomputer the processors are typically interconnected by a network of interconnected routers at least some of which are connected to the respective processors. Each of the routers is also connected to a plurality of other routers in the network. In such a computing apparatus it is of course important to route communications among the processors in an efficient manner. Perhaps more importantly however is the need to avoid deadlock situations in the network of routers. A deadlock situation can exist for example when each of a plurality of routers that are interconnected with one another to form a ring or loop has a message for the next adjacent router in the loop and all of these messages are traveling in the same direction around the loop. This type of deadlock condition and many others are well documented in the art.

Another important aspect of routing is fault tolerance. The system should preferably be able to avoid deadlock even when one or more of the routers becomes inoperative fails .

Another important aspect of operating a multiple processor computing apparatus is the allocation of processors to execute applications. For any given job the efficiency with which that job can be performed or executed is impacted by the set of P processors allocated to perform that job. For example if first and second different sets of P processors can be allocated to perform the job one of the sets of P processors will typically perform the job less efficiently than would the other set of P processors.

It is desirable in view of the foregoing to provide for fault tolerant deadlock free routing and efficient processor allocation in a multiple processor computing apparatus.

Exemplary embodiments of the invention utilize directional routing restrictions and a virtual logical channel construct to provide fault tolerant deadlock free routing in a multiple processor computing apparatus. Some exemplary embodiments perform processor allocation by creating a linear ordering of the processors based on routing rules used for routing communications between the processors. In some embodiments the linear ordering of processors loops back around upon itself and bin packing algorithms are applied to this linear ordering to obtain a processor allocation. In some embodiments the interconnected processors are conceptualized as a generally rectangular 3 dimensional grid and the MC allocation algorithm is applied with respect to the 3 dimensional grid.

Application In the context of this document an application runs on one or more compute processors and also service processors in some cases and is managed by Launcher. Users create applications and run them on a computing system. The user s purpose for using the computing system is to run applications.

Batch Job A type of job that runs unattended. Users submit batch jobs to a batch system in the form of a job script. The batch system determines when and where i.e. on which service processor the job script should be run.

Compute Processor The computing system is typically made up of many thousand compute processors. Applications run on a partition of compute processors that was allocated by the CPA.

Interactive Job A type of job that requires user interaction. Interactive jobs are submitted to the batch system in a similar way to batch jobs but without a job script. When the batch system launches an interactive job it opens up a shell on a login processor for the user to interact with. Interactive jobs are useful for tasks such as debugging.

Job A job is a task or set of tasks being performed by or on behalf of a user e.g. invoke Launcher to launch an application . Jobs are submitted by users to the batch system in the form of a job script. The batch system determines when a job should run based on a scheduling policy and the available resources. The batch system terminates a job when it exceeds its time limit. A job is considered finished when its job script exits.

Job Script A UNIX shell script defining the commands to run for a batch job. Typically a job script will contain one or more Launcher invocations.

Login Processor The computing system is typically made up of many login processors. Users are placed onto login processors by a load balancing mechanism. Launcher can only be run from login processors.

Partition A partition defines a physical set of compute processors. The batch system allocates a partition for each job it launches. A job can only access the compute processors in its partition. Purely inactive launchers those not part of a batch job also run inside of a partition. When an interactive launcher wishes to run its application it must first create a partition.

PCT One process control thread PCT daemon runs on each compute processor. Launcher communicates with PCTs in order to launch and manage its application. The CPA communicates with PCTs in certain error cases.

Showmesh The Showmesh program used by users to display the state of all compute processors in the system.

Launcher Launcher is the program that launches and manages an application running on compute processors. Launcher must request and be granted a set of compute processors from a compute processor allocator before it can run its application.

For clarity of exposition some components such as local memory devices not necessary for understanding the present invention have been omitted in .

Referring to some exemplary embodiments include R 4 rows of cabinets C 31 columns of cabinets K 3 card cages per cabinet and B 8 circuit boards per card cage. In such embodiments a compute processor cabinet which houses only compute processors would house 4 24 96 compute processors and a service processor cabinet which houses only service processors would house 2 24 48 service processors. In some embodiments 27 of the 31 columns are populated with compute processor cabinets and the remaining 4 columns 2 columns on each end of the array include only service processor cabinets. Such embodiments thus include 4 27 96 10 368 compute processors and 4 4 48 768 service processors. The network mesh of in conjunction with the generally rectangular cabinet array of permits the computing apparatus to be physically scaled upwardly to include more processors as desired.

For the cabinet array of with R rows and C columns each xy plane or planar grid of the logical network mesh corresponds to the interconnections of the routers of all boards in the same corresponding board positions in the cabinets. So the xy plane will include 4RC of the routers of . The routers on each board are connected in series with one another each router is connected to its corresponding router on the correspondingly positioned board in the next row adjacent cabinet and each set of series connected routers of a given board is connected in series with the set of series connected routers of the correspondingly positioned board of each column adjacent cabinet. This results in 4R series connected routers per column of y direction of and 4 sets of C interconnected routers per row of x direction of . Adjacent columns in are indicated at CN 1 CN CN 1 and CN 2 and adjacent rows in are indicated at RN and RN 1.

More particularly and referring to as an example the left most router of would be connected in a torus with each of the other KB 1 left most routers on the other KB 1 circuit boards within its cabinet. So each router of each board is interconnected with every other positionally corresponding router of every other board in the cabinet to form a torus. As mentioned above with respect to each planar grid of the mesh includes 4RC routers so there are 4RC interconnections in the z direction between each plane of the three dimensional network mesh as illustrated in .

Each of the disconnect cabinets includes a switching structure which is designated generally by an X in the cabinet. These switching structures permit the x direction connections in the three dimensional network grid to be selectively closed and opened in order to provide flexibility for classified unclassified partitioning. In the example of all compute cabinets between the end pairs of service cabinet columns are selectively switchable into communication with one another and with either of the end pairs of service cabinet columns. The service cabinets provide access to the user interfaces and disk storage and see also .

When all of the switch structures are closed to make all x direction connections between all cabinets then all compute cabinets and all service cabinets are available to all users. When the switch structures in the cabinets of Column are opened and all other switches are closed users at on the right hand side of can still utilize the right hand pair of service cabinet columns to access disk storage at the right hand side. In this configuration all of the compute cabinets and the left hand pair of service cabinet columns are available for classified operations for users at the left hand side and are also isolated from unclassified users at the right hand side. If this configuration is modified by closing the switches at Column and opening those at Column then users at the right hand side would have access to the compute cabinets between Columns and but still would not have access to the remainder of the compute cabinets and vice versa for the users on left hand side of the arrangement. Thus various combinations of opening and closing the switches in Columns and can provide various numbers of compute cabinets for either classified or unclassified operations but in any event unclassified users can still access service cabinets and disk storage. In other switch configurations the unclassified users can access some or even most of the compute cabinets depending on the amount of compute cabinets needed for the desired classified operations.

Referring again to and the corresponding description there are 4 KB connections between row adjacent processor cabinets so the switching structures must each be capable of switching 4 KB communication paths. In the aforementioned example where K 3 and B 8 each of the 16 switch structures illustrated in must be capable of switchably making and breaking 4 3 8 96 x direction connections between cabinets.

The aforementioned capability of launching and managing applications from a single job on both compute processors and service processors permits the service processors to render support processing e.g. graphics processing relative to the computational information produced by the compute processors involved in the job. Without the PCT emulator layer the launchers within a single job can launch and manage applications on either the compute processors or the service processors but not on both compute and service processors during the same job. Therefore without the PCT emulator any computations produced by the compute processors would need to be stored on disk storage see also and the aforementioned support processing performed by the service processors would have to be performed on the stored computations during another job.

The CPAs can be implemented as daemons e.g. UNIX programs written in C running on the respective login processors. diagrammatically illustrates various entities which interface with any given CPA daemon. The CPA daemon can communicate with each of the illustrated entities via suitable application programming interfaces and associated library functions. The use of application programming interfaces for communication among software models is well known in the art. In some embodiments the batch system running on a service processor and the launchers are the only clients of the CPA. The launcher clients are illustrated generally at and in . As illustrated some launchers are part of jobs that have been dispatched by the batch system and other launchers are interactive launchers which launch applications for activities such as interactive development and debugging operations.

In some embodiments the CPA daemons run continuously on the respective login processors. The CPA daemon accepts requests from its clients launchers and the batch system. The CPA daemon waits for client requests in its main dispatch loop. Upon receiving a client request the CPA processes the request and returns the result to the client. It then returns to waiting. Client requests are processed in FIFO order. The CPA daemon can also respond to event indicators received from the RCA Resiliency Communication Agent . As described in more detail below upon receiving an RCA event the CPA determines if it must take action and if so performs the required action. It can then return to waiting in its main dispatch loop. RCA events are processed in FIFO order.

The batch system can cooperate with a CPA to create and assign a compute processor partition for each job before it is started. In this sense a compute processor partition is simply a number of processors required by the batch job or interactive application . Each launcher that is part of a batch job must allocate compute processors from the partition in which the job is running. An interactive launcher can cooperate with a CPA to create and assign to itself its own compute processor partition and can then allocate processors from the partition for the application to be launched by the interactive launcher.

In some embodiments the compute processors are divided into a batch pool and an interactive pool the batch pool available for batch jobs and the interactive pool available for the applications launched by interactive launchers. Only the batch system may create a partition in the batch pool. When the batch system or an interactive launcher at requests the CPA to create a partition the CPA retrieves from the system database a list of available compute processors. This aspect of the invention is useful because state information for the entire computing apparatus can be stored in the system database . This means that all of the CPA daemons can be stateless. This decreases the processing burden placed on the login processors to run the CPA daemons and also makes the CPA daemons more modular in nature and more easily selectable to replace one another in the even of CPA failures. The system database in some embodiments is implemented by one or more service processors running the commercially available MySQL server and accessing disk storage see also .

Once the CPA receives the list of available processors from the system database the CPA calls a physical processor allocation algorithm which can be included as part of the CPA daemon in some embodiments and provides that algorithm with the list of available processors the partition size requested by the client and a processor ID list to use when allocating the compute processors. The processor ID list is optionally provided by the batch system or interactive launcher program to specifically identify the compute processors desired for its applications.

In some embodiments the physical processor allocation algorithm can allocate compute processors according to any suitable conventionally available algorithm. Once the compute processor allocation has been completed at the CPA communicates with the system database to update the overall system state such that it reflects the new compute processor allocation and provides the new compute processor allocation information to the requesting client.

When a batch job exits the batch system requests destruction of the partition that the job was running in. An interactive launcher requests destruction of its partition when it exits. Whenever the CPA destroys a partition it updates the system database appropriately to reflect this destruction.

As indicated above the system database stores persistent state information such as what compute processors are available for allocation how many compute processors are available for allocation how many compute processors a job is allowed to use and what launchers are running and which compute processors have been assigned to them. If any part of the CPA architecture crashes the system database information is used to restore state when the CPA architecture is restarted. This use of database backend has several benefits. First it provides robust mechanisms for storing state. When system state is to be changed a conventional atomic database transaction can be used to insure that the state is either completely updated or not at all. This improves upon prior art systems that store persistent state information in a flat file. It is difficult to ensure that a flat file is written consistently when the CPA crashes.

Another advantage is that the database backend provides a straightforward mechanism for storing and efficiently querying structured information for example using standard SQL statements. Designing database tables is less error prone and more flexible than designing custom data structures for the CPA. Finally the use of a database enables the compute processor allocator architecture to be distributed. Conventional network databases are designed to multiplex many simultaneous clients e.g. CPAs . Locking mechanisms and transaction semantics are provided to prevent clients from conflicting with one another and corrupting data.

Furthermore with respect to the system database a program designated as Showmesh in provides users with the capability of accessing the state information stored in the system database . In some embodiments the Showmesh program illustrated in runs on a service processor and uses the conventional SQL2C library to query the system database . By interacting directly with the system database on behalf of interested users the Showmesh program provides a communication path to the system database that is independent of the CPA daemons. The design of the CPA daemons can thus be simpler than in prior art systems wherein the CPA daemons support user access to the database.

Some embodiments do not require that a CPA daemon run on each login processor. In such embodiments the CPA daemon is designed such that it can process requests from launchers running on other login processors which do not have CPA daemons. However by maximally distributing CPA daemons among the login processors the burden of management duties on any single CPA daemon will be reduced. The distributed design of the CPA structure is more scalable than prior art single daemon approaches. At the same time distributing the CPA daemons only among the login processors provides advantages in terms of processing power when compared to prior art systems that provide CPA daemons on every compute processor.

In some embodiments the persistent state information maintained in the system database of includes some or all of the information described below.

The Compute Processor Allocation Table contains one row example row shown above for every compute processor in a compute system that is available for allocation. The Alloc Mode field specifies how the processor can be allocated. If the Alloc Mode is set to batch the processor may be allocated to batch jobs. If Alloc Mode is set to interactive the processor may be allocated to batch jobs and interactive Launchers. If Alloc Mode is set to reserved the processor may not be assigned in the future. The Partition ID field specifies the partition ID that a process is part of or is NONE if the processor isn t part of a partition. The Launcher ID field specifies the Launcher ID that the processor has been assigned to or NONE if the processor isn t assigned to a Launcher.

The Partition Table contains one entry example entry shown above for each compute processor partition in the system. The compute processors making up a partition can be obtained by inspecting the Compute Processor Allocation Table. The Partition ID filed stores the ID that the CPA assigned to the partition. This ID is guaranteed to be unique within a single boot shutdown cycle of the computing apparatus. The Administration Cookie field stores a pseudo random number that a client must match in order to destroy the partition. The Allocation Cookie field stores a pseudo random number that a client must match in order to allocate processors from a partition. Both cookie fields can only be read and set by the CPA daemon. The User ID specifies the UNIX user name of the partition s owner. The Batch Job ID field specifies the batch job ID that the partition has been assigned to or NONE if the partition is in use by an Interactive Launcher. The Partition Creation Time field stores the date and time when the partition was created. The Max In Use field stores the maximum number of compute processors simultaneously in use by Launchers running inside of the partition. The Batch Job Error flag is set when a batch job encounters an error with one or more compute processors in the partition. This flag is also set when a Launcher running inside of the partition that is part of a batch job exits abnormally.

In some embodiments each launcher registers with the RCA then cooperates with a CPA to obtain a compute processor allocation and then launches its application. The RCA provided on every login processor in some embodiments monitors a periodic heartbeat signal provided by the launcher. When the launcher s application has completed and the launcher exits it unregisters with the RCA. If the launcher heartbeat signal ends before the launcher unregisters with the RCA then the RCA reports this occurrence to the CPA. This indicates that the launcher has exited improperly in some manner. The CPA responds to this indication by invoking an executable on its login node which executable cleans up the compute processors by interrupting and closing any still running applications that had been launched by the launcher that exited improperly. The operation of the clean up executable is designated generally at in .

As can be seen from the foregoing description each of the routers is capable of supporting six bi directional hops between itself and six other routers one router for each direction in each dimension of the logical three dimensional network mesh. This can be seen from above and is also illustrated generally in .

The two pairs of inputs and outputs in each direction represent two logical channels referred to herein as channel and channel . Thus for each of the three dimensions each router supports channel and channel operations in both directions for that dimension. Taking the y dimension of as an example y designates channel extending through the router in the y direction y designates channel extending through the router in the y direction y designates channel extending through the router in the y direction and y designates channel extending through the router in the y direction. Channels and extending in each of the x and z dimensions can also be seen from .

Although the xz plane portion of the router has not been illustrated it can be readily seen that the xz portion is arranged in generally analogous fashion to the xy and yz portions of .

It should be evident that for any given direction in any given dimension channel and channel need not be implemented as physically separate channels but can be implemented as logical or virtual channels by simply including within each discrete communication block e.g. packet information which indicates whether that block is traveling on channel or channel . Packets in channel are not permitted to block packets in channel and vice versa

Studying the exemplary rules of in more detail it can be seen that for the xy plane all routing in the x dimension must be finished before any routing in the y direction can be performed. For the yz plane all routing in the y dimension must be completed before any routing in the z direction can be performed.

The table of indicates the general rule for virtual channels namely that a communication which arrives at the router on channel must in general be output from that router as a channel communication and a communication that arrives on channel must in general be output on channel .

The table of also includes special rules for utilizing channel and channel in the z dimension in wraparound situations. As mentioned above the general rule for handling channel and channel in all dimensions is that if the incoming communication is designated as channel when the router outputs that communication it will also be designated as channel and input communications designating channel are output as communications which also designate channel . Although all three dimensions utilize the channel channel organization this two channel organization is particularly important in the z dimension because at any given xy coordinate the z dimension router connections are configured as a torus which wraps around from the KBth plane back to the first plane in . It is therefore necessary to avoid the possibility of communication deadlock in such a wrap around configuration.

So for example all 4 RC routers in the KBth plane of can route traffic traveling in the z dimension according to the z wraparound rule of . When traffic passes through any of these routers in the z dimension it is crossing a wraparound boundary of the z dimension torus and must therefore be handled according to the z dimension wraparound rule of . More specifically if a communication traveling in the z dimension arrives at the router on channel the router must output that communication in the z dimension as a channel communication. This applies to traffic crossing through this z dimensional wraparound boundary in either direction of the z dimension. Moreover if any of these wraparound routers receives an input communication traveling in the z direction and designated as channel the router is forbidden from outputting that communication in the z dimension. By handling channels and in this fashion for traffic traveling in the z dimension through any of the 4 RC routers in the designated wraparound plane deadlock in the torus configuration can be avoided. The z dimension boundary routers of the KBth plane handle channels and in the x and y dimensions according to the general rule of . Finally for all routers in the network all routes begin on channel .

The routing conventions as described above and illustrated in advantageously provide fault tolerant deadlock free routing in the three dimensional network mesh of . The prior art approach also uses the z dimension wraparound rule of but does not impose the general virtual channel rule of so no router is subject to any rules regarding the use of the virtual channels in the x and y dimensions. But it can be shown that this lack of attention to virtual channels in the x and y dimensions can result in deadlock.

The dimensions x y and z can be interchanged in the structure of the rules of depending on the network topology without affecting the routing methodology. Also the directional choices in can be changed without affecting the routing methodology. For example phase could instead permit outputs in the y direction and z direction or in the y direction and the z direction or in the y direction and the z direction so long as the z dimension output direction permitted in phase is the same as the z dimension output direction permitted in phase and the z dimension output direction permitted in phase is the opposite of the z dimension output direction permitted in phases and . So phases and could permit outputs in the z direction while phase would permit outputs in the z direction. Generally all routing in one of the three dimensions is confined to phase with routing in one direction of each of the other two dimensions also permitted in phase with phase accounting for all remaining routing in one of the remaining two dimensions while also permitting routing in the same direction of the third dimension as in phase and with phase only permitting routing in the third dimension and in the opposite direction that was permitted for the third dimension in phases and .

Referring again to hereinbelow are described exemplary processor allocation techniques that can be implemented by the physical processor allocation algorithm according to exemplary embodiments of the invention.

The shape of the curve in is based on the routing table rules of . Because the rules of basically require that the routes between compute processors must finish moving in the x dimension before finishing their movement in y dimension and must finish moving in the y dimension before finishing their movement in the z dimension the curve of spans the x dimension before incrementing in the y dimension and also accounts for the entire xy plane before moving in the z dimension described below .

The serpentine curve of corresponds to tracking across the xy planar grid of from left to right in the x direction then incrementing by 1 in the y direction then back from right to left in the x direction then incrementing by 1 in the y direction and then repeating the pattern until all compute processors in the xy planar grid have been intersected by the serpentine pattern. The resulting number line is numbered consecutively from the first compute processor at the lower left of the xy planar grid designated as A to the final compute processor at the upper left of the xy planar grid designated as B. Because 4R is an even number the compute processor associated with B will have the same x coordinate as the compute processor associated with A for example x 0 in . An entire number line or linear ordering is constructed by connecting the serpentine portion of the number line in each plane to the serpentine portion of the number line in the adjacent plane in the z direction. For the xy plane where z 0 the coordinates on the number line increase consecutively from point A to point B as illustrated by arrows in . From point B in the z 0 plane the next coordinate on the number line is point B in the z 1 plane. The coordinates on the number line continue to increase consecutively in this z 1 plane from point B back to point A but in a direction opposite the arrowheads of as shown by arrows in . The next point on the number line after point A in the z 1 plane would be point A in the z 2 plane. The number line portion in the z 2 plane would then look the same as the number line portion for the z 0 plane that is the same as in . The next point in the number line after point B in the z 2 plane would be point B in the z 3 plane. The number line portion in the z 3 plane will look the same as the number line portion in the z 1 plane that is the same as in .

To summarize in the xy planes where z 0 or z is an even number the number line portion of applies with the coordinates of the number line increasing consecutively from point A to point B. For x y planes where z is odd the number line portion of applies with the number line coordinates increasing consecutively from point B to point A. For all xy planes where z is an odd integer N the coordinate of point B in that plane is the next consecutive coordinate after the coordinate of point B in the xy plane where z N 1. For all xy planes where z is an even integer L the coordinate of point A of that plane is the next consecutive number line coordinate after the coordinate of point A of the xy plane where z L 1. This is illustrated generally in where the serpentine portions and of are illustrated by dashed lines the continuities between those portions of the number line are illustrated by solid line and KB is even.

So long as there are an even number of z planes KB is even in the three dimensional mesh the highest coordinate of the number line will reside at point A in the xy plane having the highest z coordinate. All points A in all of the xy planes correspond to one another positionally in the cabinets and therefore each point A is in fact connected in a torus configuration with every other point A as described above with respect to . Accordingly point A in the xy plane with highest z coordinate is literally only one hop away from point A in the xy plane where z 0. Due to this connectivity the number line formed by interconnecting the serpentine number line portions of the various xy planes as shown in actually assumes a wraparound or ring configuration illustrated at in .

The application of bin packing algorithms to number line orderings is known in the art. The intervals of contiguous free processors are analogous to free space in unfilled bins. In the bin packing approach to allocating processors contiguously it is first determined if there is a contiguous interval of free processors large enough to run a given job. If the job can be allocated to contiguous processors the interval can be chosen based on adaptations of one dimensional bin packing algorithms. For example the job can be allocated to the first interval that is large enough first fit allocation . As another example the job can be allocated to the interval that minimizes the number of unallocated processors remaining in the interval after the job is allocated to the interval best fit allocation . As another example for each interval to which the job could be allocated the number of intervals of each size that would remain is determined. The job is then allocated to the interval that minimizes the sum of squares of these numbers of intervals sum of squares allocation . For any tie the job is allocated to the smallest number line value possible.

The aforementioned bin packing algorithms can be adapted for use with the closed ring number line described above with respect to . This can be done by simply taking the original number line defined from point A in the z 0 plane and ending at point A in the z KB 1 plane then placing a copy of that number line adjacent the original number line and numbering its points consecutively beginning with the highest number of the original number line. This is illustrated generally in . The broken line separating the two number lines and in corresponds to the point of wraparound in the ring structure number line of . If application of a conventional bin packing algorithm results in the allocation of some points on both sides of the broken line in then that solution will exploit the wraparound feature of the ring number line of and so will include the processors associated with coordinates 1 and 4R C 4 KB.

Some exemplary embodiments apply the conventional MC allocation algorithm to the three dimensional grid of . For each free processor the quality of a job allocation centered on that processor is evaluated. This is done by counting the number of free processors within a three dimensional sub grid centered on the free processor itself and also by counting the number of free processors within one or more three dimensional sub grid shells of processors surrounding the three dimensional sub grid. The processors are then weighted according to the shell where they are encountered. In some embodiments the weight for the central free processor is 0 the weight for the processors of the closest surrounding shell is 1 the weight for the processors of the next most closest surrounding shell is 2 and so on. Free processors are selected from the inside shell s before the outside shell s until enough are found for the allocation. The sum of the weights of the selected free processors gives the cost of the allocation and the algorithm chooses the allocation with the lowest cost. As one example a 3 3 3 shell surrounds a free processor and is also surrounded by a 5 5 5 shell with both shells centered on the free processor.

As an example if 100 processors are required for an allocation and given a first free processor if 30 free processors are identified in the surrounding 3 3 3 shell and the remaining 69 free processors are identified in the surrounding 5 5 5 shell this would yield a resultant weight of 1 30 2 69 168 where the processor weights are 1 and 2 for the 3 3 3 and 5 5 5 shells respectively. If for a second free processor 25 processors are free in its surrounding 3 3 3 shell and the remaining 74 processors come from its surrounding 5 5 5 shell this yields a resultant weight of 1 25 2 74 173. The first free processor and the free processors from its associated surrounding shells will therefore be allocated because their resultant weight of 168 is smaller. The 3 dimensional MC allocation process is illustrated generally at in .

Although exemplary embodiments of the invention have been described above in detail this does not limit the scope of the invention which can be practiced in a variety of embodiments.

