---

title: Job categorization system and method
abstract: A computer system and method for capture, managing and presenting data obtained from various often unrelated postings via the Internet for examination by a user. This system includes a scraping module having one or more scraping engines operable to scrape information data sets from listings on the corporate sites and web sites, direct feeds, and other sources, wherein the scraping module receives and stores the scraped listing information data sets in a database. The system also has a management platform coordinating all operation of and communication between the sources, system administrators and processing modules. The processing modules in the platform include scraping management module analyzing selected scraped data stored in the database, and a categorization module that examines and categorizes each data set stored in the database into one or more of a predetermined set of categories and returns categorized data sets to the database.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07702674&OS=07702674&RS=07702674
owner: Yahoo! Inc.
number: 07702674
owner_city: Sunnyvale
owner_country: US
publication_date: 20050630
---
This application claims the benefit of priority of U.S. Provisional Patent Application Ser. No. 60 661 280 filed Mar. 11 2005. This application is related to U.S. patent application Ser. No. 11 174 393 filed Jun. 30 2005 U.S. patent application Ser. No. 11 173 837 filed Jun. 30 2005 and U.S. patent application Ser. No. 11 173 656 filed Jun. 30 2005 the disclosures of all of which are hereby incorporated by reference in their entirety.

This disclosure relates to computer software and more particularly to a software system and method for managing data listings.

A challenge common to companies that need to manage and present listings of data to customers such as real estate brokers employment recruiters and travel agencies is conveying the information they have in a succinct and intelligent manner such that the users of such data gain the particular information they seek in an optimum efficient and effective manner in the shortest period of search time. Another challenge is the need to track data flow information transfer between various business units and management entities utilizing listing data.

One illustrative example involves the field of employment recruitment. The challenge for companies seeking to attract talented employees is finding the best set of candidates for the position available. The challenge for job seekers is finding the right job. One standard practice among human resource departments is to create a job description for each open position then advertise the position along with the description. Recruiters and job seekers then have to review and analyze these descriptions in order to determine a match between job seekers and particular jobs.

A number of searching tools are available to a person searching on the Internet for the right job based on his or her skill set. Typical searching tools currently available require the job seeker to select various criteria in the form of keywords such as desired locations types of jobs desired compensation levels etc. Similarly the employers provide in addition to the job description levels of skill education years of experience etc. required to be considered for a particular job. Searching tools then look up the seeker s keywords in a data base of job descriptions and return or display those job descriptions that contain the job seeker s keywords. However available search tools still either often require the employer and the job seeker to each sift through a large number of so called search results or can return no search results at all if the criteria provided is too specific or narrow.

In general a number of searching tools are available to a person searching on the Internet for any data that is compiled in listings form for example a new home in a particular area. Typical real estate searching tools currently available require the home buyer or the buyer s agent to select various criteria in the form of keywords such as desired locations types of house lot size school system street location preferences price range etc. A listing real estate broker provides typically in a multiple listing service in addition to the home description pictures and other data such as square footage of the lot the house and number of bedrooms and baths for example. Searching tools then look up the user s keywords in a data base of homes and return or display those homes that contain the user s keywords. However available search tools still either often require the user either the real estate broker or the potential buyer or other user to each sift through a large number of so called search results on multiple sites. It would be desirable then to provide a search management system that more effectively collects the listings data normalizes the data and manages the interfaces between users and providers of listings data.

The system described herein incorporates Platform for Advanced Listing Management software system for managing searching tools for any kind of listing data such as job postings resume listings real estate listings product listings etc. The system is fully distributable among several machines and is scalable. Each module described below within the software system is scalable and may include multiple instances dictated by the amount of data to be handled and processed therein.

An embodiment of the system described herein is a computer software system for managing capture and processing of listing information data captured through a data network from a plurality of sources for compilation into a searchable data structure. The system includes an administrative portal module providing system administration and operational control through a network interface and one or more listing manager modules responsive to instructions provided via the administrative portal module operable to control access to the sources control retrieval of listing information data and process the listing information data received from those sources. Each of the listing manager modules controls task managers to categorize the listing information data examine portions of the categorized listing information data for conformance to predetermined quality criteria and store the categorized listing information data for use in a search bank.

Each listing manager module includes one or more task managers that each comprise a scraping management module coordinating operation of and communication between one or more scraping engines to obtain scraped data sets from sites identified by a site management module in the administrative portal module and store the scraped data sets in a database. The listing manager module also preferably includes a quality management module coupled to the scraping management module analyzing each scraped data set stored in the database for conformance to predetermined quality rules and a listing data categorization module operable to examine and categorize each data set stored in the database into one or more of a predetermined set of categories and return categorized data sets to the database and a search bank synchronizer communicating with the database for compiling and transferring categorized data sets from the database to the search bank.

An embodiment of an exemplary system operates through the use any available means of accessing listing information. Such means may include direct feeds web based feeds XML feeds and the use of scraping technology to scour the web and obtain listing information from sites available on the Internet and particularly the World Wide Web although as listing information may be distributed on other networks now known or to become known the system and functionality described herein is applicable to any distributed information environment whereby information can be obtained by manual or automated systems.

As an exemplary implementation job seekers and job description and job postings are described herein. However the management system has much broader application than simply job hunting. It can be implemented with any type of data management system where listings of data or other compilations of data records are to be managed. The system described herein is modular and scalable and may be implemented as a stand alone system on a single computer or its modular functionality may be distributed among disparate computers servers etc. communicating through appropriate network interfaces.

A job seeker seeking information about jobs will have a larger universe of job descriptions to review when utilizing an embodiment of the system described herein. Specifically the system makes use of scraping technology to build a database that is populated with job descriptions. The database may also include job descriptions from other sources such as job descriptions supplied by corporations seeking applicants and or provided by methods other than through scraping. The system receives the job descriptions and then utilizing an internal categorization and quality management process maximizes the quality of the information contained in each individual job description to maximize usefulness to the user and to improve the user s overall job searching experience when utilizing the system described herein.

A method of obtaining handling and compiling listing data sets in accordance with the present disclosure includes obtaining listing information data sets from one or more listings on one or more sites available through the Internet storing data sets corresponding to each scraped listing in a database analyzing each data set stored in the database for conformance to predetermined quality criteria and categorizing each data set stored in the database into one or more predetermined categories and returning the categorized data set to the database. The method further may include obtaining listing information data sets from one or more of customer sites through an XML feed an RSS feed and direct input from a variety of sources. The categorizing operation preferably includes determining and assigning a confidence value for each data set in each of the predetermined categories. This determination preferably and more specifically involves comparing text of each obtained data set with text of previously categorized data sets in a categorization database and determining a confidence value in each predetermined category for each obtained data set.

A preferred embodiment of the method of this disclosure includes operations of accessing and or scraping job description data from one or more job listings on one or more corporate career sites or job boards storing the scraped job description data corresponding to each scraped job listing in a database analyzing each scraped job description data stored in the database for conformance to predetermined quality criteria categorizing each job description stored in the database into one or more predetermined job categories and returning the categorized job descriptions to the database and transferring categorized job description data from the database to a search bank.

The categorizing operation preferably includes operations of comparing text of each scraped job description with previously categorized job description text in a categorization database and determining a confidence value in each predetermined category for each scraped job description. More preferably the method includes flagging each categorized scraped job description that has a confidence value below a predetermined value for manual review and providing a manual review interface permitting a reviewer to verify any flagged categorizations.

A high level block diagram of an exemplary system utilizing a platform for advanced listing management PALM system in accordance with embodiments of the present disclosure is shown in . The system is a distributed software system designed to obtain listing information data sets from a plurality of sites via the internet or other network access process the data sets in the PALM system store the processed data sets in one or more databases and then populate one or more search banks for access through a web server cluster by a user .

The PALM platform is a listing lifecycle management platform system that facilitates automation and customization of business processes for content acquisition classification quality performance and display. In an exemplary application embodiment described below in more detail with reference to the PALM system is utilized in an employment job search and placement context. However it should be understood that the system incorporating a PALM system can be utilized to manage any complex listing scheme where a large number of data sets are involved.

Referring back to the system generally incorporates all of the potential management functionality of a listings management system into modular form in system . Thus the system basically has a portal section a series of listing managers and preferably an external process integration module . In addition the system includes platform Application Programming Interfaces APIs a customer self service portal and administration interfacing portal APIs . Basically the system interfaces with external input sites and other sources for example via the Internet . Administrative operational personnel have access to the PALM portal via the administrative portal through an intranet . Certain paid customers may also interface through the Internet into the PALM portal through a self service portal if they have been given administrative access to the PALM system .

The listing data retrieved from the external sites and sources is processed within the PALM system . The PALM system then populates one or more search banks . Information in the search banks is then accessed by a web server cluster for display via the Internet upon query by a user .

The PALM system permits an administrative operator to accelerate data listings acquisition processing and availability for display. The PALM system preferably incorporates one or more PALM processing machines or listing manager modules . The system also interacts through appropriate PALM application programming interfaces APIs for external communications such as for administrative access control examination and reporting functions and accounting finance sales and customer information functions.

The PALM portal includes a number of functional modules that can be utilized to access control and interrogate processes performed by the PALM listing managers in the PALM system .

The User Management Single Sign on module provides role based access control for all authorized administrators and supports Create Review Update and Delete CRUD use cases for managing user access permissions and roles supports standalone authentication or through centralized enterprise authentication also known as single sign on activities and provides approvals and administrator workflows. This module also permits an administrator to perform a single sign on activity in order to access any functionality for which he is authorized within the PALM portal .

An exemplary screen shot of a user management user interface for the user management module is shown in . In a user administration screen is shown which lists exemplary permitted usernames and their access permissions that shows that they are administratively authorized to perform or edit functions within the PALM system . A user interface screen for a self service customer entering through the self service portal would be much more limited as such a customer would preferably have only limited functional access within the system .

The Agent Site Management module controls the operation of scraping engines discussed in further detail below with respect to a particular embodiment involving the management of job information obtained by scraping. This module supports CRUD use cases for managing sites and site attributes such as usernames and passwords needed for an Agent to automatically access remote sites. This module also enables disables approves and denies requests for agents sites. These site attributes include 

The Quality Review module provides a manual tool that permits an operator to review listing quality review content issues and errors and validate or invalidate listings. For example the validation operations may include invalidations for test junk and offending content listings. Finally the quality review module provides a detailed manual review mechanism for the automated quality review task described below with reference to . In essence the quality review module permits an operator to retrieve data sets from the database that have been flagged as not meeting predetermined quality criteria.

The Listing Lifecycle module permits fine tuning and adjustment of throughput options and performance of the PALM system in operational control of listing data input and output. For example this module can backfill listings by Countries demographics DMAs Verticals Quality or other parameters. For example in a region of the country where there are few paid listed agricultural jobs the displayed results to a User supplemented i.e. backfilled with scraped listings from such areas or may be populated with listings that would otherwise be excluded based on a quality level determination. Thus this module can be used to adjust a blending throttle between scraped display information and paid display information change the percentage of scraped and paid listings by various parameters such as Country DMAs or verticals. It includes a functionality for comparing statistics and performance of paid scraped and premium listings. It provides CRUD use cases for managing listing types and attributes and manages content listings display and expirations schedules.

The Reporting module supports a number of reporting tasks by industry and demographics among other criteria. For example this module permits comparison of historical performance of paid and scraped listings facilitates tracking of listing click throughs and expressions of interest for paid and scraped listings and tracks traffic redirection. It also tracks the number of new listings number of paid and scraped information data sets. Finally sales and marketing teams can use this tool to up sell premium listings based on prior history of similar such listings. An exemplary screen shot of an industry report is shown in . An exemplary quality manager report is shown in .

The Category Review module provides a mechanism wherein listing information that could not be automatically catalogued or classified in an appropriate category can be manually reviewed and either properly categorized or deleted. The category review module is explained in more detail below with reference to one particular implemented embodiment of the PALM system in the field of job search and listing information management. However the category review principles described below are applicable to any system in which data listings are systematically obtained reviewed and categorized in accordance with predetermined and dynamically determined criteria. Exemplary automatic categorization techniques developed and assigned to the assignee of the present disclosure are described in U.S. patent application Ser. No. 10 920 588 filed Aug. 17 2004 and entitled Automatic Product Categorization.

Search bank synchronizer module takes cleansed and categorized data sets from the database formats it properly and then populates a copy to an appropriate one of the search banks . Similarly paid search bank synchronizer takes cleansed and categorized data sets originating from paying listing customers formats the data sets properly for search and populates the search bank with a copy to be available for searching. Such paid listings are preferably given a higher level of visibility to a user when the data set meets a user s search criteria and thus is displayed to the end user .

The configuration generator module analyzes the volume of data being handled by the system and generates the configuration parameters for each of the available PALM listing manager modules as well as determining the number and size of the PALM task manager schedulers to be allocated to each PALM listing manager module . Based on listing manager module availability volume of data being sent into the system the results of scraping operations as further described below and administrative input an administrative operator tells the configuration manager module the information necessary to optimize and levelize data throughput in the system .

The PALM system includes a number of from 1 to n PALM listing manager modules as determined by hardware availability and the configuration manager module . An exemplary one of the PALM listing managers is shown in . The PALM listing manager modules have access to and utilize data in the overall PALM system database which includes PALM metadata store staging database and cooked database along with local databases associated with each input into the PALM system . Generally administrative information is relegated to the metadata store . The staging database is a temporary database used during initial data processing. Once the initial processing has been completed the processed data is stored in the cooked database .

Each listing manager module preferably includes a master task scheduler that manages and schedules a series of tasks that must be performed on each data set that comes into the system from external sources such as sites and the customer self service block . The tasks controlled by the master task scheduler include but are not limited to a Scraping Manager module a Data Source Adapter task a data splitter task a Data Cleanser task a Data De duping task an Automatic Categorization Engine task a Rule Based Quality Engine task and a Business Rules task .

The Scraping Manager module has overall control and management of tools that scrape or obtain listing information from external sites . There are two general types of scraping tools utilized a site specific scraping tool such as Kelkoo originally developed by Kelkoo Inc. now a subsidiary of Yahoo Inc. and a URL crawler engine such as Caf Kelsa scraping engine also developed by Yahoo Inc. The crawler engine starts with a seed URL and ferrets through each and every link it encounters and thus may lead to locations and information far removed from the original address. The scraping manager module coordinates operation of these scraping tools with the agent site management module through the master task scheduler to ensure that sites scraped and crawled are not overwhelmed with activity but are frequently visited to ensure current listing information is handled.

The Data Source Adapter task module takes data from different data sources stored in the local databases as data sets are received from various inputs into the system and converts the data sets of different types into normalized data sets all of one normalized type. For example data sets may be fed into the system as text file XML HTML or RSS data feeds. These different types of data sets must be normalized before further processing. The Data Source Adapter task module ensures that all data sets are of a common normalized type.

Referring now to each PALM master task scheduler schedules and controls a series of tasks some of which manage multiple task threads. The master task scheduler schedules the data cleanser task manager the data de duping task manager the categorization task manager the quality engine task manager and the business rules task manager each of which may manage n threads of tasks.

The Data Splitter task divides chunks of data sets into different groupings of roughly similar character so that data sets having similar attributes will be processed by the same task sequence threads. This task determines the assignment of data sets to different threads. The data split task first detects if there is a configuration change in the number of available PALM master task schedulers . If there is a change the staged data from the scraping farms and other sources is rehashed into new groupings. If there has been no change then only the newly added data sets are evaluated. The data split task splits listing data based on a hash of the ASCII of predetermined fields such as a job title company and job state in the case of a job listing. This hash specifically is ascii jobtitle ascii jobcompany ascii jobstate NUM CK RUNNERS where NUM CK RUNNERS is the number of threads available to the PALM master task scheduler as determined by the configuration manager . The function of the data split task is to use a uniform hash function that will always split the same data sets into the same bucket such that the same data records preferably get processed by the same thread.

The Data Cleanser task manager controls examination of each data set or record in a thread and removal of formatting such that the listing information in each scraped listing has the same format and content structure. Specifically this task controls stripping all HTML tags from the data fields validating names and puts in appropriate codes such as a 2 letter state code for domestic US listings in addresses and location data. For international listings it puts in the appropriate international location abbreviation provinces territories . Each of the threads in this task module also performs operational checks of such fields in the data listings such as each URL to ensure that it starts with either http or https and strips profanity words validates date fields checks for invalid characters in each field e.g. all numbers in a city field. Finally each thread preferably provides correct basic punctuation operations e.g. ensuring that the first letter in a sentence is capitalized and two spaces to start each new sentence.

The Data De duping task manager manages and schedules multi threaded tasks of taking the data record or listing data set from the data cleanser manager module and comparing the data set to records in existing databases and to ensure that the PALM system is not duplicating data already received examined and stored. Any data sets that when compared to existing database content are flagged as duplicates and are removed from the database . Hence the term de duplicating or de duping. 

The Categorization task manager module manages threads that automatically perform operations to determine categories that a particular listing data set may belong to. For example a home for sale listing in Mobile Ala. might be categorized according to its location size form whether single family or duplex etc. A job data set may be categorized by field hours education location etc. Thus the categorization engine task manager controls and manages the operations necessary to automatically categorize the subject listing data sets according to predetermined categories. This task manager is multi threaded and coordinates up to n instances of category determination concurrently. This categorization task preferably also includes determination of confidence levels for the categorization determined. Exemplary categorization techniques are disclosed in U.S. patent application Ser. No. 10 920 588 filed Aug. 17 2004 and the related applications described therein all of which are assigned to Yahoo Inc.

The Rule Based Quality Engine task manager module provides a sequence of rules to which each listing data set is scrutinized by to ensure that the data set meets certain criteria and contains certain minimum levels of detailed information. Such criteria may include for example a street address for a home real estate listing or a job title for a job listing or a city location for either such implementation. One exemplary embodiment of this module is explained in further detail below in reference to job search implementation of the system . The quality engine task manager schedules two basic thread sequences a URL link check thread followed by a data validation thread . These checks and verify that the URL is in fact a currently valid URL and also perform a word matching routine between database record words and a web page downloaded from the URL to ensure that the listing descriptions match. The Data validation thread randomly selects words from the data set and matches them to the downloaded web page and preferably chooses words that have greater than 5 characters in them. If the data set does not match the downloaded web page an error flag is set. A more detailed example of this task is explained below with reference to a job searching implementation of the PALM system with reference to below.

The Business Rules Task module provides businesses the ability to apply discrimination rules to display or not display listings filter the listings show partial listings based on industry location or to completely block the site s data for display even though the data may have been crawled previously. For example in a job seeking application for the Boston area one may select display of only 10 of the scraped or crawled data for the health industry during a predetermined time period. When a fresh listing arrives the business rules engine goes through the entire data set and may weed out or mark all listings accordingly to the predefined rules.

From the above description of the PALM system it should be clear that the system is scalable multi threaded and distributive such that the functionality of the modules such as the plurality of modules may be carried out on different combinations of computing machines suitably operatively connected together so as to perform the described functions.

An overall architecture diagram of a job search system incorporating one embodiment of a PALM system shown in in accordance with an embodiment of this disclosure is shown in . The system can be thought of as having three sections an external input section a data handling section and an output handling section . Basically the data handling section reaches to the external input section for job data processes the data organizes and verifies validity of the data categorizes the job data and provides the data to the output section which may be accessed eventually by a job seeker via the Internet .

The external input section includes the job postings that may be accessed by the data handling section from such sources as corporate and company career sites and a number of other job boards . These corporate career sites and job boards currently consist of several thousand company career sites. An employer recruiter can also directly provide job listing information through the Internet to an employer recruiter interface application . Such recruiter interface application provide to the employer recruiter a user interface screen to input job information and submit an individual listing in the proper format to the data handling section .

A system gateway feed can communicate with a customer site and pull in job information in a predefined format that the customer site has previously stored for this purpose. The gateway feed permits a customer site to submit information and provides a system feed into the data handling section of the system . Alternatively a customer site may make job information available through web services . Here the system accesses the customer site via Simple Object Access Protocol SOAP to obtain job listing information. Another way job information is obtained from a customer site is through RSS 214. RSS an abbreviation of Really Simple Syndication is a lightweight XML format designed for sharing news blogs product data and many other types of web content. RSS has evolved into a popular means of sharing content between sites including the BBC Yahoo CNET CNN Disney Forbes Motley Fool Wired Red Herring and many more. Job information may also be obtained from a customer site through a direct XML feed via the internet .

Also providing input to the data handling section is a scraping engine farm . The scraping engine farm has several scraping engines that typically use different scraping technologies and methodologies which may be developed as a matter of design choice but are preferably specifically directed in a preferred embodiment herein for searching over a global electronic network such as the Internet with each engine being optimized for either a particular type of scraping task or particular type or set of corporate sites. For example the Kelkoo scraping engine developed by Kelkoo Inc. in Europe now a subsidiary of Yahoo Inc. is optimized to thoroughly scour a predetermined known corporate site or listing site. The Kelkoo scraping engine is optimized to follow internal links within the site to specific internal locations to extract job information data sets. However it does not follow external links. The Caf Kelsa Scraping engine farm developed by Yahoo Inc. and described in U.S. patent application Ser. No. 11 064 278 filed Feb. 22 2005 and entitled Techniques for Crawling Dynamic Web Content is optimized to systematically examine a seed URL and follow every link within the site and follow every internal and external link that may be provided on that URL as well as links it finds on its crawl. 

The input section feeds data from these various sources and feeds to a staging database that is part of the overall database via a bus . The staging database is then accessed in the data handling section by the platform for advanced listing management PALM system . The PALM system also has several modules that provide input into the management system . For example a customer relationship manager CRM module and an other external application module may provide information and extract reports and other information that are uniquely available within the PALM system . Project management operations sales and marketing personnel can also provide input to and control for the PALM system via an intranet as will be described in more detail below.

The data output section comprises the job search web server client cluster and a number of data source modules to this cluster . The scraped search bank is one of these. An ad system premium listing module a paid search bank an overture system content match module and a link builder module are queried by the job search web server client cluster .

The ad system premium listing module organizes and provides the cluster with advertisements from specific employers or recruiters that have a paid premium account with the host of the system . These premium advertisements may be displayed to the job seeker in a special box bannered highlighted or otherwise set off from the other listings that may be presented to a job seeker in response to a particular search request.

The paid search bank module is a special search bank for which an employer member may access upon a fee payment to the host of the system . This paid search bank module identifies stores and tracks job listings from those job recruiter employer or corporations who pay a fee to ensure that their posted job listings receive a higher or emphasized placement on a user interface presented to the job seeker . Thus the paid postings are provided directly into the search bank by the member company via a member desktop or gateway . Paid search bank contains information provided by job listing entities that have paid a premium to the operator of the system described herein to push listings in connection with certain desired search categories provided by a user so that such search results are provided in a prominent position to the user via the user interface in exchange for a premium payment.

The Overture system content match module queries whether there are any advertisements in its database that match the job searcher s search criteria. These advertisements are previously stored in or linked to a paid database for use by the host of the system . Examples of such advertisements are shown in the search results user interface screen shot shown in .

The link builder module provides linkage cookies and addresses to link to other sources of jobs that match the search terms provided by the job seeker . In some instances in order for a job description to be viewed the job seeker must be passed to a particular website to see the listing. In such circumstances the site might require a particular security element such as a cookie etc. before the job information may be viewed. Accordingly link builder module provides the necessary interface characteristics in the case where a site needs a particular cookie or other identifier. The link builder module manages the process to build a URL which includes the necessary information required by the site such as for example a session cookie to access the job listing. The result of the link builder module may be provided to the job seeker in addition to the particular jobs of interest from his her search request.

With continued reference to the web server cluster acts as a gateway interface to a job seeker seeking to utilize the system described herein. The job seeker in order to initiate a search request on the system is preferably presented with a user interface similar to that shown in . The cluster then searches to obtain information from the system search banks and and presents it in an easy to use and efficient manner to the querying job seeker such as in the exemplary results interface shown in .

A job seeker entering a search request into a user interface such as that depicted in interfaces with the server cluster which in turn presents an aggregated result to the job seeker as shown in . Thus the user would see as described below premium listings through the provision of listings identified by the ad system premium listing module job search bank the banks and crawled jobs from bank .

Turning now to an exemplary screen shot of a user query result interface is shown. This user interface gives the job seeker an opportunity to review all of the job information that match his query. In addition it permits the job seeker to submit a different or more refined query. Display portion gives the user an opportunity to review all of the job information that would match a particular search criteria for example in a software developer position in Illinois. The job seeker may review all of the job information available as a result of the search for software developer positions or may review only those descriptions that have been updated in the past 24 hours 7 days or other preselected time period. Also the job seeker may structure his or her search by experience level location or other characteristic or subcategories within a job description.

The interface also displays result segments separated by multiple preferable result groupings. Thus the system may present a segment for premium listings obtained from ad system premium listing module which permits the host of the system to utilize the system as a revenue enhancing tool by providing the opportunity for business seeking employers to pay premium to have their job listings obtain a more prominent position in the result portion of the user interface presented to the job seeker .

The user interface also preferably includes a second subsection which presents results of the search from the paid job search bank . A third subsection presents non premium algorithmic search results which is a direct result of searching the scraped search bank . A fourth section provides more general paid links from the overture system content match module . Finally a number of advertisements may be displayed from a search of the ad system premium listing module .

Scraping involves the following components shown in the Kelkoo Sniffer a series of Agents to scrape web sites for jobs preferably a MySQL database such as staging database to store the scraped jobs and agent logs and Runner script managed by the Agent Site management module in the PALM system to launch the agents .

The following is a summary of how data flows preferably through the scraping farm in the system . At the beginning of the scraping cycle the job current table in the cooked database another portion of the overall database is truncated and its contents is copied to an archive table not shown . Archives of scraped jobs are preferably stored for a limited period only e.g. 7 days .

The Kelkoo Sniffer in the scraping engine is a Java program that is used to launch adapters a.k.a. agents . The scraping engines scrape the job boards via the Agents . Each agent preferably consists of three text files agent.info agent.props and agent.sql. A single agent is used to scrape a single web site. The agent files are stored in an agent specific directory. Then the Agents dump the scraped jobs into a job table note that there can be several job tables two of which are shown in . The Runners copy the job records from the job table s to the job current table . Components downstream from the runner such as the Quality Manager module and the Categorizer modules and receive copies of the job records and perform quality management and categorization operations on the records in the job current table which is preferably part of the cooked database . The results are then passed to the cooked database shown in .

The Kelkoo Sniffer search engine thinks about agents as virtual SQL tables. The schema of the virtual table is defined in the agent s sql file. The info file is a SELECT statement against the virtual table that the Sniffer search engine runs. The props file contains the scraping logic that is used to fill the virtual table. The scraping logic is a sequence of steps that are carried out by various filters. Filters are Java classes that make up the Adapter Development Kit ADK . Filters are executed sequentially and can read and write variables to a common context. There are filters to find a string or a pattern in an html page and save it manipulate variables of the context loop over a re occurring pattern and execute other filters in a loop go to a page identified by a URL and retrieve its content etc.

The output of an agent is a text file that contains SQL INSERT statements for each scraped job. The Sniffer search engine uses this data file to load the scraped job records into a MySQL table called job the actual table name is a configuration parameter . The Sniffer is configured via various command line parameters and an arbitrary number of property files that are passed in on the command line. The most important configuration parameters of the Sniffer search engine are Name of the MySQL database database user and password name of the table to dump the scraped records to and the Path to the agent request files and the directory that contains the agents .

The Sniffer search engine is preferably single threaded it loads and runs one agent at a time. After running an agent the Sniffer search engine inserts a record to the report table with information about the time of the run the name and path of the agent the number of scraped records jobs and possible errors.

The agent files are stored in a CVS repository. The version of the agent that has passed QA is tagged with a special CVS tag. This scheme allows agent developers testers and the production system to work on the same tree yet to avoid running un tested agents in production.

The agent runner is a Perl script that is developed for the system . The Runner requires that the agent files be available on the local file system. Before the Runner is started the local CVS tree is synced to the production tag to download all the agent files that should be run. The runner performs the following steps 

The PALM system controls and manages throughput from the input section to the output section . Preferably once a day or at some other predetermined interval the one or more PALM listing managers are instructed to access data in the staging database process that data and update the search banks and in the output section . Since there may be a tremendous amount of data to be processed the PALM system typically involves a number of PALM listing managers each operating on data from the staging database relatively independently.

The system can incorporate a number of PALM listing manager modules all essentially operating independently and in parallel each working on data first designated in the data split task to that particular manager s runner number. The PALM listing managers receive configuration information from the configuration generator . The configuration generator designates runner numbers to the available PALM listing managers in the system .

Each PALM listing manager receives metadata from and stores metadata to the PALM metadata database which is preferably a part of the overall distributed database . This database is preferably shared as illustrated in . For example the PALM listing manager of machine takes input from for example the staging database performs Task A and generates task output . This Task A output for example is then input to Task B. At the same time this Task A output may be temporarily stored in the staging database . The output of Task B is also temporarily stored in the staging database or metadata database as appropriate for use by one of the other PALM listing managers in this example Machine . Machine pulls its needed input if not available in the staging database from the metadata database as needed to perform Task C. The output of task C can similarly be stored back in the database or for use of another listing manager s task. By utilizing this scheme to temporarily image data the several operating PALM listing managers do not necessarily need to wait in line for the other listing managers to complete their tasks. In this way overall processing throughput is enhanced.

Each PALM listing manager in the PALM system in the system has a master task scheduler that controls a stack of tasks to operate on the data that is scraped from various sites accessed through the Internet as well as job information data sets obtained from customer sites and other sources through the RSS feed the XML feed the web services SOAP and or the Employer Recruiter Application . In the embodiment shown in concerning employment listings and job searching applications each of these listing data sets no matter from where obtained are initially stored in the staging database . The PALM system operates on the data in the staging database and passes it to a cooked cleansed and categorized database using an intermediate PALM Meta Data store . Once the job listing data sets are cooked the data sets are passed to the output section specifically populating search banks and .

Basic process flow operations within each of n PALM listing managers each controlled by a PALM master task scheduler are shown in . Operational flow begins in initialization operation where the PALM system begins its processing cycle in accordance with a predetermined schedule. First the PALM system determines which listing manager will handle what tasks.

Once a particular PALM listing manager is designated by the configuration manager to handle a chunk or batch of data sets the PALM master task scheduler controls operations through . Then each respective individual task manager and will process data sets in parallel threads in the staging database as described below with reference to operations .

Control transfers to operation to begin allocation of data sets within the staging database to the available PALM task threads. This is a Data Split Task which is managed and performed in the Data Splitter task module . The output data of the Data Split Task is returned to the staging database in operation along with a runner number of the corresponding PALM Task thread to which it is assigned.

The data split task first detects if there is a configuration change in the number of available PALM task threads 1 n as determined by the configuration generator . If there is a change the staged data from the scraping farms and other sources is rehashed into new groupings. If there has been no change then only the newly added data sets are evaluated. The data split task splits listing data sets based on a hash of the ASCII of the job title company and job state fields. This hash specifically is ascii jobtitle ascii jobcompany ascii jobstate NUM CK RUNNERS where NUM CK RUNNERS is the number of PALM threads available as determined by the configuration generator . The function of the data split task is to use a uniform hash function that will always split the same jobs into the same bucket such that the same data records e.g. job listings get processed by the same threads.

Control then transfers to query operation . In query operation the query is made whether there is another entry in the staging database that does not have a PALM task thread assigned to it. If the answer is yes control transfers back to operation where the next listing data set is retrieved from the Staging database and examined. If the answer is no then there are no more data sets to be split and control transfers to operation .

Operations through are preferably performed by each of the PALM task managers preferably in parallel on each data set having a runner ID corresponding to the thread to which it is assigned in the particular PALM task manager .

In operation the master task scheduler pulls the first staging database entry with its runner ID number and performs a data cleansing task managed by data cleanser task manager . The data cleansing task manager thread pulls the complete data record from the staging database and removes all formatting such that each job listing data set is in the same format and content structure. Specifically the task strips all HTML tags from the data fields validates the United States state names and puts in a 2 letter state code. For international listings it puts in the appropriate international location abbreviation provinces territories . The cleansing task thread checks the URL to ensure that it starts with either http or https . Next this task strips all profanity words validates date fields checks for invalid characters in each field e.g. all numbers in a city field. This task also checks for a maximum number of words in a field. For example a city name cannot have 15 words in it. It also puts country names in 3 letter country code formats corrects spelling in fields such as the jobtitle description. Finally it provides correct basic punctuation e.g. first letter in a sentence is capitalized and two spaces to start each new sentence.

Once the Data Cleansing Task thread has been performed on a job listing data set in a thread the listing is returned to the staging database in operation . Control then transfers to query operation . In query operation the question is asked whether there is another data set with that PALM master task scheduler s runner ID. If so control passes back to operation and the next data set is retrieved and cleansed. If not control transfers to operation .

In operation a data set is retrieved from the staging database and sent to the listing level de duping task in the de duping module . The Listing Level De duping task is shown in . An exemplary set of table entries is shown in the following Staging table Cooked Data table and De Duped tables.

First the de duping task looks for Row in the Staging table 1 in the cooked data table 2. It is there. Therefore Row is ignored. The Row in the staging table is then compared to the cooked database to see if it is there. It is not. Therefore Row of the cooked data table 2 is added to the De duped cooked data table 3. Next the same process is repeated for each and every entry in the staging database. When this process of either ignoring or adding has been completed the cooked data table 2 rows associated with runner number are compared to the staging table 1 to determine if there are any runner rows in the staging database that are not in the cooked database table 2. In this example the third entry in the cooked table 2 is not in the staging table 1. Therefore this entry the general manager row is deleted. The result is that the de duped cooked database is regenerated and verified once per day or once per period as defined by the system administrator.

A more general diagram of the de duping task process is shown in . The process is called when the data split task and the cleansing task have been completed on the data set having the thread runner ID stored in the staging database. Control begins in operation where initialization of the de duping module is completed. Control then transfers to operation where the first row in the staging database is retrieved and examined against the row entries in the cooked database . Control transfers to query operation .

In query operation the query is made whether there is an identical row in the cooked database. If there is control transfers to operation the staging row being examined is deleted. Control transfers then back to operation where the next row in the staging database is retrieved and examined. If however the answer in query operation is no there is no identical row in the cooked Database then this row is added to the cooked database in operation . Control then transfers to query operation where the query is made whether there are any more staging database rows. If yes then control transfers back to operation where the next staging database row is retrieved and the process repeats. If no the last staging database row has been examined then control transfers to query operation .

In query operation the rows in the cooked database with the same runner ID are compared to the entries in the staging database. If there are any same runner ID rows in the cooked database that are NOT in the staging database then these rows are deleted from the Cooked Database. The reason for this is that if the staging database does not have the job listing then the listing must have been pulled by the employer or filled and thus removed from the bulletin board or career listings and thus is no longer a valid job opportunity to a job seeker and thus of no use in this employment opportunity system. On the other hand if all the same runner ID rows in the cooked database are in the staging database all are current and control transfers to return operation .

Now that the PALM master task scheduler has seen the data set through the split task the cleansing task and the De duping task and the cooked database is now de duped for the particular data set each new entry into the content of the cooked database is submitted to the categorization task and the quality manager tasks . The categorization task is described below with reference to . The quality manager tasks are described with reference to .

Scraped jobs obtained through the scraping engines do not have category assignments such as Accounting Banking Engineering medical dental etc. In order for us to support the browse by category feature that jobseekers are most familiar with many human categorizers would need to spend a great deal of time to manually classify jobs as they are scraped. However this has substantial drawbacks. It is a very time consuming process. By the time the jobs are manually classified they may be outdated already. Such a process requires a lot of human resources. Further different categorizers may not categorize in the same consistent manner. For this reason the PALM system includes an automatic Job Categorization System shown in . This system is capable of categorizing a job in a fraction of a second. It is substantially faster than human categorizer and it is consistent.

This Job Categorization System contains several modules. A job categorization Job Cat Service module which carries out the actual categorization routine. The Job Categorization engine module described with that manages communication between a Job current table in the cooked database a manual category database and the Job Category Cat Service module . A Categorization Training Process performed by the category review module is used to enhance and or maintain the accuracy level of the Job Cat Service . This categorization training process involves the use of the job categorization manual review interface module and categorization experts that have access via intranet shown in .

As described above the jobs scraped are added to a MySQL job current table in the cooked database once they have cleansed and de duped. Then the Job Categorization Process will take each job from the job current table and send it through the job categorization control process module to the Job Cat Service module to get a category and confidence assignment. Then the scraped job is sent back to the categorization control process module and returned to the job current table . However if a job falls below a predetermined confidence threshold it will be flagged i.e. a flag set and when it passes through the categorization control process module a copy is also sent to the mancat database for manual review via the manual review interface module . The results of the manual review process performed in review module are then used by the Categorization Training Process to tune a new Job Cat Service value to replace the old one. The result of classification is written back to job current table and sometimes the mancat table . The Manual review module provides a UI to review both jobs in job current and mancat tables.

Query operation asks whether there is a matching URL existing in the mancat table for the latest particular job description. If there is then control transfers to operation . If not the job is a new job and control transfers to operation .

In operation a string compare routine is performed on the last job with the same URL. Control then transfers to query operation . Query operation asks whether the listing in the mancat table is the same as the current job being examined. If the job string compare is equal then the answer is yes and control transfers to operation since it appears that the job is the same job. On the other hand if the answer is no the job is new and control again transfers to operation .

Query operation asks whether the dcp cat matches the man cat of the latest job with the same URL. If the answer is yes then the man cat and dcp cat are set equal and the dcp cat confidence is set equal to 1. The job parameters back to the job current table and control transfers to query operation . Query operation asks whether there are more scraped jobs in the job current table to be categorized. If not control transfers to return operation . If there are more scraped jobs to be categorized control passes back to operation and the job parameters for the next job are retrieved and formatted.

Returning to query operation if the URL does not exist in the mancat table then control transfers to operation . In operation the Dcp cat and dcp confidence are set the confidence value is checked against the threshold that has been predetermined and if the threshold is greater than the confidence value the review flag is set equal to 1. The job parameters are then passed to the job current table and again control passes to query operation .

Returning to query operation if the current jog has a URL in the mancat table the job is the same as the last job with the same URL but the dcp cat and an cat of the latest job do not match then something may be wrong or missing and the job parameters are passed to both operations and . Operation sets the dcp cat the dcp confidence values sets the expert review flag 1 and feeds this data to the Job current table . Operation sets the expert review flag 1 and sends a copy of this job s parameters to the mancat database so that manual review will be performed. In parallel control again passes to the query operation as described above.

Thus for each job the Job Categorization Control Process take job attributes from the job current table formats them and sends them over to Job Cat Service managed by a well known public domain routine called Apache method POST gets back a category and confidence score goes through a chain of decision questions and writes results back to the tables.

The Job Cat Service also provides a web UI that allows administrators and system operators to type in a job at least the job description and submit the job to the Job Cat Service for categorization separately from the normal operation of the system . Such an exemplary user interface is shown in .

The Job Cat Service module depends on Apache a well known web server for hosting training process shown in . The Job Cat Service contains a binary package that is a shared library of PHP extensions and also includes a Categorization library. Building the Job Cat Service first requires a set of basic definitions i.e. a taxonomy of job categories and associated unique ID numbers. An exemplary set is shown in Table 1 below.

An exemplary table of training job descriptions training data is associated with each of the categories in Table 1. This set of descriptions plus the content of the mancat database is used to teach the Service to recognize classifications from the provided job description parameters that are preclassified. An example of this table is shown in Table 2 below.

For new training sessions it is preferable to use both jobs from this table and those in the mancat table. As more and more manual reviewed jobs become available it is preferable to eventually drop the original training set from the read only database.

In a preferred embodiment the columns of this table 2 and the mancat table are different and this difference will remain and the script that creates the training file will do all necessary mappings. The training process consists of several PEARL scripts. A create training file.pl script takes jobs from both the mancat table and a train data table and writes out a file containing all jobs in a DCP accepted format to generate the merged training data . A train hj dcp.pl script is used to tune a few of the most useful parameters for classification. Each of the configurations specified will leave an output directory containing all the parameters that are needed to build a Job Cat Service data package and a log file. A parse training log.pl script reads each of the log files generated by the train hj dcp.pl script and generates a report on accuracy for each configuration. An archive training results.pl script is used to archive the training results for that configuration after a configuration is used for deployment.

The training process is basically a manual process that draws from the training data the taxonomy and sets of rules and schema . Various dictionaries and tuning parameters may also be utilized. The results involve optimization of new classifier parameters with the results being provided into the job categorization service as shown in . Since the training process is mostly manual it is preferable to train on a few parameters manually check the results e.g. detail pages of classification term weights etc and change some of the rules and dictionaries by hand and repeat the process with different configurations in order to find the optimal settings for deployment. When such an optimal configuration is achieved the new classifier parameters are passed to the Job Categorization Service . Once the Job Categorization Service is built up and running scraped jobs can then be processed as described above.

The following discussion provides a detailed explanation of the listing categorization process of the present disclosure using an exemplary job categorization process.

In an exemplary Lexical Analysis three text fields are processed 1 title 2 Job description and 3 Company category. A lexer lexical analysis module may apply the following processing steps 

Each unique string resulting from these steps constitutes a unique token. Certain tokens are added additional weight and is tracked in a weight.dict file. Here is a job specific sample of the file 

The feature corresponding to each text token may simply be the per document count of the occurrences of the token in the document. Each token instance may be assigned a weight that depends on the field from which it was extracted and may depend on the position of the feature within the field. Specifically one may multiply the token counts from the Title and Company Category by two before adding them to the overall count. The Description term counts may be taken unaltered. A position dependent rule one may implement is to weight the last word of the title more heavily in an attempt to find the head noun in a noun phrase i.e. the title which may lead to a slight increase in accuracy.

One may also use token feature clustering e.g. distributional clustering where several tokens members of a single cluster are all treated as the same token.

As discussed above the job records may contain fields that aren t free text. They may include 1 Company id and 2 Salary and others. Embodiments in which these features are used are described in the section entitled Category Refinement.

Feature selection may be performed by ranking individual features x by their mutual information I C X with the category variable 

where the x sum is over x 0 and x 1 and the c sum is over all the categories classes . The probability estimates for p c x are obtained by simple counting and the binary variable x indicates the presence x 1 or absence x 0 of the associated term. In terms of the terminology this is strictly speaking a different feature than the associated term count used in a classifier Na ve Bayes for example for actual categorization. This may be beneficial for numerical reasons. The alternative is to sum over all possible term counts from 0 to infinity which may cause problems due to the potential sparseness of the associated probability estimates.

The ranked list is processed in order of decreasing mutual information. For each number m of features a classifier is trained using the first m from the list and its accuracy is measured using cross validation. Features are added until the accuracy measured this way starts to decrease.

The number of features may also be controlled by setting a frequency threshold limit. Features whose frequency is less than the threshold limit may be eliminated. There may be two or three combinations of the threshold and number of features that result in the best accuracy figures as reported by cross validation.

According to one aspect of the disclosure a method for categorizing a job offering in multiple passes is provided. The first step is performing a first categorization to associate the job with a first category. If the first job category is in a set of co refinable job categories performing a second categorization within the set of co refinable job categories to associate the job offering with a second job category. Further the second job is in the set of co refinable job categories the first job is in a first set of jobs and the set of co refinable job categories is a proper subset of the first set of jobs. A set of co refinable job categories is defined as a set of job categories that have by any method been determined to have relatively high likelihood of being confused one for another or having one job category in the set being chosen in place of another job category in the set.

Those categories that are selected for use in the second pass the co refinable job categories are selected based on the category that you are looking at. For example the co refinable job categories may be determined based on likelihood of confusing the other categories for the particular category. One method of determining whether two categories are often confused is to perform a manual categorization of a set of jobs. The manual categorization will be treated as correct the gold standard. Then perform an automatic categorization of the same set of jobs. Graph the results in a matrix where one axis represents the categories chosen by the gold standard in this case manual categorizer and the other axis represents the categories chosen by the automatic categorizer. Excluding all of the cells in the matrix where the manual and automatic categorization chose the same category possibly along the diagonal depending on implementation the cells with the highest probability represent the categories that are most likely to be confused. The set of co refinable product categories may then be based on the cells that are most likely to be confused and may in fact contain multiple distinct sets of co refinable product categories and these sets may each contain a different number of categories than the other.

Herein is provided one example of category refinement. The techniques described herein are not limited to such an embodiment. Consider an automatic classifier built based on a Na ve Bayes categorizer. Example Na ve Bayes categorizers are described in David D. Lewis Na ve Bayes at forty The independence assumption in information retrieval in Claire N edellec and C eline Rouveirol editors Proceedings of ECML 98 10th European Conference on Machine Learning number 1398 pages 4 15 Chemnitz D E 1998 herein referred to as Lewis . The categorizer may have two or more levels of categorization. At the top root level one may perform a flat categorization where each category is described by a single multinomial distribution as described in the section entitled Na ve Bayes. One may use a mixture of multinomials to model the term probability distribution for certain categories. Strictly speaking this violates the Na ve Bayes conditional independence assumption but one may simply assume that certain categories may be further decomposed into other categories that do follow the assumption but it is unknown a priori what they are.

Then a second categorization is performed on certain categories that are in a set of co refinable categories. The categories may be those in the list below which has been divided into three confusion groups . The top level nodes in each group have classifiers that perform a second classification into just the categories in the confusion group.

For Company id one may use the simplest possible model multi Bernoulli. That is one may have a different probability value estimate for every category company pair. That is one may have estimates for the set of values p c m where m represents company id. These values are denoted by c m.

Describing Salary statistics with a log normal distribution Salary may be approximately distributed according to a Log Normal distribution which means simply that log price is distributed according to a simple Normal Gaussian distribution. Let z log price . Resulting in the following where and are the mean and standard deviation of a Normal distribution 

In machine learning and pattern classification the objects to be categorized or classified are represented by what are referred to as feature vectors x which contain the information that is used to determine the most likely category that the document belongs to. The so called Na ve Bayes classifier for documents assumes something called the bag of words model see Lewis . This means that the particular sequence of words is ignored and only their counts are used. This limitation is circumvented somewhat when phrases are detected as tokens and treated as if they were individual terms. In the Na ve Bayes case the feature vectors may be of the form x k k . . . km where kdenotes the number of occurrences in the document to be categorized of the iterm and m is the total number of terms in the lexicon which in this case refers to the set of terms being used to do the categorization after stopword removal and so on.

The Bayes Classifier may use probability models in the following way Given a feature vector x compute the conditional probability that the document belongs to category c given the document s feature vector x. To perform categorization choose the value c for c i.e. the assignment of the document to a category that maximizes p c x . Mathematically this may be expressed as max 

Because one may only interested in the value of c and not the specific value of p c x one may ignore the marginal probability p x which only depends on x and not c max 5 

To carry out the actual categorization process specific formulas are needed for p c and p x c . The Na ve Bayes bag of words model may use a multinomial distribution for this. That is 

This formula includes the following shorthand notation for something called a multinomial coefficient 

Because this multinomial coefficient is a function of only the document and not the class it too may be ignored in the categorization process. The parameters are often referred to as Bernoulli parameters and may be estimated from the training data. This . . . is a shorthand set notation. For example really denotes i 1 2 . . . m i.e. the complete set of these parameter values.

For each category one may have values for p x c and p c and each of these may have their own estimated parameter values. The Bernoulli parameter for term i within category c is represented and may be estimated by the following 

To perform the categorization described by 5 one may need estimates for the marginal class probabilities p c . One may represent these estimates by and use a form of Laplace s rule for them also 

An implementation of the categorization described by 5 may be expressed in terms of this as follows max 11 

It may be beneficial to use the logarithm of 10 as a discriminant function for both numerical and computational reasons. Thus one may have 

The screen shot of the exemplary user interface is presented to an administrator operator or categorization expert through the intranet using a web browser. The interface provides three different modes via a pull down menu as shown. The all categories mode lists all categories and their corresponding confidence values sorted in descending order by confidence. The Detail Statistics mode shows the details on why a particular category is chosen. This mode is useful for an operator who tunes the system . The Best Category mode shows only the top category for the job and its confidence. This is equal to the first result shown in All Categories mode except here we show the category ID number not a string. This mode is intended for automatic classification of jobs in the database where the category ID number is preferred over the category name.

An operational flow diagram of the job categorization manual review process that takes place in the job categorization manual review module is shown in . Operational flow begins when an administrative operator or a categorization expert logs in via the PALM administration portal in operation . When the administrator logs in he or she is presented in operation with a user interface as shown in . This user interface permits the administrator or expert reviewer choices of job category company and selection of a type of review to conduct. Control then transfers to operation . In operation a first next job description is retrieved from the mancat database or the job current file in the cooked database depending on the administrator s prior selections in operation . The administrator is presented with a user interface such as the exemplary interface shown in .

This user interface displays the first next job description along with the category confidence levels determined for each category. In this example the job is a post doc position at IBM Corp. The confidence levels are zero for all but Engineering Architecture and Pharmaceutical Biotech and none of the levels match 100 . This position has been categorized as Engineering Architecture but the confidence level is only 0.657 so it was flagged for manual review.

Referring back to when the job description is retrieved in operation control transfers to operation where the administrator analyzes the categorization based on the full job description. The administrator then has three choices of action. First he can invalidate the job in query operation . Second he can get more job details in query operation by clicking on the job URL to enhance his review. Third he can update a category definition or insert a new category in query operation . If his decision is to invalidate the job in query operation then control transfers to operation where the job is removed from the database and from the mancat database . Control then transfers to query operation . Query operation asks whether there is another job description in the queue of the mancat database or job current table that has its expert review flag 1 set. If so control transfers back to operation where the next job is retrieved for review.

However if the decision in operation is not to invalidate the job control resets the expert review flag 0 returns the job to the job current table and control transfers to query operation . If the choice in operation is to get more job details control transfers to operation where the details are retrieved and control transfers back again to operation . If the administrator then chooses not to get more details the job description record is again returned to the job current table after resetting the expert review flag 0 and control passes again to query operation . If the choice in operation is to update the category in query operation then control passes to operation .

In operation the category of the job description is changed or a new one added and saved. The expert review flag is set 0 and the job description is then transferred to the job current table and control transfers to query operation . If there are no more job descriptions with expert review flags set 1 control transfers to return operation and the review session is complete.

Additionally it is preferable that a job categorization control process module reviews periodically the information in the cooked database in order to accurately categorize each job listing. It is important that a job listing be placed in the proper job category such as for example information technology healthcare accounting etc. The job categorization control process module preferably is automated or through a manual review interface module may be augmented by input from categorization experts which are preferably human. However the function of the experts may alternatively is as the case with listing reviewer entities discussed earlier be automated routines in the future as such systems become more sophisticated. The job categorization control process module is preferably automated while the manual review process module provides a check on quality thus providing a high degree of accuracy in job categorization. The results of this categorization process are stored in mancat database which is a contraction name for the manual categorization database.

In the Quality Manager task shown in each entry row in the cooked database is retrieved and evaluated on two levels URL validation and content validation. In the URL validation the task first checks and verifies that the links to http and https resources are valid. In essence the system accesses the URL to verify the link connections. The second operation involves checking whether there are any warning response messages. If so the listing may be flagged for manual review. The URL Validation operation also detects whether any of the links have been redirected or otherwise modified and determines support for session cookies. In the Content Validation part of the quality Manager task checks are made for inconsistencies in the data. For example various checks such as performing specified rules verifying the job description conducting a matching algorithm on the description and verifying match between city state and country. Finally the quality manager process may be accomplished with n separate threads that operate concurrently. Each quality manager task thread operational flow is shown in .

The rule based quality engine task module performs a series of operations on each data set processed through the sequence shown in . The configuration manager determines how many listing manager modules are available. In addition within each listing manager module the quality manager task may manage N quality manager task threads and . Specifically at operation the operational sequences are called by the quality engine module task managers . Each quality manager task manager may have control of n threads . Each sequence begins in operation where the required registers are initialized. Control then transfers to operation . In operation depending on how many quality engine manager threads are available and how many threads are assigned as determined by the configuration generator the data sets in the cooked database to be examined for quality are retrieved. Control then transfers to operation . Here the data sets are split into n partitions. Thus the number of data sets in one partition corresponds to the number of threads available within the partition. Control then transfers to operation . The following operations are performed for each data set in each of the partitions in parallel.

In operation each data set is compared against a set of document rules. For example these rules include determination whether the job description text field has at least 5 or more words in it whether the job title field is filled i.e. not null whether the job company name field is filled i.e. not null and whether the job location field is filled i.e. not null. If any of these fields are null or contrary to the rule the data set fails the document rules and will not be indexed. Control then passes to query operation . In query operation the question is asked whether the data set passed the rules tests. If the answer is yes control transfers to operation . If the answer is no control transfers to operation where an error flag is set and then to operation where a record of the missing data is sent to the reporting module . Control then transfers to operation .

In operation the location fields of the data set are checked to verify that the location of the city corresponds to the state listed in the state field and country corresponds accordingly. Control then transfers to operation . Query operation asks whether the data set passed each of the location verification tests. If the answer is yes control transfers directly to operation . If the answer is no again an error flag is set in operation and a location error report is sent to the report module in operation . Control then transfers to operation .

In operation the content of each of the fields in the data set being examined is compared to a set of profane or impermissible words to determine whether there is any profanity or otherwise unacceptable words in the data set. Control then transfers to query operation which asks whether any profane or unacceptable language was found. If there were unacceptable words found control transfers to operation where an error flag is set and to operation where an error report of the unacceptable language is sent to the report module . On the other hand if no profanity was found control transfers directly to operation .

In operation any preliminary URL address is accessed and is checked to determine whether any error messages are generated upon access. In addition if there are any session cookies required these are recorded in this operation. Pre URL addresses and cookie information may be present or required in some sites requiring access information for example prior to a user getting to the desired URL. This operation verifies that any preliminary URL information in the data set is current and correct. If there are any error messages these are noted. Control then transfers to query operation . Query operation asks whether there were any error messages received when the pre URL address was called. If the answer is no control transfers directly to operation . If the answer is yes there were errors again an error flag is set in operation and an error report sent to the reports module in operation . Control then transfers to operation .

In operation the final URL address is called and any session cookies required are recorded. At this time any error messages are noted. This operation is important in order to verify that the data set still remains current in the database. Especially in the case of job posting data sets the job may have been filled just the day before. In such a case the posting may be cleared yet the database still thinks the job is current. This operation attempts to catch such recent change situations and accommodate such activity. In many instances this operation is successful and helps ensure that the database is maintained current. Control then transfers to query operation where the question is asked whether there were any error messages such as would indicate that the job had been pulled for example. If no error messages were received control transfers to operation . However if error messages were received control transfers to operation where an error flag is set and then to operation where an error report is sent to the reporting module . Control then passes to return operation since the URL is invalid if errors were received and the data set will not be indexed and returned to the cooked database .

In operation the web page at the URL is downloaded for examination. Control then transfers to operation where the web page is cleansed of HTML data as was previously done in the data cleanser module . Control then transfers to operation . In operation the data set content is matched word for word to the web page content. This operation verifies that the data set correctly reflects the web page which is another verification mechanism that the listing is current. Control then transfers to query operation . Query operation asks whether there were any errors in the matching operation . If there were errors then the data set is corrupt or the job posting is somehow different and therefore it will be returned to the cooked database and not indexed for forwarding to the search bank . Accordingly the answer is yes so control transfers to operation where an error flag is set then to operation where an error report is sent to the reporting module and then control passes to return operation .

In each case where an error flag is set operations the set flag will prevent the data set from being indexed and returned to the cooked database for forwarding to a search bank. However a copy of the data set will be made available in the cooked database for an administrator to examine in the quality review module .

Return operation returns control to task at operation which then transfers control to operation where another series of rules based tasks are performed. Overall control then returns in operation .

Although functional components modules software elements hardware elements and features and functions described herein maybe depicted or described as being fixed in software or hardware or otherwise it will be recognized by persons of skill in the art that the features and functions described herein maybe implemented in various software hardware and or firmware combinations and that the functions described herein may be distributed into various components or subcomponents on the network and are not fixed to any one particular component as described herein. Thus the databases described may be separated unified federated or otherwise structured to best suit the preferences of the implementer of the features and functions described herein. Also the functions described herein as preferably being performed manually maybe performed manually or maybe divided into subtasks which may be automated and ultimately performed by intelligent subsystems which mimic human operator interaction such as artificial intelligence systems which maybe trained by human operations and ultimately function autonomously. Further features functions and technical specifications are found in the attached descriptions further below as well as the figures contained therein.

While the apparatus and method have been described in terms of what are presently considered to be the most practical and preferred embodiments it is to be understood that the disclosure need not be limited to the disclosed embodiments. It is intended to cover various modifications and similar arrangements included within the spirit and scope of the claims the scope of which should be accorded the broadest interpretation so as to encompass all such modifications and similar structures. The present disclosure includes any and all embodiments of the following claims. All patents patent applications and printed publications described referred to or discussed herein are hereby incorporated by reference herein in their entirety.

