---

title: Computer interaction based upon a currently active input device
abstract: Described is a computer-implemented system and method that dynamically detects which input device (e.g., pen or mouse) is currently in use, and based on the device, varies a program's user interaction model to better optimize the user's ability to interact with the program via that input device. A tablet input subsystem receives pen and touch data, and also obtains keyboard and mouse data. The subsystem analyzes the data and determines which input device is currently active. The active device is mapped to an interaction model, whereby different user interface appearances, behaviors and the like may be presented to the user to facilitate improved interaction. For example, a program may change the size of user interface elements to enable the user to more accurately scroll and make selections. Timing, tolerances and thresholds may change. Pen hovering can become active, and interaction events received at the same location can be handled differently.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07802202&OS=07802202&RS=07802202
owner: Microsoft Corporation
number: 07802202
owner_city: Redmond
owner_country: US
publication_date: 20050317
---
The present invention relates generally to computing devices and more particularly to user interaction with computing devices.

Contemporary computing devices allow users to input information in a number of ways including by keyboard by one or more types of pointing devices and by dedicated hardware buttons typically on portable devices . Voice data and eye movement data e.g. gaze detection are other types of data users can input into computer systems via microphones and cameras respectively.

With respect to pointing devices a computer system may receive user input from a mouse and or a touchpad and if configured with a digitizer by sensing stylus pen proximity and touch e.g. entered via a pen or a finger. Some personal computers such as tablet based personal computers have the digitizer built into the display screen. This is highly beneficial for many users as such a computing device can be operated with or without a keyboard.

However each input device has unique physical and ergonomic characteristics that result in various challenges. One important set of challenges arises because of the way various input devices interact with the content displayed on the screen. For example a lot of user interface mechanisms such as drop down menus scroll bars and so forth were modeled for interaction via a mouse which is typically placed on the desk next to a computer monitor. However a pen is used directly on the display itself. Thus when using the pen a user s hand often blocks a portion of the screen making it difficult to interact with the computer. As one example many dropdown menus are very difficult for right handed users to use with a pen because they drop down so as to be obscured by the user s right hand. Scroll bars can be difficult to manipulate for left handed users. In sum these problems as well as many other similar and related problems are experienced by computer users as a result of the variety of alternative input devices.

Briefly the present invention provides a system and method that detects the type of input device and modifies a data interaction model e.g. some of the user interface elements and or behavior based on the type of input device that is currently active so as to facilitate an improved user interaction experience. Possible input devices include one or more pointing devices pen and touch digitizers and or a mouse as well as a keyboard hardware buttons a camera and a microphone. The interaction model may also be modified based on attributes of the user such as whether the user is left handed or right handed.

In one implementation a tablet input subsystem receives pen and touch data and also obtains keyboard and mouse data as well as possibly other input data. The tablet input subsystem or an associated component analyzes the data to determine which input device is the source and selects the most recently active input device as a currently active input device. The evaluation is performed dynamically during runtime because multiple input devices may be used by the same user on the same machine in a single session. Some minimum usage time may be required before changing from one currently active input device.

Data identifying the currently active input device is provided to a program which then maps a user interaction model to the currently active input device. In this manner a program changes its user interface behavior and or appearance based on the current device generally in some way that better enables the user to interact with the program e.g. more accurately scroll make selections and so forth.

By way of example when a pen is detected as the active input device for a right handed user dropdown menus may be displayed to the left of the pen selection. Scrollbars buttons icons and other selections e.g. on a menu may grow e.g. relative to their size when a mouse is the active device to facilitate more accurate touch or pen input depending on which is in use. The cursor button target size visual feedback color scheme timing tolerances and other threshold may change to facilitate interaction such as to provide easy targeting and actuating. Changes may occur e.g. to tolerances without visible audible or other tactile feedback.

Other specific examples include hover feedback that is when a pen is in use and is hovering over or near a user interface element. The user interface element may grow change color become surrounded by a box or other shape and or otherwise change to facilitate contact. A region may be magnified based on touch and or hover.

Still further examples include changing the cursor based on the type of input device e.g. from a mouse pointer to an image of a microphone for speech to a larger targeting cursor for touch and so forth. The cursor may fade or change to a default value when the pen is out of range. The current active input device detection may be used to change the timing tolerances and thresholds for click events and gestures.

Different actions may change based on the input device such as a right click being disabled for a pen device and or single taps instead of double taps allowing one tap program launching from a pen. Gestures may be enabled disabled and or have different meanings based on the device e.g. pen versus touch versus mouse. The icons or other interaction selections being offered may also change based upon the active input device e.g. a launch button may be enabled for a certain device and disabled at other times.

Input data received at the same location may be treated differently depending on the input device that is the source of that data e.g. ink from a pen scroll from a finger and content marking for selection from a mouse. Some of the options may be user configurable.

When receiving input from two or more input devices at the same time the conflict is resolved by selecting the most recently used input device as the currently active device possibly delaying any change until after a device has been used for some minimum time. Note however that in some cases using two input devices at the same time may yield a third user interaction e.g. if the user holds down a button while using the pen the receiving program may respond differently to the pen action. A changed state may automatically revert to a default state such as when the device that causes the change stops being used for some amount of time.

Other advantages will become apparent from the following detailed description when taken in conjunction with the drawings in which 

The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with the invention include but are not limited to personal computers server computers hand held or laptop devices tablet devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like such as PDAs personal digital assistants cell phones smart phones and digital cameras.

The invention may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures and so forth which perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of the computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

The computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by the computer and includes both volatile and nonvolatile media and removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can accessed by the computer . Communication media typically embodies computer readable instructions data structures program modules or other data. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of the any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers herein to illustrate that at a minimum they are different copies. A user may enter commands and information into the computer through input device such as a tablet electronic digitizer a microphone a keyboard and pointing device commonly referred to as mouse trackball or touch pad. Other input devices not shown may include a joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . The monitor may also be integrated with a touch screen panel or the like that can input digitized input such as handwriting into the computer system via an interface such as a touch screen interface . Note that currently active input device in accordance with various aspects of the present invention.

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

The present invention is primarily directed to user input data entered via various input devices wherein the underlying user interface interaction model is dependent on the type of input device currently in use so as to facilitate an improved user interaction experience. Although many of the examples herein describe pointing devices it will be appreciated that aspects of the present invention are applicable to virtually any type of input device s data such as keyboard entered data hardware button entered data camera entered data and microphone entered e.g. speech data. Thus although for purposes of simplicity the present invention will be primarily described with reference to pointing device input the present invention should not be limited in any way to any particular types of input devices and or by the examples used herein.

As will be understood numerous ways to implement the present invention are feasible and only some of the alternatives are described herein. For example many of the changes that may occur to the underlying user interface such as to change the appearance of what is being displayed in some way are only some examples of the many possible types of changes that can be readily accomplished. As such the present invention is not limited to any particular examples used herein but rather may be used various ways that provide benefits and advantages in computing in general.

Turning to there is shown an example architecture in which various hardware input human interface devices are shown that provide user input data such as a keyboard a mouse a pen digitizer a touch digitizer one or more buttons a camera and a microphone . Other input mechanisms are feasible such as a television remote control as represented in via the block . Each of these hardware devices connects through a suitable driver e.g. to an operating system level component. Note that for purposes of example each input device is shown as having its own driver however one driver may handle the input of more than one device.

As represented in the keyboard and mouse generated data are shown as being received at one operating system level subsystem referred to as a window input subsystem while pen and touch generated data are shown as being received at another operating system level subsystem referred to as a tablet input subsystem . The other input data are received at one or more suitable subsystems represented in by the block . However other architectures are feasible.

In general the tablet input subsystem receives the pen and touch generated data and also obtains any keyboard and mouse data received at the window input subsystem . To obtain the keyboard and mouse data the tablet input subsystem employs low level hooks to monitor the window input subsystem which in a Windows based operating system may be accomplished via well known application programming interface calls. Other types of input may be similarly monitored.

In general and as described below when the user uses one of the input devices that device becomes an active input device and an input mode controller may change the underlying user interaction model in some way based on the active input device. The input mode controller may be integrated into the operating system s shell user interface UI program into other operating system components or into application programs. For example the shell UI can change the appearance of icons when the mouse is the active input device relative to when the pen is the active input device. The change may be in accordance with preferences and settings including those provided by the user such as whether the user is left or right handed.

The input mode controller can then communicate input events to a program e.g. the shell component or application program having focus. The events include pointer and other input data that identifies the input device source. The program then maps the active input device to an interaction model of a set of interaction models.

As an already existing technology a human interface device HID standard differentiates the various devices and the tablet input subsystem receives HID data packets from each with information in the packets that identify the type of device that generated the data. This information along with mouse and or keyboard data received via the low level mouse hooks and possibly other input data can thus determine when an input device is active as well as which one is active. Note that the present invention is not limited to HID data packets and may work with any generalized input API provided the API allowed the originating device to be identified upon receiving input data. For example not all keyboards and mice work via the HID interface whereby the present invention may use an alternate input API e.g. operating system provided low level hooks to determine when input occurs from such devices.

In accordance with various aspects of the present invention the tablet input subsystem includes or is coupled to the input mode controller to control the underlying user interface interaction model in some appropriate way based upon which input device is currently active. The evaluation is performed dynamically during runtime because multiple input devices may be used by the same user on the same machine in a single session.

Although there are numerous ways to modify the interaction model and only some examples are shown herein in general the user interface is changed to better optimize the user interaction experience for the active input device. For example in one implementation only the user interface for the particular user interface element with which the user is interacting is changed e.g. when the pen becomes active the button directly underneath the pen may enlarge rather than every control on the page which remain normal size. However for some user interface elements this may not apply for example the cursor is always considered in use so whenever new input device becomes active the cursor is changed.

Thus in one example when a pen is detected as the active input device for a right handed user dropdown menus may be displayed to the left of the pen selection. In contrast if the mouse is being used the dropdown menus can be to the right as it is in most contemporary shell programs and application programs. This is generally represented in mouse and B pen where the user interface is dynamically altered at runtime to compensate for any hand blockage created when a pen is used. As can be seen in when the pen is detected as being used as the active input device and the settings or touch sensing indicate the user is right handed the input mode controller e.g. in the shell user interface program presents dropdown menus to the left so that they are not displayed under the user s hand.

Note the position has been altered for the pen thereby improving user interaction with the device however the basic user interface elements may remain the same. As a result the user need not learn an entirely new way of interacting with the machine. Although the operating system and the application programs may react differently depending on the input device used the primary end result may remain consistent to the user so as to avoid requiring the user to learn new interactive models for each input device. For example users know that a button is always invoked by actuating it in some way but each device may present the user with an optimized cursor button target size visual feedback color scheme timing threshold auditory feedback or additional user interface assistance to facilitate easy targeting and actuating of the button with the currently active device.

Other specific examples include hover feedback as generally represented in the different interaction models of . In when the mouse is used in a program window as indicated by the mouse pointer regular selection behavior is provided. In however when a pen is hovering over a selection the selection grows to facilitate contact therewith. Note that this is typically advantageous because a pen is generally less accurate for most users than a mouse. As can be readily appreciated scrollbars are another type of user interface element that can be enlarged when a pen is detected as the active input device which may be the state at all times while the pen is active or only when the pen is hovering on or near a scrollbar. The enlarging of the element may be even larger when touch is the active input device e.g. to facilitate contact by a finger. Speech input possibly along with proximity detection can greatly enlarge displayed output so that for example a user can communicate by speech with a computer from a relatively larger distance than typical and still see visible results.

Another example of altered feedback is represented in . In a user can touch an item once resulting in a magnified region corresponding to the area. The user thus knows at what location the computer is sensing or has just sensed the finger and if desired can then touch the item in the same general location to select it or touch the magnified area to select it e.g. within a reasonable amount of time such as while it remains visible. If not the desired area the user can touch a different location whereby a different selection can be highlighted or not touch anywhere for a timeout period whereby the magnified region will disappear until the touch digitizer next senses touch. Note that the region can be displayed to the extent it will fit generally to the left for right handed users and to the right for left handed users. Further note that instead of a double touch selection with touch such a magnified region can be used with a hovering pen whereby only a single tap is then needed to select.

Still further examples include changing the cursor based on the type of input device e.g. from a mouse pointer to an image of a microphone for speech to a larger targeting cursor for touch and so forth. The cursor may fade or change to a default value when the pen is out of range. Note that tolerances may be changed even without visible audible or other tactile feedback. Current active input device detection may be used to change the timing tolerances and thresholds for click events and gestures.

When a pen is in use right click may be disabled or may be activated by pen tap and hold a gesture e.g. a flick to the right or may be based on the barrel button state. Similarly a single pen tap on a selection may make a selection such as to launch an application program with a double click required when the mouse is active.

The interaction selections being offered may also change based upon the active input device e.g. a launch button may be shown for a special pen input surface whenever the pen is active and disabled hidden or grayed out at other times. Programs may also change modes and behaviors e.g. defaulting to ink mode when a pen is active and changing what pointer events mean e.g. in the same program in the same general data receiving state pen input inks touch input scrolls and mouse input marks a selection for cutting or copying. In a selection marking state the mouse may mark content for cutting or copying via a rectangle while the pen may mark content free form or via a lasso selection.

As can be readily appreciated at least some of the options including those exemplified above may be user configurable. Thus one user might want a single pen tap on an icon to launch the corresponding program while another user may prefer a double pen tap or a gesture.

It is possible to receive input from two or more input devices at the same time. In general the conflict as to which input device is the active one is resolved by selecting the most recently used input device. However a time window may be employed before switching from one to another to ensure that the user has actually switched. In this way inadvertently bumping the mouse when entering pen input for example does not cause a change.

A changed state may automatically revert to a default state. For example when the pen is out of range of the digitizer the computing device can enter a user interface model better optimized for mouse or touch input. If the pen comes back into range the state may be changed again to better optimize for pen input.

As can be seen from the foregoing detailed description there is provided a method and system that dynamically adapts a user interaction model during runtime for an active input device. Various appearances timing tolerances selections and so forth may change to better optimize for the particular input device that is currently active. In this manner specific user interface changes map to the input device currently being used.

While the invention is susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood however that there is no intention to limit the invention to the specific form or forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention.

