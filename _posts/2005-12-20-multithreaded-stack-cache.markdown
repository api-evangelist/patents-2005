---

title: Multi-threaded stack cache
abstract: Systems and methods for storing stack data for multi-threaded processing in a specialized cache reduce on-chip memory requirements while maintaining low access latency. An on-chip stack cache is used store a predetermined number of stack entries for a thread. When additional entries are needed for the thread, entries stored in the stack cache are spilled, i.e., moved, to remote memory. As entries are popped off the on-chip stack cache, spilled entries are restored from the remote memory. The spilling and restoring processes may be performed while the on-chip stack cache is accessed. Therefore, a large stack size is supported using a smaller amount of die area than that needed to store the entire large stack on-chip. The large stack may be accessed without incurring the latency of reading and writing to remote memory since the stack cache is preemptively spilled and restored.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07805573&OS=07805573&RS=07805573
owner: NVIDIA Corporation
number: 07805573
owner_city: Santa Clara
owner_country: US
publication_date: 20051220
---
Embodiments of the present invention generally relate to instruction execution for multi threaded processing and more specifically to using a cache memory to store the top entries of a stack.

Conventional multi threaded processing systems use stacks to store data or subroutine return addresses in memory. Each stack is typically configured to store a large number of entries e.g. hundreds or thousands of entries and separate stacks are used for each processing thread. Therefore the amount of memory needed to store the stacks on the same die as the multi threaded processing units may increase the cost of producing the processing system. In order to reduce the cost in some conventional systems the stack memory is not included on the same die as the multi threaded processing units. In those systems the latency incurred accessing the stacks may reduce processing performance of the multi threaded processing units.

Accordingly there is a desire to support large stack sizes for use during multi threaded processing without reducing the processing performance due to the latency incurred while accessing the large stacks.

The current invention involves new systems and methods for storing stack data for multi threaded processing in a specialized cache. An on chip stack cache is used to store a predetermined number of stack entries for a thread. When additional entries are needed for the thread entries stored in the stack cache are spilled i.e. moved to remote memory. As entries are popped off the on chip stack cache spilled entries are restored from the remote memory. The spilling and restoring processes may be performed while the on chip stack cache is accessed. Therefore a large stack size is supported using a smaller amount of die area than that needed in designs where the entire large stack is stored on chip. The large stack may be accessed without incurring the latency of reading and writing to remote memory since the stack cache is preemptively spilled and restored.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

Stack data used during multi threaded processing is stored in a specialized on chip stack cache that preemptively spills entries to and restores entries from a remote storage resource remote memory . The spilling and restoring processes may be performed while the on chip stack cache is accessed. Therefore a large stack size that includes the combined storage capacity of the on chip stack cache and the remote memory is supported using a smaller amount of die area than that needed to store the entire large stack on chip.

A graphics device driver driver interfaces between processes executed by host processor such as application programs and a programmable graphics processor translating program instructions as needed for execution by graphics processor . Driver also uses commands to configure sub units within graphics processor . Specifically driver may program registers within graphics processor based on application programming interface API imposed limits for nesting levels that specify a maximum stack depth for use during multi threaded processing.

Host computer communicates with graphics subsystem via system interface and a graphics interface within a graphics processor . Data received at graphics interface can be passed to a front end or written to a local memory through memory controller . Graphics processor uses graphics memory to store graphics data and program instructions where graphics data is any data that is input to or output from components within the graphics processor. Graphics memory can include portions of host memory local memory register files coupled to the components within graphics processor and the like.

Graphics processor includes among other components front end that receives commands from host computer via graphics interface . Front end interprets and formats the commands and outputs the formatted commands and data to an IDX Index Processor . Some of the formatted commands are used by programmable graphics processing pipeline to initiate processing of data by providing the location of program instructions or graphics data stored in memory. IDX programmable graphics processing pipeline and a raster operations unit each include an interface to memory controller through which program instructions and data can be read from memory e.g. any combination of local memory and host memory .

IDX optionally reads processed data e.g. data written by raster operations unit from memory and outputs the data processed data and formatted commands to programmable graphics processing pipeline . Programmable graphics processing pipeline and raster operations unit each contain one or more programmable processing units to perform a variety of specialized functions. Some of these functions are table lookup scalar and vector addition multiplication division coordinate system mapping calculation of vector normals tessellation calculation of derivatives interpolation and the like. Programmable graphics processing pipeline and raster operations unit are each optionally configured such that data processing operations are performed in multiple passes through those units or in multiple passes within programmable graphics processing pipeline . Programmable graphics processing pipeline and raster operations unit also each include a write interface to memory controller through which data can be written to memory.

In a typical implementation programmable graphics processing pipeline performs geometry computations rasterization and pixel computations. Therefore programmable graphics processing pipeline is programmed to operate on surface primitive vertex fragment pixel sample or any other data. For simplicity the remainder of this description will use the term samples to refer to graphics data such as surfaces primitives vertices pixels fragments or the like.

Samples output by programmable graphics processing pipeline are passed to raster operations unit which optionally performs near and far plane clipping and raster operations such as stencil z test and the like and saves the results or the samples output by programmable graphics processing pipeline in local memory . When the data received by graphics subsystem has been completely processed by graphics processor an output of graphics subsystem is provided using an output controller . Output controller is optionally configured to deliver data to a display device network electronic control system other computing system other graphics subsystem or the like. Alternatively data is output to a film recording device or written to a peripheral device e.g. disk drive tape compact disk or the like.

Samples such as surfaces primitives or the like are received from IDX by programmable graphics processing pipeline and stored in a vertex input buffer including a register file FIFO first in first out cache or the like not shown . The samples are broadcast to execution pipelines four of which are shown in . Each execution pipeline includes at least one multi threaded processing unit to be described further herein. The samples output by vertex input buffer can be processed by any one of the execution pipelines . A sample is accepted by an execution pipeline when a processing thread within the execution pipeline is available to process the sample. Each execution pipeline signals to vertex input buffer when a sample can be accepted or when a sample cannot be accepted. In one embodiment of the present invention programmable graphics processing pipeline includes a single execution Pipeline containing one multi threaded processing unit. In other embodiments of the present invention programmable graphics processing pipeline includes a plurality of execution pipelines .

Execution pipelines may receive first samples such as higher order surface data and tessellate the first samples to generate second samples such as vertices. Execution pipelines may be configured to transform the second samples from an object based coordinate representation object space to an alternatively based coordinate system such as world space or normalized device coordinates NDC space. Each execution pipeline may communicate with texture unit using a read interface not shown in to read program instructions spilled stack data and graphics data such as texture maps from local memory or host memory via memory controller and a texture cache . Texture cache is used to improve memory read performance by reducing read latency. In one embodiment of the present invention texture cache is omitted. In another embodiment of the present invention a texture unit is included in each execution pipeline . Alternatively each execution pipeline has a dedicated instruction read interface to read program instructions from local memory or host memory via memory controller .

Execution pipelines output processed samples such as vertices that are stored in a vertex output buffer including a register file FIFO cache or the like not shown . Processed vertices output by vertex output buffer are received by a primitive assembly setup unit . Primitive assembly setup unit calculates parameters such as deltas and slopes to rasterize the processed vertices and outputs parameters and samples such as vertices to a raster unit . Raster unit performs scan conversion on samples such as vertices and outputs samples such as fragments to a pixel input buffer . Alternatively raster unit resamples processed vertices and outputs additional vertices to pixel input buffer .

Pixel input buffer outputs the samples to each execution pipeline . Samples such as pixels and fragments output by pixel input buffer are each processed by only one of the execution pipelines . Pixel input buffer determines which one of the execution pipelines to output each sample to depending on an output pixel position e.g. x y associated with each sample. In this manner each sample is output to the execution pipeline designated to process samples associated with the output pixel position. In an alternate embodiment of the present invention each sample output by pixel input buffer is processed by one of any available execution pipelines .

Each execution pipeline signals to pixel input buffer when a sample can be accepted or when a sample cannot be accepted. Program instructions configure programmable computation units PCUs within an execution pipeline to perform operations such as tessellation perspective correction texture mapping shading blending and the like. Processed samples are output from each execution pipeline to a pixel output buffer . Pixel output buffer optionally stores the processed samples in a register file FIFO cache or the like not shown . The processed samples are output from pixel output buffer to raster operations unit .

One characteristic of the system disclosed in and A is that it may be configured to embody a SIMD single instruction multiple data architecture where a thread is assigned to each sample processed in the one or more execution pipelines . Therefore a single program may be used to process several sets of samples. Thread control unit receives samples or pointers to samples stored in pixel input buffer and vertex input buffer . Thread control unit receives a pointer to a program to process one or more of the samples.

In one embodiment of the present invention thread control unit assigns a thread threadID to each sample to be processed. A thread includes a pointer to a program instruction program counter such as the first instruction within the program thread state information and storage resources for storing intermediate data generated when processing the sample. In other embodiments of the present invention rather than assigning a different threadID to each thread thread control unit assigns a threadID to several threads that are processed as a group. However there are points in a program i.e. branches where threads in a thread group are allowed to diverge from one another so that one or more threads may execute instructions on their respective samples that do not need to be executed by the other threads in the thread group. Divergent threads in a thread group may be synchronized at various points in the program to guarantee that some level of synchronized processing may be achieved at those points. Once all of the threads in the thread group are synchronized the threads resume execution in lock step i.e. each sample is processed by the same sequence of instructions in a SIMD manner.

Instruction processing unit uses the program counter for each thread to read program instructions from instruction cache to execute the thread. When a requested program instruction is not available in instruction cache it is read possibly along with other program instructions stored in adjacent memory locations from graphics memory via texture unit . A base address corresponding to the graphics memory location where a first instruction in a program is stored may be used in conjunction with a program counter to determine the location in graphics memory where a program instruction corresponding to the program counter is stored. In an alternate embodiment of the present invention instruction cache can be shared between multithreaded processing units within execution pipeline .

Instruction processing unit receives the program instructions from instruction cache and executes branch instructions using stack cache . In one embodiment there are five types of branch instructions conditional branch instructions call instructions PreBreak instructions return instructions and break instructions. Alternative embodiments may include more or fewer types of branch instructions as well as different types of branch instructions. Call PreBreak and divergent conditional branches all push entries onto cache stack while return break and branch synchronization instructions all pop entries from cache stack .

Execution of a call instruction results in a program counter changing to a different value either earlier or later in the program and the current program counter is pushed onto stack cache . Conversely when a return instruction is executed the program counter is popped from stack cache . A PreBreak branch enables a specific loop of instructions to be executed. In the program the first instruction in this loop typically follows the PreBreak instruction i.e. the first instruction in the loop is the fall through instruction . Therefore the threads executing the PreBreak branch do not have to branch to a specific instruction to execute the loop of instructions as is necessary with conditional branches and call return branches. Rather the threads simply execute the next instruction in the program to begin executing the loop in the PreBreak branch. State information about the threads that execute the PreBreak branch as well as after loop address information is pushed onto stack cache when a PreBreak branch instruction is executed. The after loop address information is popped from the stack when a break instruction is executed at the end of the PreBreak loop.

For execution of other instructions not branch instructions source data is gathered and the program instruction is output to one of the PCUs for execution. The source data may be read from pixel input buffer vertex input buffer local memory host memory or the like. Processed samples are output to a destination specified by the instruction. The destination may be vertex output buffer pixel output buffer or registers within multithreaded processing unit . Alternatively the destination may also include local memory host memory or the like.

When operating in a synchronized mode each thread being processed by a particular multi threaded processing unit independently executes the same operations or instructions on its respective sample. This type of synchronized processing is advantageous because among other things it allows groups of like samples to be processed simultaneously which increases graphics processing efficiency.

In one embodiment execution pipeline may be configured to simultaneously process twenty four independent thread groups. The different thread groups may be simultaneously processed in a MIMD multiple instruction multiple data manner relative to each other since each thread group may be processed by a different program or a different portion of the same program. In one embodiment each thread group may include up to thirty two threads. A particular multithreaded processing unit within execution pipeline may process one or more such thread groups.

Thread state information representing the current state of each thread being executed is stored in a thread state unit . Thread state unit may be a register file FIFO memory circular buffer or the like. Thread state unit is configured to maintain an active mask and an active program counter for each of the thread groups processed by multithreaded processing unit . The active mask is a string of bits that indicates which threads in the thread group are currently active i.e. currently executing instructions . Each bit in the active mask corresponds to one thread in the thread group. In one embodiment a bit is set if its corresponding thread is active. Thus when all bits in the active mask are set multithreaded processing unit is operating in fully synchronized mode for execution of the thread group associated with the active mask. The active program counter indicates the address of the instruction in the program currently being executed by the active threads.

As the multithreaded processing unit processes instructions in the program it may encounter one or more branch instructions. When a branch instruction is encountered instruction processing unit may push thread execution data onto stack cache that includes the current program counter. The thread execution data may also include state information related to various threads in the thread group such as an active mask. After pushing the thread execution data onto stack cache instruction processing unit may disable certain threads in the thread group while keeping the other threads active. The active threads then execute the instructions associated with the branch. Again the type of branch instruction encountered may determine which threads if any in the thread group are disabled and which threads remain active.

For example when a call branch instruction is executed a call thread execution data is pushed onto the stack that includes state information about the threads that execute the call return branch as well as return address information. Specifically the call thread execution data includes an active mask and a return program counter. The active mask indicates which threads are active when the call instruction is encountered. Because call instructions are not conditional there are no thread divergences associated with a call return branch. Thus the active mask included in the call thread execution data also indicates which threads execute the call return branch. The return program counter provides a return address for the threads that execute the call return branch and reflects the address of the instruction in the program subsequent to the call instruction i.e. the fall through instruction .

In another example when a PreBreak branch instruction is executed break thread execution data is pushed onto the stack that includes state information about the threads that execute the PreBreak branch as well as after loop address information. Specifically the break thread execution data includes an active mask and an after loop program counter. Similar to call instructions PreBreak instructions are not conditional. Therefore the active mask included in the break thread execution data not only indicates the threads that are active when the PreBreak instruction is encountered but also the threads that execute the PreBreak branch. The after loop program counter reflects the address of the instruction that the threads execute after executing a break instruction in the code following the PreBreak branch.

Stack cache enables divergence and flow control information to be stored and accessed in a way that precludes this information from being overwritten regardless of the number of thread divergences that occur or the number of nested conditions that exist in a program. The actual number of divergences and nesting levels that can be supported is of course limited by the size of the stack the combined size of stack cache and any memory storing spilled stack thread data used to store the divergence and flow control information which is described in further detail below as well as the SIMD width of the particular thread group. Regardless of the size of the memory stack however the system and method are robust. For example driver or a compiler may be configured to ensure that the stack depth is not exceeded for a particular program thereby avoiding problems related to memory stack size limitations.

When a branch call or PreBreak instruction is encountered instruction processing unit determines whether the instruction includes a set synchronization bit also called a set sync bit . A set synchronization bit indicates that a thread divergence may occur due to the branch. If the instruction includes a set synchronization bit instruction processing unit pushes a synchronization thread execution data onto stack cache before processing the instruction. The synchronization thread execution data includes state information related to the threads that were active when the branch instruction was first encountered. When a synchronization thread execution data is popped from the stack the threads that were active when the branch instruction was first encountered are synchronized.

If some but not all of the active threads take the branch then a thread divergence occurs and instruction processing unit pushes divergence thread execution data onto stack cache . The divergence thread execution data includes state information about the threads that do not take the branch i.e. the threads for which the inverted condition is not satisfied . When instruction processing unit executes a call instruction call thread execution data is pushed onto stack cache that includes state information about the threads that execute the call return branch as well as return address information. Specifically the call thread execution data includes an active mask and a return program counter. The active mask indicates which threads are active when the call instruction is encountered. When the call instruction includes a set synchronization thread execution data instruction processing unit pushes a synchronization thread execution data is pushed onto stack cache .

When a branch instruction is a return or break instruction then the end of a subroutine associated with a call return branch or the end of a loop of instructions associated with a PreBreak branch has been reached and instruction processing unit pops from the stack the thread execution data on the top of the stack and sets the active mask equal to the mask included in the popped thread execution data and sets the active program counter to the program counter included in the popped thread execution data. If the branch instruction is a return instruction then the popped thread execution data will be a call thread execution data that was pushed by the call instruction. Specifically the call thread execution data includes a return address e.g. the address and active mask information for the instruction after the original call instruction.

Some instructions may include a pop synchronization bit that has the opposite meaning of a set synchronization bit. A pop synchronization bit indicates that the threads that have diverged since the last synchronization thread execution data was pushed onto stack cache are to be executed to the address of the instruction that includes the pop synchronization bit in order to reestablish the level of thread synchronization that existed when this last synchronization thread execution data was pushed onto stack cache . In other words the process will reestablish the level of thread synchronization that existed when the last branch instruction including a set synchronization bit was encountered.

The stack cache is a storage mechanism that operates in a last in first out fashion and comprises a logical collection of hardware managed sixty four bit thread execution data which are described in further detail below in conjunction with . One independent stack is associated with one thread group. As will become apparent in the descriptions of B and C stack cache is populated with thread execution data i.e. thread execution data are pushed onto the stack and unwound i.e. thread execution data are popped from the stack in a way that precludes any thread execution data from being overwritten regardless of the number of divergences that occur or the number of nesting levels that exist in the program. Further the operation of stack cache in combination with the information included in the different types of thread execution data provide an efficient mechanism for executing the various instructions in a program having several branches and for synchronizing threads as they navigate the different branches of that program.

A thread stack state unit stores a counter for each thread group threadID indicating the number of stack entries in stack storage that store pushed thread execution data for the particular thread group. The counter for each threadID is updated by stack cache control unit as thread execution data is pushed to and popped from stack storage . Thread stack state unit also stores a head entry pointer for each threadID. Like the counter the head entry pointer for a particular threadID is updated by stack cache control unit as thread execution data for the threadID is pushed to and popped from stack storage . Finally thread stack state unit stores a valid flag and a dirty flag for each entry in stack storage . In some embodiments of the present invention the valid and dirty flags are stored for sets of entries for a single threadID. For example a valid and dirty flag may be stored for each set of four entries representing a thread set as described in conjunction with .

The valid flag indicates that unpopped thread execution data is stored in the entry. The valid flag is asserted when thread execution data is pushed to the entry and is negated when thread execution data is popped from the entry. The dirty flag indicates whether or not the thread execution data stored in the entry has been spilled to the remote memory portion of the stack. The dirty flag is asserted when thread execution data is pushed to the entry and is negated when the thread execution data is spilled to the remote memory. Use of the valid and dirty flags is described in greater detail in conjunction with B and C.

Rather than moving individual entries to and from the remote memory stack cache control unit moves thread sets to and from the remote memory. Moving thread sets may use the bandwidth available between execution pipeline and the remote memory more efficiently than moving single entries. For example when memory controller includes a 256 bit interface between local memory it is more efficient to move a single 256 bit thread set in one transaction rather than moving four 64 bit entries in four transactions. Furthermore storing fewer bits for the valid and dirty flags storing a flag per thread set rather than per entry reduces the number of storage resources needed in thread stack state unit .

In step stack cache control unit stores the thread execution data in stack storage at the entry corresponding to the head entry 1 or 1 depending on the direction of the circular buffer for the threadID. In step thread stack state unit increments the stack entry count for the threadID to indicate that thread execution data has been pushed. In step thread stack state unit updates the head entry pointer for the threadID to point to the entry storing the pushed thread execution data. In step thread stack state unit marks the entry storing the pushed thread execution data as valid and dirty.

In step stack cache control unit determines if a portion of the entries in the thread stack for the threadID should be spilled to stack spill storage . In some embodiments of the present invention stack cache control unit uses a programmable threshold value to determine if the thread stack should be spilled. The threshold value may correspond to the number of entries that store pushed data in the thread stack valid and dirty entries . In some embodiments of the present invention stack cache control unit determines a thread set should be spilled when a thread set boundary is crossed and the next thread set is valid and dirty. For example cache control unit determines a thread set should be spilled when the first entry in thread set of is used to store pushed data i.e. the boundary between thread set and is crossed and thread set is valid and dirty i.e. stores unspilled data.

If in step stack cache control unit determines that a portion of the entries in the thread stack for the threadID should not be spilled to stack spill storage then in step the push processing is complete. Otherwise in step stack cache control unit moves N entries from stack storage to stack spill storage where N is an integer number of entries greater than or equal to one and less than the total number of entries in stack storage . Stack cache control unit also updates a spill stack pointer stored in thread stack state unit to point to the top entry for the threadID in stack spill storage . N may be the number of entries in a thread set or another value that is fixed or programmable. In step thread stack state unit marks the N entries in stack storage as clean i.e. negates the dirty flag s corresponding to the entries and proceeds to step completing the push processing. Note that a separate state machine may be used to perform the spilling process so that steps and may be completed while data is pushed onto stack storage .

In step thread stack state unit clears the valid flag for the popped entry i.e. marks the entry as invalid. In step thread stack state unit decrements the stack entry count for the threadID to indicate that thread execution data has been popped. In step thread stack state unit updates the head entry pointer for the threadID to point to the next entry that stores pushed thread execution data. Entries that have been spilled from stack storage to stack spill storage remain valid until they are overwritten by pushed data and are marked as dirty. Although the entries may have been preemptively spilled to stack spill storage the entries have not been overwritten and may be popped from stack storage without being restored from stack spill storage . When an entry marked valid and clean is popped from stack storage the spill stack pointer stored in thread stack state unit to point to the top entry for the thread in stack spill storage is updated to effectively pop the entry from stack spill storage .

In step stack cache control unit determines if there are entries in stack spill storage that store thread execution data for the threadID and if not then in step the pop processing is complete. Otherwise in step stack cache control unit pops a number of entries N for the threadID from stack spill storage and restores the number of entries to N entries in stack storage that are invalid. In some embodiments of the present invention entries are restored from stack spill storage when the number of invalid entries for the threadID is greater than a fixed or programmable threshold value. In step thread stack state unit sets the valid flags for the restored entries i.e. marks the entries as valid. In step thread stack state unit may clear the dirty flags for the restored entries i.e. marking the entries as clean. The spill stack pointer stored in thread stack state unit is also updated to point to the top entry for the thread in stack spill storage and in step the pop processing and restoring is complete. Note that a separate state machine may be used to perform the restoring process so that steps and may be completed while data is popped from stack storage .

In step stack cache control unit determines if a set boundary i.e. a boundary between two thread sets has been crossed. If not in step the pop processing is complete. If however in step stack cache control unit determines that a set boundary has been crossed then in step thread stack state unit clears the valid flag for the popped entry i.e. marks the thread set as invalid. When the set boundary is crossed the last entry in a thread set has been popped.

In step stack cache control unit determines if a thread set is stored in stack spill storage for the threadID and if not then in step the pop processing is complete. Otherwise in step stack cache control unit determines if the next thread set is valid and if so then in step the pop processing is complete. When the next thread set is valid the current thread set and the next thread set each contain entries that may be popped. In this embodiment of the present invention stack cache control unit determines a thread set should be restored when a thread set boundary is crossed and the next thread set is invalid i.e. does not store pushed thread execution data.

If in step stack cache control unit determines that the next thread set is not valid then in step stack cache control unit pops a thread set for the threadID from stack spill storage and restores the thread set to the invalid thread set in stack storage . In step thread stack state unit marks the restored thread set as valid and clean. The spill stack pointer stored in thread stack state unit is also updated to point to the top entry for the thread in stack spill storage and in step the pop processing and restoring is complete.

Persons skilled in the art will appreciate that any system configured to perform the method steps of B or C or their equivalents is within the scope of the present invention. Stack cache is used to store a predetermined number of stack entries for a thread. When additional entries are needed for the thread entries stored in the stack cache are spilled i.e. moved to stack spill storage in a remote memory. As entries are popped off the stack cache spilled entries are restored from stack spill storage . The spilling and restoring processes may be performed while the stack cache is accessed. Therefore a large stack size is supported using a smaller amount of die area than that needed to store the entire large stack on chip. The seemingly large stack may be accessed without incurring the latency of reading and writing to remote memory since entries in stack cache are preemptively spilled and restored.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. The listing of steps in method claims do not imply performing the steps in any particular order unless explicitly stated in the claim.

