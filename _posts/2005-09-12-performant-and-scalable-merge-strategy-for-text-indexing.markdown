---

title: Performant and scalable merge strategy for text indexing
abstract: A full-text search index system and method is generated by creating instances of a database index from an in-memory inverted list of keywords associated with a text identifier and the occurrences of the keyword in the text. Instances of the index are placed in a priority queue. A merge scheduling process determines when a merge should be initiated, selects instances of the index to be merged and selects a type of merge to perform. Instances of an index are assigned a temporal indicator (timestamp). A set of instances is selected to be merged. The set of instances is validated and merged.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07590645&OS=07590645&RS=07590645
owner: Microsoft Corporation
number: 07590645
owner_city: Redmond
owner_country: US
publication_date: 20050912
---
This application is a continuation application and claims priority to U.S. patent application Ser. No. 10 164 052 filed Jun. 5 2002 entitled Performant and Scalable Merge Strategy for Text Indexing which is incorporated herein by reference in its entirety.

This invention relates generally to the field of computing and in particular to full text indexing in a database environment.

Full text searching of unstructured and semi structured data is becoming more and more important in the world of computing. For many years the information retrieval community has had to deal with the storage of documents and with the retrieval of documents based on one or more keywords. Since the burgeoning of the World Wide Web and the feasibility of storing documents on line retrieval of documents based on keywords has become a thorny problem. A number of software solutions have been developed which have attempted to address some of these problems.

A large portion of digitally stored information is presently stored in the form of unstructured textual data both in plain text files and in formatted documents. Although the bulk of this textual data is stored in file systems there are advantages to storing such data in relational databases. By doing so the advantages of a database including high performance access query capability simple application based user interfaces for end users and secure remote access are made available.

Database management systems DBMSs such as SQL Server are widely used to search structured data. It is impractical however to search unstructured data e.g. text documents the same way structured data is searched because doing so is too expensive.

For example in order to retrieve information from structured data in a database a user typically provides a query written in a query language such as SQL where the query specifies the structured information to be retrieved the search term or terms the field in which the search term is to be found and the manner in which the retrieved information is to be manipulated or evaluated in order to provide a useful result which is typically a relational operator or a function. To process the query the database system typically converts the query into a relational expression that describes algebraically the result specified by the query. The relational expression is used to produce an execution plan which describes particular steps to be taken by a computer to produce the requested result. Because the search term and the field where the search term is sought are specified such results can be returned quickly. Indexes based on key fields e.g. an index based on name or social security number for a personnel database routinely assist in efficient searching.

A similarly conducted search for the same search term in unstructured data would require a word by word search of the entire text database and is unworkable.

Typically today an inverted index for searching documents is created by building a custom data structure external to the database system before a search query is entered. These solutions usually involve pulling data out of the database via bridges or gateways and storing the data as files in the file system so that textual indexing can be applied. Some systems actually store index data in a database but use an external engine to build and query the index. This approach does not provide a seamless way for a user to combine a textual query with a regular structured relational query and limits the extent to which a query can be optimized.

Typically a full text index is organized as a tree where internal nodes represent keywords and whose external nodes contain document identifiers and occurrences. When searched the keyword s are looked up in the index and the documents containing the keyword s are retrieved. Naturally whenever the collection of documents changes a new index must be built or the existing index must be updated.

Although full text searching is frequently a capability of database management systems the implementation of full text search is typically unable to take advantage of the features of the database management system which is to say relational database management systems generally are unable to accommodate full text searching of documents within the structure of the database. Typically the index created to search the document database is not itself part of the database system i.e. is separate from the database s index system . Because the index created is not part of the database system certain limitations arise and certain highly advantageous aspects of database systems do not apply to typical full text search systems.

Limitations associated with a full text search system that relies on an external index include the following 

Similarly some of the advantages of database management systems are not applicable to a full text search system based on a custom index. For example most database systems have excellent facilities for data recovery in the event of database degradation however these data recovery systems do not work for the index file because the index file is not a DBMS data store. Hence data corruption can be a frequent problem with a file system index file. If there is a hardware malfunction it is very difficult to efficiently reach a point where the documents database and the documents index are in sync because the two different systems have different recovery protocols.

Backup and restore mechanisms for the index file generally do not have the advanced features typically available for database files as discussed above.

Scalability issues exist for the index file. Scalability refers to partitioning one logical table into multiple physical tables on the same machine or on different machines in order to accommodate very large collections of data. For example instead of storing a large database on a single resource it is frequently desirable to split or partition the database across a number of resources. Database data stores generally maintain data in tables that can reside locally on a single data store or can be distributed among several data stores in a distributed database environment.

Advantages to partitioning are the reduction of processing load on a single resource faster access to data and if a particular machine experiences a hardware failure only part of the data is lost. Partitioning however is typically not available for a file system index file because partitioning a file system file requires a separate infrastructure. Thus typically the index file although frequently very large cannot be partitioned so a single resource must be dedicated to the index.

Hence a need exists in the art to provide a full text searching system wherein the index is built upon standard database technology.

Most of the methods of building text indexes based on keyword document identifier and occurrence lists share the mechanism of building compressed inverted lists and merging the inverted lists to build merged indexes. For example when a database is searched data is typically scanned and then indexed. Each time a crawler finishes crawling a batch of data an indexer may build an inverted list of keywords with data identifiers and the occurrences of the keyword s in the data an index . Typically the index is persisted. Frequently several or many indexes are built per data set because typically the body of data is quite large. Indexes are then merged together. During the merge of an old index into a new index typically a table lookup must be done for every data identifier in the older index to see if the data has been changed or deleted since the older index was built. For example if a particular data item was present in the older index but is deleted or changed later the information about the data from the old index is not included in the new index. Typically for performance reasons this table is stored in memory. It would be helpful if the number of table lookups could be reduced especially if the need for an in memory data structure for the lookup table could be reduced or eliminated.

A full text indexing system and method is built using standard database technology e.g. the Microsoft SQL SERVER storage and query engine . An inverted list is generated comprising keywords contained in data e.g. text an identifier associated with the data and the occurrence s of the keyword in the list. An instance of a database index part of the DBMS indexing system is created and stored in a priority queue. A scheduling process determines when instances of the index are to be merged. Instances are prioritized by number of rows keywords and size. One of three types of merges is selected and the instances are merged.

A crawl associated with a temporal indicator called herein a timestamp and equivalently represented by the variable TS crawls through scans data. Data that has been crawled is also associated with the crawl timestamp. A crawl timestamp is assigned to a crawl at the start of the crawl. A table called herein DocIdMap maps a full text data item identifier to a numeric identifier. The numeric identifier of the data item is also preferably associated with the crawl timestamp of the crawl that scanned the data.

A plurality of indexes may be generated by one crawl. Each of the indexes generated by a crawl receives the timestamp of the crawl that generated it so that all of the indexes generated by the same crawl will have the same timestamp. Indexes generated by a subsequent crawl will have a timestamp that is greater or recognizable as later occurring than that of the earlier crawl. A crawl is associated with a timestamp based on the start time of the crawl so that crawls and their resulting indexes are ordered temporally.

A set of indexes to be merged is selected. Validity of the set of indexes is determined. Merge transactions merge a set of validated indexes into one comprehensive index. The merge process scans a set of indexes in order of keywords and for each keyword from each index scans the numeric identifiers in the associated list of numeric identifiers. If a numeric identifier qualifies for the new index the numeric identifier and its associated occurrences are put into the new index. If a numeric identifier does not qualify for the new index the numeric identifier and its associated occurrence list is skipped and the next numeric identifier is considered.

Whether or not a numeric identifier of a particular index qualifies for the new index is determined by comparing the timestamp of the index with the maximum timestamp of the set of indexes to be merged and the timestamp of the identifier. A lookup into the DocidMap table is only required when a data item is deleted or when a timestamp for the numeric identifier is less than the maximum timestamp of the indexes to be merged. This reduces the need for table lookups and in some cases eliminates the lookup completely. Because the need for table lookups is significantly reduced all of the table does not have to be resident in memory all the time. As a special case in case of a full crawl there is no need for a lookup of the mapping table.

Although not required the invention can be implemented via an application programming interface API for use by a developer and or included within the network browsing software which will be described in the general context of computer executable instructions such as program modules being executed by one or more computers such as client workstations servers or other devices. Generally program modules include routines programs objects components data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the program modules may be combined or distributed as desired in various embodiments. Moreover those skilled in the art will appreciate that the invention may be practiced with other computer system configurations. Other well known computing systems environments and or configurations that may be suitable for use with the invention include but are not limited to personal computers PCs automated teller machines server computers hand held or laptop devices multi processor systems microprocessor based systems programmable consumer electronics network PCs minicomputers mainframe computers and the like. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network or other data transmission medium. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus .

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CDROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies. A user may enter commands and information into the computer through input devices such as a keyboard and pointing device commonly referred to as a mouse trackball or touch pad. Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB .

A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . A graphics interface such as Northbridge may also be connected to the system bus . Northbridge is a chipset that communicates with the CPU or host processing unit and assumes responsibility for accelerated graphics port AGP communications. One or more graphics processing units GPUs may communicate with graphics interface . In this regard GPUs generally include on chip memory storage such as register storage and GPUs communicate with a video memory . GPUs however are but one example of a coprocessor and thus a variety of coprocessing devices may be included in computer . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface which may in turn communicate with video memory . In addition to monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

One of ordinary skill in the art can appreciate that a computer or other client device can be deployed as part of a computer network. In this regard the present invention pertains to any computer system having any number of memory or storage units and any number of applications and processes occurring across any number of storage units or volumes. The present invention may apply to an environment with server computers and client computers deployed in a network environment having remote or local storage. The present invention may also apply to a standalone computing device having programming language functionality interpretation and execution capabilities.

Full text search indexing systems utilizing an external index may be implemented as shown in . Typically a group of documents includes individual documents etc. one or more of which may be loaded into a database basetable of a DBMS . In general a document is loaded into the columns of a row in the basetable one document per column. The database system including database management system engine and data stores databases typically associates document with a unique document identifier called herein full text key which is used to identify the row in the basetable that contains the document. The text of document is provided to word breaker by search engine . Word breaker determines which words of the text of document should be keywords also known as tokens . Word breaker also determines the location s of the keyword in the document as a number representing byte offset from the beginning of the document. In other words if word breaker determines that a key word of document is innovation and the word innovation is the first fourteenth and twenty fifth word in document the word breaker would typically return the keyword innovation and the byte offsets at which the first fourteenth and twenty fifth words are located to search engine . An index of tokens or keywords is generated by an external executable such as a search engine . For each keyword associated with the text of document an entry in index would be created typically containing the keyword and the occurrences of the keyword in document for each keyword in document

For example and referring now to suppose a document of a collection of documents including document etc. is loaded into a database basetable . Document is assigned a full text key by the database management system . For example database management system may assign document the full text key . The full text key assigned by the database management system typically is alphanumeric and can be rather large. Assume document full text key contains the text This innovation is a significant step towards search and indexing technology. somewhere within the document. When full text key is loaded into basetable This innovation is a significant step towards search and indexing technology is loaded into basetable of database management system . The words This innovation is a significant step towards search and indexing technology for example might be loaded into word locations through of a row in the content field of basetable .

External search engine may retrieve data associated with full text key from basetable and send this data to word breaker . Typically a word beaker would be invoked to pick out the keywords or tokens from the text of document . In the example word breaker has determined that the keywords contained in document include innovation and significant . Word breaker also typically determines the byte offset from the beginning of the document at which the keywords also referred to as tokens are found. For example word breaker may determine that innovation can be found in document at byte offsets and and that significant can be found in document at byte offsets and . This information may be passed back to search engine .

Search engine typically uses this information to create index and may convert byte offset to word offset. In the example described above index would contain keyword innovation followed by full text key followed by the locations word offset at which innovation can be found in the document i.e. . Suppose another document assigned a full text key of also contains the word innovation at word locations and . The index in this case would contain an entry for full text key followed by the locations at which innovation can be found i.e. in full text key . Similar entries for keyword significant would also be generated for full text key and full text key . Thus a file system index typically is created containing keywords full text keys for documents containing each keyword and the locations within each document containing the keyword specifying the location by word offset of the keyword in that document.

A user querying the database might ask for example for all the documents containing the words innovation located near the word significant . Depending on the specific query form a table of results might be returned listing the document numbers containing the words innovation located near significant . Additionally a rank for each document is also typically returned where the rank is based on some proprietary formula such as ranked by number of times the word s appeared in the document whether the requested word s appeared in the title of the document etc. . Rank typically reflects the relevance of the document to the search query.

Index is typically a structure built on the file system and is not a database file and therefore is subject to the limitations in recoverability reliability management and scalability described above.

At step a new batch scan transaction is begun. Each batch begins with a new transaction. At step a row of data is retrieved from basetable . A thread from thread pool scans basetable and stores data i.e. text from a row of basetable in text shared memory . For example thread may scan row of basetable storing the row in chunks in shared text memory at some location in the shared memory associated with thread

At step a unique numeric document identifier referred to as DocId henceforth is generated which preferably translates a sometimes large full text key that is not necessarily numeric in nature to a compact numerical identifier called herein DocId . For example full text key may be translated to DocId 1 and full text key may be translated to DocId 2 . A table that maps full text keys to DocIds is maintained preferably keyed on full text key. As each row is crawled the new unique DocId generated is stored as well in text shared memory .

Text shared memory preferably can be accessed by word breaker . As the rows are crawled the full text columns stored in shared memory are parsed by filters and word breakers collectively represented by reference numeral of . Filters typically are modules that parse formatted documents e.g. WORD documents or POWERPOINT documents and emit chunks of text to be parsed by word breaker .

Filters of may retrieve for example the data contained in row of basetable stored in shared memory and emit a text chunk. Word breaker feeds on the emitted text chunks and determines which of the words in the document are keywords. For each of the words determined by word breaker to be a keyword the location s of that keyword in the document is determined. Word breaker writes the keyword and occurrence information for each DocId to keyword shared memory .

At step a thread e.g. thread picks up the keyword the DocId and the occurrences of the keyword from keyword shared memory and inserts them into an inverted list called herein InvertedList associated with the batch.

InvertedList preferably is a short in memory list of keywords and keyword occurrences. An inverted list is generated for the batch of documents processed by the thread transaction by transaction. Alternatively more than one inverted list is generated per batch of documents.

InvertedList as illustrated by is preferably implemented as an in memory balanced tree of keywords etc. found in the documents processed in the batch. In one embodiment for each keyword a heap of DocIds and containing that keyword is maintained. For each DocId and in the heap a sorted list of locations and word position preferably implemented as word offset which is the same as the basetable word location is maintained where the respective keyword appears in the document. This information is preferably stored in local memory not shown associated with thread

Rows from the basetable are processed until it is determined that the batch is complete step . Batches may be determined to be complete when some configurable parameter representing a number of documents processed has been reached. When the batch is complete at step thread creates an instance of index e.g. index . Index is a standard database table e.g. a SQL B tree . A database table containing meta data about instances of index is also updated. In the example the meta data table is updated to reflect index instance . This table is called herein FtIndex Dictionary. The relevant structure of an exemplary FtIndex Dictionary is depicted in i.e. FtIndex Dictionary may include additional entries .

Possible values for IndState include StateNew StateOpen StateClosed and StateInMerge. IndState for the index instance is set to StateNew when the index object is created but is not yet used. IndState for the index instance is set to StateOpen when the index is in use but does not yet contain any committed data. IndState for the index instance is set to StateClosed when the index instance contains committed data and processing is complete. IndState for the index instance is set to StateInMerge when the index instance is being merged with other indexes.

After the inverted list is persisted into an instance of an index and DocIdMap has been updated for the batch of rows processed by the thread the batch is committed e.g. SQL commit at step . The 2 part commit transaction ascertains that all locks and resources are acquired and then performs the disk writes. The commit transaction guarantees that no document in a batch is partially committed. If all locks and resources are not acquired the database changes are entirely backed out. As soon as an instance of an index is committed the index is queryable.

An InvertedList is persisted by inserting one or more rows for each keyword in InvertedList into a new instance of FtIndex. DocIdList is generated by compressing sorted DocIds from InvertedList into a binary stream. Occurrences of the keyword from InvertedList are also compressed into a binary stream for each DocId containing the keyword. Occurrence lists of two consecutive DocIds stored in OccListList are preferably separated by a sentinel in the compressed stream to delimit the end of one occurrence list from the beginning of the next occurrence list. When the InvertedList is persisted into an instance of FtIndex and FtIndex is committed the state of that instance of FtIndex becomes StateClosed. The number of DocIdList BLOBs stored in a row in an instance of FtIndex is preferably limited by a parameter. At step the instance of FtIndex is added to priority queue .

Periodically instances of index e.g. FtIndex are merged into fewer instances. Merging instances of the index improves query performance because fewer indexes have to be checked when a query is processed. Merging instances into fewer instances also optimizes the amount of storage and memory required because data repeated in index instances is eliminated e.g. FtIndex A and FtIndex B may both contain the keyword innovation so that two rows one in FtIndex A and one in FtIndex B can be combined into one row in FtIndex X . Finally the number of I O input output operations is reduced because when the merged index is rewritten related data is physically located closer together.

It is desirable to merge indexes of approximately the same size for efficiency considerations. The priority queue of indexes is preferably implemented as a heap data structure having the sort order IndRows IndDataSize so that the smallest index in that order is located at the top of the heap. Hence as indexes are selected from the priority queue in step for merging indexes containing less data are selected before those containing more data. It will be appreciated that an alternative to sorting in ascending order and selecting from the top of the queue is sorting in descending order and selecting from the bottom of the queue. At step an index is selected to merge. Indexes are pulled from priority queue until any of the following conditions occur. It should be understood that the order of the steps and has been selected for exemplary purposes only and in practice any ordering or combination of these steps may be implemented by the conceived invention. A counter called herein SelectedIndexCounter that tracks the number of indexes selected to merge is incremented. In one embodiment of the invention a configurable parameter referred to herein as MergeFanout determines the preferred number of indexes to be merged in one merge. At step SelectedIndexCounter is compared to MergeFanout. If MergeFanout is reached or exceeded processing continues at step . If MergeFanout is not reached processing continues at step .

It is desirable to merge indexes of approximately the same size however even indexes of the same overall size may merge inefficiently. For example FtIndex001 and FtIndex002 may be approximately the same size but FtIndex001 may have a small number of keywords rows with a large number of associated DocIds and associated data while FtIndex002 may have a larger number of keywords rows with a smaller number of documents associated therewith. Alternatively FtIndex001 and FtIndex002 may have approximately the same number of unique keys rows but FtIndex001 may reflect the indexing of far more documents than does FtIndex002. Hence steps and are performed. At step IndRows for the selected index is compared to the average number of rows in the other selected indexes. If IndRows for the selected index is much greater than the average processing continues at step . If not processing continues at step . At step IndDataSize for the last index selected is compared to the average IndDataSize of the other selected indexes. If IndDataSize for the last index selected is much greater than the average processing continues at step . If IndDataSize for the last index selected is not much greater than the average processing continues at step in which the index is added to MergeList and the next index is selected from the merge queue step .

At step the last selected index is returned to the priority queue and processing continues at step . At step the selected indexes in MergeList are merged. Merging preferably can occur in one of three ways. illustrates a first merge strategy in accordance with one aspect of the invention. In MergeOutOfPlace indexes selected from the priority queue represented by exemplary indexes I I . . . I are merged in the order of Keyword PropertyId to form a merged DocIdList OccListList and OccCountList and the merged rows are inserted into a new index e.g. I . This type of merge is preferably performed when most of the rows need to be updated. MergeOutOfPlace is typically very fast because a bulk insert can be performed.

An indexing module e.g. thread of thread pool builds instances of an index and assigns a temporal indicator to the instance. Instances of the index are placed in a priority queue . A merging thread e.g. of merge pool selects instances from priority queue to merge and validates the set of selected instances. Invalid instances are returned to priority queue . Valid instances are merged to generate an instance of a merged index e.g. of . The merged index is returned to the priority queue .

The following database tables illustrated in and are built and maintained. Preferably the database management system utilized is SQL SERVER but use of any suitable database management system is contemplated by the invention.

Assume for the sake of clarity of discussion that a view of DocIdMap is defined for each active DocIdTimestamp T denoted as DocIdMap T which comprises a list of all DocIds where the DocIdTimestamp of the DocId is T. The view DocIdMap T includes a unique clustered index on DocId . DocidMap T for T infinity thus contains a list of deleted DocIds.

An indexed view Partial DocIdMap is preferably maintained. Partial DocIdMap includes a subset of all the DocIds in DocIdMap that got updated since the last full crawl. That is preferably Partial DocIdMap includes a list of all DocIds having a DocIdTimestamp later than the DocIdTimestamp of the latest full crawl. The view Partial DocIdMap includes a unique clustered index on DocId.

In any given crawl data that has been indexed preferably is not re indexed that is if the data being crawled is a set of documents and Crawl indexes Document Document will not be crawled again by Crawl although Document may be crawled by a subsequent crawl.

When a crawl is initiated a crawl identifier CrawlId is assigned to the crawl. The crawl is also assigned a CrawlTimestamp . CrawlTimestamp is a unique ordered number so that if a second crawl is initiated after a first crawl is initiated the second crawl will receive a CrawlTimestamp that is greater or otherwise indicates that the crawl is happening later than the first crawl timestamp. CrawlStatus for the crawl is set to in progress . An entry is made in CrawlDictionary for the crawl persisting the values for CrawlId CrawlTimestamp and CrawlStatus . Preferably timestamps are generated by a random number generator where only increasing numbers can be generated. Periodically the number generator may be reset.

In one embodiment the timestamp is reset by allowing all the active crawls to finish and performing a full merge of all the indexes. After the full merge is performed only one index with one timestamp is left. The timestamp of this index is updated to a new timestamp. The timestamp of all non deleted data identifiers are also updated to the new timestamp. The new value is a small value e.g. 1 . The timestamp counter is reset to this small value. Alternatively any suitable method of resetting the timestamp may be performed.

Other suitable methods for generating an ordered unique timestamp are contemplated by the invention. Timestamps are preferably based on a common time reference. Timestamps preferably are assigned based on the starting time of the crawl. If there are two crawls that start at exactly the same time preferably one crawl will still receive a timestamp that is greater than the other s timestamp.

At step data to be indexed is retrieved. The data to be indexed may be retrieved from a database e.g. from basetable or alternatively may be retrieved from a file system. The data to be indexed is identified by a full text key DocKey . For example perhaps the row in basetable full text key DocKey axpa is retrieved. DocKey typically is a somewhat large alphanumeric identifier that may not be particularly well suited to compression. Therefore preferably DocKey is mapped to a smaller unique numeric document identifier DocId e.g. 1 . The data item identified by DocKey and DocId is associated with the Timestamp of the crawl that accessed it so that DocIdTimestamp is set to CrawlTimestamp . DocKey DocId and DocIdTimestamp are updated in DocIdMap . Indexed View Partial DocIdMap gets updated likewise. View DocIdMap T for the timestamp DocIdTimestamp likewise gets changed.

At step the data retrieved is indexed e.g. an inverted list of keywords DocId and occurrences of keyword s in the data item is generated . At step if more data is to be scanned by the transaction processing returns to step . If all the data to be scanned by the transaction has been processed step is performed. At step an instance of an index FtIndex is generated. A unique identifier is generated and is associated with the index instance. The index identifier is called herein IndexId . The index instance is also preferably associated with an IndexTimestamp which is the same as the CrawlTimestamp of the crawl generating the index which is the same as the DocIdTimestamp of the data being indexed. Preferably the index includes a plurality of keywords and is in keyword order. Associated with each keyword preferably is a list of DocIds that include the keyword and a list of occurrences e.g. a list of word locations at which the keyword is found in the document . FtIndexDictionary is updated for the index instance.

At step the index instance is added to a priority queue. At step if the batch is complete the process continues at step . At step CrawlStatus is set to done . Done means that no additional data will be crawled with the same CrawlId . Thus there will be no more data with the timestamp of CrawlTimestamp for that crawl. If the batch is not complete new data is retrieved for indexing and a new index instance is initiated.

If a data item to be indexed has changed between crawls in one embodiment the data item is preferably flagged for re indexing. Alternatively the process controlling the crawl may be notified that a particular data item must be re indexed.

Referring now concurrently to assume that CrawlID associated with CrawlTimestamp is initiated at step . CrawlDictionary row column is updated with CrawlID row column with CrawlTimestamp and row column with CrawlStatus i.e. in progress . It should be understood that the particular rows and columns and identifying names used throughout the examples are exemplary only and the invention contemplates any suitable table entry convention. It should also be noted that for clarity only IndexId and IndexTimestamp are displayed in FtIndex Dictionary in .

At step assume that data from basetable row column identified by DocKey of basetable row column is retrieved. Assume that DocKey maps to DocId . DocIdMap row column is updated with DocKey row column is updated with DocId and row column with DocIdTimestamp .

At step the data is indexed e.g. an inverted list is generated . At step assume that the transaction is not done so another data item for example from basetable row column e.g. the data associated with DocKey of basetable row column is retrieved. Assume DocKey maps to DocId . DocIdMap row column is updated with DocKey row column with DocId and row column with DocIdTimestamp .

At step assume the transaction is done. At step a new index instance e.g. IndexId 1 is generated by persisting the inverted list. IndexId receives the timestamp of the crawl that generated it i.e. 1 . FtIndexDictionary row column is updated with IndexId and row column with IndexTimestamp . At step IndexId is added to the priority queue.

Assume that at step it is determined that the batch is not done. Processing returns to step . At step assume that data from basetable row column identified by DocKey of basetable row column is retrieved. Assume that DocKey maps to DocId . DocIdMap row column is updated with DocKey row column with DocId and row column with DocIdTimestamp . At step the data is indexed e.g. an inverted list is generated .

At step assume that the transaction is done so processing continues at step . At step a new index instance e.g. IndexId 2 is generated by persisting the inverted list. Row column of FtIndexDictionary is updated with IndexId and row column with IndexTimestamp . At step IndexId is added to the priority queue. Assume that at step it is determined that the batch is done. At step row column of CrawlDictionary is updated to done CrawlDictionary row column .

Still referring to assume that a second crawl is now initiated. Assume that CrawlID associated with CrawlTimestamp is initiated at step . CrawlDictionary row column is updated with CrawlID row column is updated with CrawlTimestamp and row column with CrawlStatus i.e. in progress . Assume further that the data associated with DocKey has changed since CrawlId and that DocKey has been flagged for re indexing. For example row column of basetable may contain an update flag. Alternatively other methods of flagging the data item or otherwise sending notification of a need for re indexing are contemplated by the invention.

At step the updated data from basetable row column identified by DocKey of basetable row column is retrieved. DocKey still maps to DocId but DocIdMap row column is updated with DocIdTimestamp . DocIdMap T will now contain an entry for DocId for timestamp T 1 and an entry for DocId for timestamp T 2.

At step the data is indexed e.g. an inverted list is generated . At step assume that the transaction is not done so another data item for example from basetable row column e.g. the data associated with DocKey of basetable row column is retrieved. Assume DocKey maps to DocId . DocIdMap row column is updated with DocKey row column with DocId and row column with DocIdTimestamp .

At step assume the transaction is done. At step a new index instance e.g. IndexId 3 is generated by persisting the inverted list. IndexId receives the timestamp of the crawl that generated it i.e. 2 . FtIndexDictionary row column is updated with IndexId and row column with IndexTimestamp . At step IndexId is added to the priority queue.

Assume that at step it is determined that the batch is not done. Processing returns to step . At step assume that data from basetable row column identified by DocKey of basetable row column is retrieved. Assume that DocKey maps to DocId . DocidMap row column is updated with DocKey row column with DocId and row column with DocIdTimestamp . At step the data is indexed e.g. an inverted list is generated .

At step assume that the transaction is done so processing continues at step . At step a new index instance e.g. IndexId 4 is generated by persisting the inverted list. Row column of FtIndexDictionary is updated with IndexId and row column with IndexTimestamp . At step IndexId is added to the priority queue. Assume that at step it is determined that the batch is done. At step row column of CrawlDictionary is updated to done not shown .

Merge transactions merge a set of existing indexes into one comprehensive index. Preferably each index has entries for keywords encountered during the crawl of data indexed in the index. The index preferably is in ascending keyword order. For each keyword the index includes a list of DocIds for the data in which the keyword has occurred at least once. Associated with each of the DocIds preferably is a list of occurrence locations at which the keyword occurs in the data. The DocId list and the occurrence information preferably are stored in a compressed form in order to optimize storage and to minimize I O requirements.

Indexes to be merged are selected as described below. The merge process scans a set of indexes. The indexes preferably are in ascending keyword order. Associated with each keyword preferably is a list of DocIds in ascending order . This DocIdList is scanned for each keyword from every index. If a DocId qualifies for the new index the DocId and its associated occurrences is placed into the new index. If the DocId and its associated occurrences does not qualify for the new index that DocId is skipped and the next element considered.

At step the CrawlStatus for the CrawlId that generated the index is examined. If each crawl associated with an index in set S has CrawlStatus of Done processing continues at step . If any index is associated with a Crawl whose CrawlStatus is not done the index is returned to the priority queue and the next index is examined step .

At step it is determined if the set of indexes S includes a set of indexes representing a full crawl. If the set of indexes S does include a set of indexes representing a full crawl all indexes with a timestamp less than the timestamp of the set of indexes representing a full crawl are deleted from S at step and processing continues at step .

If the set of indexes S does not include a set of indexes representing a full crawl the timestamp of each of the indexes in set S is compared to the set of indexes in the priority queue U. If an index in the priority queue is associated with a timestamp T that is identical to or greater than the timestamp of an index that exists in U but does not exist in S then the indexes with timestamp T are removed from S and returned to the priority queue at step . If however every index in S has a timestamp which is less than each index in U but not in S processing continues at step .

Referring now concurrently to assume that IndexIds and rows and of FtIndexDictionary are selected to be merged. At step the IndexTimestamp of IndexId row column of FtIndexDictionary is compared to the IndexTimestamp of IndexId row column of FtIndexDictionary . Since both timestamps are 1 this is a homogeneous merge the merge index will receive a timestamp of and processing continues at step .

Assume now that IndexIds and are selected at step to be merged. At step the IndexTimestamp of IndexId row column of FtIndexDictionary is compared to the IndexTimestamp of IndexId row column of FtIndexDictionary and the IndexTimestamp of IndexId row column of FtIndexDictionary . Since the IndexTimestamp of IndexIds and are 1 but IndexTimestamp of IndexId is 2 the timestamps are not all the same so processing continues at step .

At step the CrawlStatus of the crawl associated with IndexTimestamp for each index in S is determined. Row column of CrawlDictionary indicates that CrawlStatus for CrawlTimestamp is Done but CrawlStatus for CrawlTimestamp is listed as In progress . Thus IndexId is removed from S and returned to the priority queue at step .

Assume instead that CrawlStatus for CrawlTimestamp is also Done . In this case processing continues at step . At step it is determined whether S includes a set of indexes that represents a full crawl. A full crawl as used herein refers to a crawl that has scanned all the data in a data set. For example assume CrawlId scanned DocKeys through in Basetable generating IndexIds and . In this case CrawlId would be a full crawl. If all the indexes generated by CrawlId IndexIds and were included in set S all the indexes generated by CrawlId IndexId would be deleted from the merge set S at step . In fact none of the indexes generated by crawls predating a full crawl are required because those indexes were replaced with newer data. Thus preferably the outdated indexes are removed from the database.

Assuming again that S contains IndexIds and at step the timestamp of each of the indexes in set S IndexIds and having respective timestamps of and is compared to the timestamp of the other indexes in the merge queue IndexId with timestamp . Since IndexIds with timestamp does not have a timestamp less than IndexId with timestamp i.e. 2 is not less than 2 IndexId is removed from S and returned to the priority queue at step .

Assume that S contains IndexIds and . Suppose now that a new crawl with timestamp has re indexed DocId generating IndexId with IndexTimestamp . DocId thus has changed for the third time. Now there are no indexes in U that are not in S with timestamps less than or equal to timestamp i.e. the only index is U not in S is 3 and 3 is greater than 2 thus the set of indexes IndexId and is a valid set of indexes to be merged and processing continues at step .

To merge indexes the selected rows from the index rows for the smallest keyword in the set of all keywords in all the indexes is selected. For that keyword the DocId and Occurrence information from all the indexes that contain that keyword is merged. Then the next keyword in ascending order is selected to merge. When multiple rows from one or more indexes having the same keyword is merged the DocIdList from each of the index rows is retrieved and then the set of DocIdLists in merged order of DocIds is iterated over. For each DocId if the DocId is the most recent in the index set S the DocId does not have to be the most recent according to DocIdMap then that DocId and its associated occurrence information is inserted into the DocIdList of the merged index. Otherwise that DocId data is skipped.

Next DocId is processed step . DocId is present in IndexId . Innovation can be found at word locations for DocId row column second list . At step IndexTimestamp row column of FtIndexDictionary for IndexId is determined. IndexTimestamp of IndexID is TS 1 . The timestamp of IndexId is compared to MaxTimestamp . Because IndexId IndexTimestamp and MaxTimestamp are not the same DocIdMap T is accessed to see if DocId was recrawled between timestamp the timestamp of the IndexId and timestamp MaxTimestamp and DocIdMap Infinity is checked to see if DocId was deleted.

At step Partial DocIdMap is checked to see if an entry exists for DocId in DocIdMap where timestamp T is greater than the IndexTimestamp for IndexId and less than or equal to MaxTimestamp or timestamp T infinity. Thus DocIdMap entry is checked for the presence of DocId . DocId is not found in DocIdMap entry because DocId was not changed after CrawlId created IndexId . Because DocId was not changed between timestamp and timestamp IndexId contains the most recent data for DocId and DocIdMap does not contain DocId . Because DocId was not deleted DocIdMap Infinity does not contain DocId . IndexId thus contains the most recent data in the merge set for DocId and is written to MergeIndex M at step .

This process continues for all DocIds in a DocIdList for all keywords. When the last DocId for the last keyword has been processed the timestamp of the MergeIndex M is set to MaxTimestamp and the index is persisted and placed in the priority queue.

Thus it can be seen that in the case of homogenous merges no table lookups are required while in the case of heterogeneous merges the following is true 

If a data item such as a document is crawled in the latest crawl participating in a merge then there is a lookup for that document for every keyword that is deleted from the document from the previous crawls. In this case the cost is proportional to the product of the number of distinct keywords removed from the document and logarithm of the number of documents changed entries in Partial DocIdMap . Once a heterogeneous merge with maximum timestamp T is performed T can be defined as the timestamp of the last full crawl and thus Partial DocIdMap can be collapsed.

If a document did not get crawled then there is a lookup for every distinct keyword in that document. Cost of the lookup is proportional to the number of distinct keywords in the document and is the logarithm of the number of documents changed.

The cost of the lookup in both cases is proportional to the logarithm of the number of documents that got crawled in the latest crawl or got deleted after the previous crawl.

Thus the cost of the lookup is smallest if either the newest crawl is a full crawl in which case no lookups are done and the cost is 0 or most of the documents have been re crawled in the new crawl with most of the old keywords still in the text occurrence might have changed and any number of new keywords may have been added or very few documents have been re crawled. The crawls and merges preferably can be controlled to fit the existing situation to minimize cost and maximize efficiency.

It is noted that the foregoing examples have been provided merely for the purpose of explanation and are in no way to be construed as limiting of the present invention. While the invention has been described with reference to various embodiments it is understood that the words which have been used herein are words of description and illustration rather than words of limitation. Further although the invention has been described herein with reference to particular means materials and embodiments the invention is not intended to be limited to the particulars disclosed herein rather the invention extends to all functionally equivalent structures methods and uses such as are within the scope of the appended claims. Those skilled in the art having the benefit of the teachings of this specification may effect numerous modifications thereto and changes may be made without departing from the scope and spirit of the invention in its aspects.

