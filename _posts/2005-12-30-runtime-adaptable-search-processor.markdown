---

title: Runtime adaptable search processor
abstract: A runtime adaptable search processor is disclosed. The search processor provides high speed content search capability to meet the performance need of network line rates growing to 1 Gbps, 10 Gbps and higher. The search processor provides a unique combination of NFA and DFA based search engines that can process incoming data in parallel to perform the search against the specific rules programmed in the search engines. The processor architecture also provides capabilities to transport and process Internet Protocol (IP) packets from Layer 2 through transport protocol layer and may also provide packet inspection through Layer 7. Further, a runtime adaptable processor is coupled to the protocol processing hardware and may be dynamically adapted to perform hardware tasks as per the needs of the network traffic being sent or received and/or the policies programmed or services or applications being supported. A set of engines may perform pass-through packet classification, policy processing and/or security processing enabling packet streaming through the architecture at nearly the full line rate. A high performance content search and rules processing security processor is disclosed which may be used for application layer and network layer security. Scheduler schedules packets to packet processors for processing. An internal memory or local session database cache stores a session information database for a certain number of active sessions. The session information that is not in the internal memory is stored and retrieved to/from an additional memory. An application running on an initiator or target can in certain instantiations register a region of memory, which is made available to its peer(s) for access directly without substantial host intervention through RDMA data transfer. A security system is also disclosed that enables a new way of implementing security capabilities inside enterprise networks in a distributed manner using a protocol processing hardware with appropriate security features.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07685254&OS=07685254&RS=07685254
owner: 
number: 07685254
owner_city: 
owner_country: 
publication_date: 20051230
---
This invention relates generally to content search storage and networking semiconductors and in particular to high performance content search network storage and security processors that can be used within networking storage security bioinformatics chipsets servers search engines and the like.

Many modern applications depend on fast information search and retrieval. With the advent of the world wide web and the phenomenal growth in its usage content search has become a critical capability. A large number of servers get deployed in web search applications due to the performance limitations of the state of the art microprocessors for regular expression driven search.

There have been significant research and development resources devoted to the topic of searching of lexical information or patterns in strings. Regular expressions have been used extensively since the mid 1950s to describe the patterns in strings for content search lexical analysis information retrieval systems and the like. Regular expressions were first studied by S. C. Kleene in mid 1950s to describe the events of nervous activity. It is well understood in the industry that regular expression RE can also be represented using finite state automata FSA . Non deterministic finite state automaton NFA and deterministic finite state automaton DFA are two types of FSAs that have been used extensively over the history of computing. Rabin and Scott were the first to show the equivalence of DFA and NFA as far as their ability to recognize languages in 1959. In general a significant body of research exists on regular expressions. Theory of regular expressions can be found in Introduction to Automata Theory Languages and Computation by Hopcroft and Ullman and a significant discussion of the topics can also be found in book Compilers Principles Techniques and Tools by Aho Sethi and Ullman.

Internet protocol IP is the most prevalent networking protocol deployed across various networks like local area networks LANs metro area networks MANs and wide area networks WANs . Storage area networks SANs are predominantly based on Fibre Channel FC technology. There is a need to create IP based storage networks.

When transporting block storage traffic on IP designed to transport data streams the data streams are transported using Transmission Control Protocol TCP that is layered to run on top of IP. TCP IP is a reliable connection session oriented protocol implemented in software within the operating systems. TCP IP software stack is very slow to handle the high line rates that will be deployed in future. Currently a 1 GHz processor based server running TCP IP stack with a 1 Gbps network connection would use 50 70 or more of the processor cycles leaving minimal cycles available for the processor to allocate to the applications that run on the server. This overhead is not tolerable when transporting storage data over TCP IP as well as for high performance IP networks. Hence new hardware solutions would accelerate the TCP IP stack to carry storage and network data traffic and be competitive to FC based solutions. In addition to the TCP protocol. other protocols such as SCTP and UDP protocols can be used as well as other protocols appropriate for transporting data streams.

Computers are increasingly networked within enterprises and around the world. These networked computers are changing the paradigm of information management and security. Vast amounts of information including highly confidential personal and sensitive information is now being generated accessed and stored over the network which information needs to be protected from unauthorized access. Further there is a continuous onslaught of spam viruses and other inappropriate content on the users through email web access instant messaging web download and other means resulting in significant loss of productivity and resources.

Enterprise and service provider networks are rapidly evolving from 10 100 Mbps line rates to 1 Gbps 10 Gbps and higher line rates. Traditional model of perimeter security to protect information systems pose many issues due to the blurring boundary of an organization s perimeter. Today as employees contractors remote users partners and customers require access to enterprise networks from outside a perimeter security model is inadequate. This usage model poses serious security vulnerabilities to critical information and computing resources for these organizations. Thus the traditional model of perimeter security has to be bolstered with security at the core of the network. Further the convergence of new sources of threats and high line rate networks is making software based perimeter security to stop the external and internal attacks inadequate. There is a clear need for enabling security processing in hardware inside core or end systems beside a perimeter firewall as one of the prominent means of security to thwart ever increasing security breaches and attacks.

FBI and other leading research institutions have reported in recent years that over 70 of intrusions in organizations have been internal. Hence a perimeter defense relying on protecting an organization from external attacks is not sufficient as discussed above. Organizations are also required to screen outbound traffic to prevent accidental or malicious disclosure of proprietary and confidential information as well as to prevent its network resources from being used to proliferate spam viruses worms and other malware. There is a clear need to inspect the data payloads of the network traffic to protect and secure an organization s network for inbound and outbound security.

Data transported using TCP IP or other protocols is processed at the source the destination or intermediate systems in the network or a combination thereof to provide data security or other services like secure sockets layer SSL for socket layer security Transport layer security encryption decryption RDMA RDMA security application layer security virtualization or higher application layer processing which may further involve application level protocol processing for example protocol processing for HTTP HTTPS XML SGML Secure XML other XML derivatives Telnet FTP IP Storage NFS CIFS DAFS and the like . Many of these processing tasks put a significant burden on the host processor that can have a direct impact on the performance of applications and the hardware system. Hence some of these tasks need to be accelerated using dedicated hardware for example SSL or TLS acceleration. As the usage of XML increases for web applications it is expected to put a significant performance burden on the host processor and would also benefit significantly from hardware acceleration. Detection of spam viruses and other inappropriate content require deep packet inspection and analysis. Such tasks can put huge processing burden on the host processor and can substantially lower network line rate. Hence deep packet content search and analysis hardware is also required.

Internet has become an essential tool for doing business at small to large organizations. HTML based static web is being transformed into a dynamic environment over last several years with deployment of XML based services. XML is becoming the lingua franca of the web and its usage is expected to increase substantially. XML is a descriptive language that offers many advantages by making the documents self describing for automated processing but is also known to cause huge performance overhead for best of class server processors. Decisions can be made by processing the intelligence embedded in XML documents to enable business to business transactions as well as other information exchange. However due to the performance overload on the best of class server processors from analyzing XML documents they cannot be used in systems that require network line rate XML processing to provide intelligent networking. There is a clear need for acceleration solutions for XML document parsing and content inspection at network line rates which are approaching 1 Gbps and 10 Gbps to realize the benefits of a dynamic web based on XML services.

Regular expressions can be used to represent the content search strings for a variety of applications like those discussed above. A set of regular expressions can then form a rule set for searching for a specific application and can be applied to any document or stream of data for examination of the same. Regular expressions are used in describing anti spam rules anti virus rules XML document search constructs and the like. These expressions get converted into NFAs or DFAs for evaluation on a general purpose processor. However significant performance and storage limitations arise for each type of the representation. For example an N character regular expression can take up to the order of 2memory for the states of a DFA while the same for an NFA is in the order of N. On the other hand the performance for the DFA evaluation for an M byte input data stream is in the order of M memory accesses and the order of N M processor cycles for the NFA representation on modern microprocessors.

When the number of regular expressions increases the impact on the performance deteriorates as well. For example in an application like anti spam there may be hundreds of regular expression rules. These regular expressions can be evaluated on the server processors using individual NFAs or DFAs. It may also be possible to create a composite DFA to represent the rules. Assuming that there are X REs for an application then a DFA based representation of each individual RE would result up to the order of X 2 states however the evaluation time would grow up to the order of X N memory cycles. Generally due to the potential expansion in the number of states for a DFA they would need to be stored in off chip memories. Using a typical access time latency of main memory systems of 100 ns it would require about X 100 ns N M time to process an X RE DFA with N states over an M byte data stream. This can result in tens of Mbps performance for modest size of X N M. Such performance is obviously significantly below the needs of today s network line rates of 1 Gbps to 10 Gbps. On the other hand if a composite DFA is created it can result in an upper bound of storage in the order of 2 which may not be within physical limits of memory size for typical commercial computing systems even for a few hundred REs. Thus the upper bound in memory expansion for DFAs can be a significant issue. Then on the other hand NFAs are non deterministic in nature and can result in multiple state transitions that can happen simultaneously. NFAs can only be processed on a state of the art microprocessor in a scalar fashion resulting in multiple executions of the NFA for each of the enabled paths. X REs with N characters on average can be represented in the upper bound of X N states as NFAs. However each NFA would require M iterations for an M byte stream causing an upper bound of X N M processor cycles per loop . Assuming the number of processing cycles are in the order of 10 cycles then for a best of class processor at 4 GHz the processing time can be around X N M 2.5 ns which for a nominal N of 8 and X in tens can result in below 100 Mbps performance. There is a clear need to create high performance regular expression based content search processors which can provide the performance in line with the network rates which are going to 1 Gbps and 10 Gbps.

The methods for converting a regular expression to NFA and DFA are well known. The resulting automata are able to distinguish whether a string belongs to the language defined by the regular expression however it is not very efficient to figure out if a specific sub expression of a regular expression is in a matching string or the extent of the string. Tagged NFAs enable such queries to be conducted efficiently without having to scan the matching string again. For a discussion on Tagged NFA please refer to the paper NFAs with Tagged Transitions their Conversion to Deterministic Automata and Application to Regular Expressions by Ville Laurikari Helsinki University of Technology Finland.

U.S. Patent Applications 20040059443 and 20050012521 describe a method and apparatus for efficient implementation and evaluation of state machines and programmable finite state automata. These applications show an apparatus that is used to evaluate regular expressions using an array of NFAs to create high performance processing of regular expressions. The application recognizes the upper bound in the storage issues for DFAs as a reason to implement regular expressions using NFAs. However the applications fails to recognize that even though the DFA worst case storage requirement is substantially higher compared to NFAs many DFAs have less storage needs than NFAs. DFAs for many regular expressions can result in lower number of states compared to an NFA. For example in an anti spam application based on the open source tool SpamAssassin a large number of the regular expression rules result in DFAs which are smaller than NFAs. Hence it is important not to ignore DFA implementation based only on the worst case scenario. These patent applications also create NFA engines that process a single RE per NFA block. Thus if a RE uses fewer states than the minimum states of the NFA block there is no provision to be able to use multiple REs simultaneously in the same block. In my invention I describe a content search processor which uses an array of runtime adaptable search engines where the search engines may be runtime adaptable DFA search engines or runtime adaptable NFA search engines or a combination thereof to evaluate regular expressions. Content search engine of my search processor also provides flexibility of using multiple REs per NFA or DFA engine. My invention also provides capabilities to support Tagged NFA implementations which are not supported or discussed in these applications. Further these applications do not address the need of dynamically configuring the hardware or the rules being applied based on the transported data being sent to or received from a network. The processors of my invention can be dynamically adapted to apply hardware based rule sets dependent on the transported data which is not described in the above applications. Further my invention shows that certain DFAs can be more hardware resource efficient to implement compared to NFAs and can enable today s state of the art FPGAs to implement a large number of regular expressions without having to devote large investments in creating application specific integrated circuits using advanced process technologies. This is also specifically discussed as not feasible to do in the above applications. My invention also shows content search acceleration can be used to improve application acceleration through content search application programmer interface API and the search processor of this invention.

Hardware acceleration for each type of network data payload can be expensive when a specialized accelerator is deployed for each individual type of network data. There is a clear need for a processor architecture that can adapt itself to the needs of the network data providing the necessary acceleration and thereby reduce the impact on the host performance. This patent describes such a novel architecture which adapts itself to needs of the network data. The processor of this patent can be reused and adapted for differing needs of the different types of the payload and still offer the benefits of hardware acceleration. This can have a significant reduction in the cost of the acceleration solutions deployment compared to dedicated application specific accelerators.

Dynamically reconfigurable computing has been an area that has received significant research and development interest to address the need of reconfiguring hardware resources to suit application needs. The primary focus of the research has been towards creating general purpose microprocessor alternatives that can be adapted with new instruction execution resources to suit application needs.

Field programmable gate arrays FPGA have evolved from simple AND OR logic blocks to more complex elements that provide a large number of programmable logic blocks and programmable routing resources to connect these together or to Input Output blocks. U.S. Pat. No. 5 600 845 describes an integrated circuit computing device comprising a dynamically configurable FPGA. The gate array is configured to create a RISC processor with a configurable instruction execution unit. This dynamic re configurability allows the dynamically reconfigurable instruction execution unit to be changed to implement operations in hardware which may be time consuming to run in software. Such an arrangement requires a preconfigured instruction set to execute the incoming instruction and if an instruction is not present it has to be treated as an exception which then has a significant processing overhead. The invention in U.S. Pat. No. 5 600 845 addresses the limitation of general purpose microprocessors but does not address the need of dynamically configuring the hardware based on the transported data being sent to or received from a network.

U.S. Patent Application number 20030097546 describes a reconfigurable processor which receives an instruction stream that is inspected by a instruction test module to decide if the instruction is supported by existing non reconfigurable hardware or the reconfigurable hardware configured by a software routine and executes the instruction stream based on the test result. If the instruction is not supported then the processor decides a course of action to be taken including executing the instruction stream in software. The patent application number 20030097546 also does not address the need of dynamically configuring the hardware based on the transported data being sent to or received from a network.

U.S. Patent Application number 20040019765 describes a pipelined reconfigurable dynamic instruction set processor. In that application dynamically reconfigurable pipeline stages under control of a microcontroller are described. This is yet another dynamically reconfigurable processor that can adapt its pipeline stages and their interconnections based on the instructions being processed as an alternative to general purpose microprocessors.

The field of reconfigurable computing has been ripe with research towards creating dynamically reconfigurable logic devices either as FPGAs or reconfigurable processors as described above as primarily addressing the limitations of general purpose processors by adding reconfigurable execution units or reconfigurable coprocessors. For example Reconfigurable FPGA processor diploma thesis paper by Andreas Romer from Swiss Federal Institute of Technology targets the need of creating an ASIC like performance and area but general purpose processor level flexibility by dynamically creating execution functional units in a reconfigurable part of a reconfigurable FPGA like Xilinx Virtex and XC6200 devices. Similarly the paper by J. R. Hauser and J Wawrzynek entitled published in Proceedings of the IEEE Symposium on FPGAs for Custom Computing Machines FCCM 97 targets the need for creating custom co processing support to a MIPS processor addressing the limitations of the general purpose processing capabilities of the MIPS processor.

Published research or patent applications have not addressed the need of dynamically configuring the hardware based on transported data as well as actions to be taken and applications services to be deployed for that specific data being sent to or received from a network. This patent describes a novel architecture which adapts itself to the needs of the network data and is run time adaptable to perform time consuming security policy operations or application services or other data processing needs of the transported data and defined policies of the system incorporating this invention. The architecture also comprises a deep packet inspection engine that may be used for detecting spam viruses digital rights management information instant message inspection URL matching application detection malicious content and other content and applying specific rules which may enable anti spam anti virus and the like capabilities.

I describe a high performance run time adaptable search processor using hardware acceleration for regular expressions. The regular expressions are converted into equivalent DFAs and NFAs and then the most cost effective solution is chosen. The content search processor comprises of a set of DFA processing engines and NFA processing engines. The converted REs are then mapped either to an appropriate NFA or DFA engine. The processor may also include a set of composite DFA processing engines which can be used to absorb the growth in the number of rules beyond the number supported by the array of FSA engines. Thus new system hardware may not be necessary until the composite DFA results in significant memory usage which is beyond that available with the memory associated with the content search processor.

In many content search applications like security there is a need to constantly update the rules or the signatures being used to detect malicious traffic. In such applications it is critical that a solution be adaptable to keep up with the constantly evolving nature of the security threat. In an always connected type of usage models it is extremely important to have the latest security threat mitigation rules updated in the security system on a frequent basis. When a composite DFA type architecture is used compiling and releasing any new security rules or policy can consume a large amount of time where the updates may not be timely to avoid the impact of the security threat. In such environments the release of new rule base may take up to 8 to 24 hours which is quite delayed response to constantly evolving threat. In the processor of this invention that issue is addressed since the release of new rules is a matter of converting those rules into NFAs and DFAs and updating only these very small rules into the content search processor. Thus the response to new threats can be immediate and would not require huge delays which occur from integration of the new rules in the composite rule base and converting those into composite DFAs.

There are several instances of REs which include only a few states. For example if the content search includes looking for .exe or .com or .html or the like the NFA or DFAs for these REs include a small number of states. Thus if all DFA or NFA engines support say 16 states then it may be possible to include multiple rules per engine. This invention enables the maximum utilization of the FSA engines by allowing multiple rules per FSA engine. The engines also provide FSA extension logic to chain the base engines together to create super blocks that can handle larger FSAs.

Berry and Sethi in their paper From Regular Expressions to Deterministic Automata Published in Theoretical Computer Science in 1986 showed that regular expressions can be represented by NFAs such that a given state in the state machine is entered by one symbol unlike the Thompson NFA. Further the Berry Sethi NFAs are free. A V term RE can be represented using V 1 states NFA using Berry Sethi like NFA realization method. The duality of Berry Sethi method also exists where all transitions that lead the machine out of a state are dependent on the same symbol. This is shown in the paper A Taxonomy of finite automata construction algorithms by Bruce Watson published in 1994 in section 4.3. I show a method of creating NFA search engine architecture leveraging the principles of Berry Sethi s NFA realization and the dual of their construct. The NFA search engine is programmable to realize an arbitrary regular expression.

In this invention I also show how the content search processor of this invention can be used to create general application acceleration in a compute device like a server personal computer workstation or the like. I show an example content search API which can be used as a general facility that may get offered by an operating system for those devices to applications running on them which can utilize the content search processor and significantly improve the performance of those applications compared to having them run on the general purpose processor of these devices.

An example application of anti spam is illustrated in this application which can be accelerated to become a high line rate application unlike current solutions which run on general purpose processor of the compute servers on which they run.

I also illustrate an example of using the content search processor coupled with a protocol processor such that the content search processing can be done in line with the traffic and appropriate actions be taken dependent on the traffic content. The processor of this invention can thus be used to apply content specific search rules for traffic stream. For example if a specific packet or stream of packets carry SMTP traffic then the protocol processor can let the content search processor know that and provide the appropriate flow information. Then the content search processor retrieves the flow context for the current flow from the memory and retrieves the SMTP application rules context from the application memory associated with the search engines. The search engines get configured to process the content of the specific flow and its associated application context. Thus content search can continue across multiple packets of the same flow even when packets in the flow arrive with significant time gap between them and multiple different application rule contexts can also be applied. By using such an architecture a significant performance benefit can result compared to architectures where the rule context can not be changed rapidly and where the context may need to be brought in from global memory into each of the FSA engines.

I also describe a high performance hardware processor that sharply reduces the TCP IP protocol stack overhead from host processor and enables a high line rate storage and data transport solution based on IP.

This patent also describes the novel high performance processor that sharply reduces the TCP IP protocol stack overhead from the host processor and enables high line rate security processing including firewall encryption decryption intrusion detection and the like. This patent also describes a content inspection architecture that may be used for detecting spam viruses digital rights management information instant message inspection URL matching application detection malicious content and other content and applying specific rules which may enable anti spam anti virus and the like capabilities. The content inspection engine may be used for detecting and enforcing digital rights management rules for the content. The content inspection engine may also be used for URL matching string searches content based load balancing sensitive information search like credit card numbers or social security numbers or health information or the like. The content inspection engine results may be used to direct the operation of the run time adaptable processor as well.

This patent also describes a novel processor architecture that is run time adaptable to the needs of the data sent to or received from a network. The run time adaptable features of this processor can be used to deploy services that operate on network data under control of user definable policies. The adaptable processor may also be used to dynamically offload compute intensive operations from the host processor when not performing operations on the network data or in conjunction with network data processing if enough adaptable hardware resources are available. The processor performs protocol processing like TCP IP or SCTP or UDP or the like using the high performance protocol processor disclosed and then uses an adaptable processing hardware to provide other functions or services like socket layer security Transport layer security encryption decryption RDMA RDMA security application layer security content inspection deep packet inspection virus scanning or detection policy processing content based switching load balancing content based load balancing virtualization or higher application layer processing or a combination thereof. Higher layer processing may further involve application level protocol processing for example protocol processing for HTTP HTTPS XML SGML Secure XML other XML derivatives Telnet FTP IP Storage NFS CIFS DAFS and the like which may also be accelerated by dynamically adapting or reconfiguring the processor of this patent. This can significantly reduce the processing overhead on the host processor of the target system without adding major system cost of adding dedicated accelerator hardware.

The processing capabilities of a system deploying the runtime adaptable processor of this patent can continue to expand and improve without the need for continually upgrading the system with host processor to achieve performance benefits. The hardware of the processor may comprise computational elements organized into compute clusters. Computational elements may provide logical and arithmetic operations beside other functions. A computational element may operate on 1 bit 2 bit 4 bit 8 bit or n bit data sizes as may be chosen by the implementation. Thus multiple computational elements may together provide the desired size of operators. For example if each computational element provides operations on a largest data size of 8 bits then to operate on 32 bit operands four computational elements may each operate on a byte of the 32 bit operand. The computational elements within the compute clusters can be programmatically interconnected with each other using a dynamically changeable or adaptable interconnection network. The compute clusters may also be dynamically interconnected with each other programmatically forming an adaptable network. Thus arbitrary interconnections can be created between the array of computational elements within a compute cluster as well as outside of the compute clusters. The computational elements may each be dynamically changed to provide necessary function s or operation s by programmatically selecting and connecting the necessary logic blocks through muxes or other logic blocks or other means. The computational elements may also be simple processors with ALU and other functional blocks. In this case to change hardware function of the computational element CE it is programmatically instructed to execute certain function s or operation s . The operation s selected for each of the computational element can be different. Thus the processor hardware of this patent can be dynamically i.e. during operation or at runtime be changed or adapted to provide a different functionality. For explanation purposes let us take an example of a compute cluster with 8 computational elements each providing 8 bit operations. If we want to perform two 32 bit operations like a 32 bit addition followed by an n bit shift operation then the computational elements may be grouped into two sets of four each. The first group would be programmed to provide addition operation where each of them may operate on 8 bits at a time. The appropriate carry and flags and other outputs would be available through the interconnections between the CEs which may be programmatically selected. The second group of CEs would be programmed to provide a shift operation on the incoming data. One such setup of the CEs may be called an Avatar or a virtual configuration setup. The CEs may then continue to provide these operations on the input operands for a period of time that this avatar is maintained. Then it is possible to dynamically change the avatar to a new avatar. For instance in the example used above let us assume that after a certain time period which may be as small as a clock period or multiple clock periods or other period the processor needs to switch from providing acceleration support from 32 bit Add followed by Shift to something like two 16 bit Subtraction followed by two 16 bit logical AND. In such an instance the hardware is setup to form four groups of two CEs each group operating on 16 bit operands. First four CEs in this case may now be dynamically switched or changed from providing addition function to subtraction function. Further they may now be dynamically switched to operate in two groups to provide 16 bit operation instead of one group providing 32 bit operation in the previous avatar. Similarly the second group of four CEs from the previous avatar may now be dynamically switched or changed to provide logical AND operations and may also be setup as two groups providing 16 bit operations. This forms a new avatar of the hardware which has been dynamically changed as per the need of the required functionality at the time. Thus the runtime adaptable protocol processor of this patent can change its functions at a fine granularity along with the interconnections of these operators provided by the CEs to form runtime changeable or adaptable hardware platform. The operations supported may be lot more complex than those used in the examples discussed above. The examples were provided primarily to provide a better appreciation of the capabilities and were deliberately kept simplistic. Though the examples suggested a unidirectional flow this is not to be construed as the only mode of operation. The outputs from the operations in the examples above could be recycled to the first group of CEs which would allow a pipelined loop of the hardware. More complex scenarios are feasible with the dynamically adaptable nature of the CEs and the interconnection network where different stages of CEs may be switched over a period of time to provide different functionality as may be required by the algorithm or application or service being enabled in hardware. Pipelined stages of operations are thus possible with arbitrary loop backs as necessary. Hence the applications or services being accelerated or supported in hardware can increase over time where the users may decide to accelerate applications of choice by mapping them appropriately to the runtime adaptable protocol processor as and when necessary or feasible due to cost performance resources application discovery application development or any other reasons that may cause them to invent or develop new applications or services. The hardware system may thus be adapted to use such applications without the need for incurring costs of buying new systems or accelerators and the like. The system capabilities can be increased over time as new services are developed and or deployed that exploit the adaptable component of the processor of this invention. The new services or policies or a combination thereof may be deployed to the appropriate systems over a network under user control.

Traditionally TCP IP networking stack is implemented inside the operating system kernel as a software stack. The software TCP IP stack implementation consumes as mentioned above more than 50 of the processing cycles available in a 1 GHz processor when serving a 1 Gbps network. The overhead comes from various aspects of the software TCP IP stack including checksum calculation memory buffer copy processor interrupts on packet arrival session establishment session tear down and other reliable transport services. The software stack overhead becomes prohibitive at higher lines rates. Similar issues occur in networks with lower line rates like wireless networks that use lower performance host processors. A hardware implementation can remove the overhead from the host processor.

The software TCP IP networking stack provided by the operating systems uses up a majority of the host processor cycles. TCP IP is a reliable transport that can be run on unreliable data links. Hence when a network packet is dropped or has errors TCP does the retransmission of the packets. The errors in packets are detected using checksum that is carried within the packet. The recipient of a TCP packet performs the checksum of the received packet and compares that to the received checksum. This is an expensive compute intensive operation performed on each packet involving each received byte in the packet. The packets between a source and destination may arrive out of order and the TCP layer performs ordering of the data stream before presenting it to the upper layers. IP packets may also be fragmented based on the maximum transfer unit MTU of the link layer and hence the recipient is expected to de fragment the packets. These functions result in temporarily storing the out of order packets fragmented packets or unacknowledged packets in memory on the network card for example. When the line rates increase to above 1 Gbps the memory size overhead and memory speed bottleneck resulting from these add significant cost to the network cards and also cause huge performance overhead. Another function that consumes a lot of processor resources is the copying of the data to from the network card buffers kernel buffers and the application buffers.

Microprocessors are increasingly achieving their high performance and speed using deep pipelining and super scalar architectures. Interrupting these processors on arrival of small packets will cause severe performance degradation due to context switching overhead pipeline flushes and refilling of the pipelines. Hence interrupting the processors should be minimized to the most essential interrupts only. When the block storage traffic is transported over TCP IP networks these performance issues become critical severely impacting the throughput and the latency of the storage traffic. Hence the processor intervention in the entire process of transporting storage traffic needs to be minimized for IP based storage solutions to have comparable performance and latency as other specialized network architectures like fibre channel which are specified with a view to a hardware implementation. Emerging IP based storage standards like iSCSI FCIP iFCP and others like NFS CIFS DAFS HTTP XML XML derivatives such as Voice XML EBXML Microsoft SOAP and others SGML and HTML formats encapsulate the storage and data traffic in TCP IP segments. However there usually isn t alignment relationship between the TCP segments and the protocol data units that are encapsulated by TCP packets. This becomes an issue when the packets arrive out of order which is a very frequent event in today s networks. The storage and data blocks cannot be extracted from the out of order packets for use until the intermediate packets in the stream arrive which will cause the network adapters to store these packets in the memory retrieve them and order them when the intermediate packets arrive. This can be expensive from the size of the memory storage required and also the performance that the memory subsystem is expected to support particularly at line rates above 1 Gbps. This overhead can be removed if each TCP segment can uniquely identify the protocol data unit and its sequence. This can allow the packets to be directly transferred to their end memory location in the host system. Host processor intervention should also be minimized in the transfer of large blocks of data that may be transferred to the storage subsystems or being shared with other processors in a clustering environment or other client server environment. The processor should be interrupted only on storage command boundaries to minimize the impact.

The IP processor set forth herein eliminates or sharply reduces the effect of various issues outlined above through innovative architectural features and the design. The described processor architecture provides features to terminate the TCP traffic carrying the storage and data payload thereby eliminating or sharply reducing the TCP IP networking stack overhead on the host processor resulting in packet streaming architecture that allows packets to pass through from input to output with minimal latency. To enable high line rate storage or data traffic being carried over IP requires maintaining the transmission control block information for various connections sessions that are traditionally maintained by host kernel or driver software. As used in this patent the term IP session means a session for a session oriented protocol that runs on IP. Examples are TCP IP SCTP IP and the like. Accessing session information for each packet adds significant processing overhead. The described architecture creates a high performance memory subsystem that significantly reduces this overhead. The architecture of the processor provides capabilities for intelligent flow control that minimizes interrupts to the host processor primarily at the command or data transfer completion boundary.

The conventional network security model deployed today involves perimeter security in the form of perimeter firewall and intrusion detection systems. However as increasing amount of business gets conducted on line there is a need to provide enterprise network access to trusted insiders employees partners customers and contractors from outside. This creates potential threats to the information assets inside an enterprise network. Recent research by leading firms and FBI found that over 70 percent of the unauthorized access to information systems is committed by employees or trusted insiders and so are over 95 percent of intrusions that result in substantial financial loss. In an environment where remote access servers peer networks with partners VPN and wireless access points blur the boundary of the network a perimeter security is not sufficient. In such an environment organizations need to adopt an integrated strategy that addresses network security at all tiers including at the perimeter gateways servers switches routers and clients instead of using point security products at the perimeter.

Traditional firewalls provide perimeter security at network layers by keeping offending IP addresses out of the internal network. However because many new attacks arrive as viruses or spam exploiting known vulnerabilities of well known software and higher level protocols it is desirable to develop and deploy application layer firewalls. These should also be distributed across the network instead of being primarily at the perimeter.

Currently as the TCP IP processing exists as the software stack in clients servers and other core and end systems the security processing also is done in software particularly the capabilities like firewall intrusion detection and prevention. As the line rates of these networks go to 1 Gbps and 10 Gbps it is imperative that the TCP IP protocol stack be implemented in hardware because a software stack consumes a large portion of the available host processor cycles. Similarly if the security processing functions get deployed on core or end systems instead of being deployed only at the perimeter the processing power required to perform these operations may create a huge overhead on the host processor of these systems. Hence software based distributed security processing would increase the required processing capability of the system and increase the cost of deploying such a solution. A software based implementation would be detrimental to the performance of the servers and significantly increase the delay or latency of the server response to clients and may limit the number of clients that can be served. Further if the host system software stack gets compromised during a network attack it may not be possible to isolate the security functions thereby compromising network security. Further as the TCP IP protocol processing comes to be done in hardware the software network layer firewalls may not have access to all state information needed to perform the security functions. Hence the protocol processing hardware may be required to provide access to the protocol layer information that it processes and the host may have to redo some of the functions to meet the network firewall needs.

The hardware based TCP IP and security rules processing processor of this patent solves the distributed core security processing bottleneck besides solving the performance bottleneck from the TCP IP protocol stack. The hardware processor of this patent sharply reduces the TCP IP protocol stack processing overhead from the host CPU and enables security processing features like firewall at various protocol layers such as link network and transport layers thereby substantially improving the host CPU performance for intended applications. Further this processor provides capabilities that can be used to perform deep packet inspection to perform higher layer security functions using the programmable processor and the classification policy engines disclosed. This patent also describes a content inspection architecture that may be used for detecting spam viruses digital rights management information instant message inspection URL matching application detection malicious content and other content and applying specific rules which may enable anti spam anti virus and the like security and content inspection and processing capabilities. The processor of this patent thus enables hardware TCP IP and security processing at all layers of the OSI stack to implement capabilities like firewall at all layers including the network layer and application layers.

The processor architecture of this patent also provides integrated advanced security features. This processor allows for in stream encryption and decryption of the network traffic on a packet by packet basis thereby allowing high line rates and at the same time offering confidentiality of the data traffic. Similarly when the storage traffic is carried on a network from the server to the storage arrays in a SAN or other storage system it is exposed to various security vulnerabilities that a direct attached storage system does not have to deal with. This processor allows for in stream encryption and decryption of the storage traffic thereby allowing high line rates and at the same time offering confidentiality of the storage data traffic.

Classification of network traffic is another task that consumes up to half of the processing cycles available on packet processors leaving few cycles for deep packet inspection and processing. IP based storage traffic by the nature of the protocol requires high speed low latency deep packet processing. The described IP processor significantly reduces the classification overhead by providing a programmable classification engine. The programmable classification engine of this patent allows deployment of advanced security policies that can be enforced on a per packet per transaction and per flow basis. This will result in significant improvement in deploying distributed enterprise security solutions in a high performance and cost effective manner to address the emerging security threats from within the organizations.

To enable the creation of distributed security solutions it is critical to address the need of Information Technology managers to cost effectively manage the entire network. Addition of distributed security without means for ease of managing it can significantly increase the management cost of the network. The disclosure of this patent also provides a security rules policy management capability that can be used by IT personnel to distribute the security rules from a centralized location to various internal network systems that use the processor of this patent. The processor comprises hardware and software capabilities that can interact with centralized rules management system s . Thus the distribution of the security rules and collection of information of compliance or violation of the rules or other related information like offending systems users and the like can be processed from one or more centralized locations by IT managers. Thus multiple distributed security deployments can be individually controlled from centralized location s .

This patent also provides means to create a secure operating environment for the protocol stack processing that even if the host system gets compromised either through a virus or malicious attack allows the network security and integrity to be maintained. This patent significantly adds to the trusted computing environment needs of the next generation computing systems.

Tremendous growth in the storage capacity and storage networks have created storage area management as a major cost item for IT departments. Policy based storage management is required to contain management costs. The described programmable classification engine allows deployment of storage policies that can be enforced on packet transaction flow and command boundaries. This will have significant improvement in storage area management costs.

The programmable IP processor architecture also offers enough headroom to allow customer specific applications to be deployed. These applications may belong to multiple categories e.g. network management storage firewall or other security capabilities bandwidth management quality of service virtualization performance monitoring zoning LUN masking and the like.

The adaptable processor hardware may be used to accelerate many of the applications or services listed above based on the available reprogrammable resources deployed applications services policies or a combination thereof.

I provide a new high performance content search processor that can relieve the performance bottleneck of content search from the host processor. The search processor of this patent is used to perform a large number of regular expression searches in parallel using NFA and DFA based search engines. The search processor can perform high speed content search at line rates from 1 Gbps to 10 Gbps and higher when the best of class server microprocessor can only perform the same tasks at well below 100 Mbps. The content search processor can be used not only to perform layer 2 through layer 4 searches that may be used for classification and security applications it can also be used to perform deep packet inspection and layer 4 through layer 7 content analysis and security applications.

I provide a new high performance and low latency way of implementing a TCP IP stack in hardware to relieve the host processor of the severe performance impact of a software TCP IP stack. This hardware TCP IP stack is then interfaced with additional processing elements to enable high performance and low latency IP based storage applications.

This system also enables a new way of implementing security capabilities like firewall inside enterprise networks in a distributed manner using a hardware TCP IP implementation with appropriate security capabilities in hardware having processing elements to enable high performance and low latency IP based network security applications. The hardware processors may be used inside network interface cards of servers workstations client PCs notebook computers handheld devices switches routers and other networked devices. The servers may be web servers remote access servers file servers departmental servers storage servers network attached storage servers database servers blade servers clustering servers application servers content media servers grid computers servers and the like. The hardware processors may also be used inside an I O chipset of one of the end systems or network core systems like a switch or router or appliance or the like.

This system enables distributed security capabilities like firewall intrusion detection virus scan virtual private network confidentiality services and the like in internal systems of an enterprise network. The distributed security capabilities may be implemented using the hardware processor of this patent in each system or some of its critical systems and others may deploy those services in software. Hence overall network will include distributed security as hardware implementation or software implementation or a combination thereof in different systems depending on the performance cost and security needs as determined by IT managers. The distributed security systems will be managed from one or more centralized systems used by IT managers for managing the network using the principles described. This will enable an efficient and consistent deployment of security in the network using various elements of this patent.

This can be implemented in a variety of forms to provide benefits of TCP IP termination high performance and low latency IP storage capabilities remote DMA RDMA capabilities security capabilities programmable classification policy processing features runtime adaptable processing high speed content search and the like. Following are some of the embodiments that can implement this 

The described architecture may be embodied in a high performance server environment providing hardware based TCP IP functions or hardware TCP IP and security or search functions or a combination of the foregoing that relieve the host server processor or processors of TCP IP and or security and or search software and their performance overhead. Any processor of this invention may be a companion processor to a server chipset providing the high performance networking interface with hardware TCP IP and or security and or content search. Servers can be in various form factors like blade servers appliance servers file servers thin servers clustered servers database server content search server game server grid computing server VoIP server wireless gateway server security server network attached storage server or traditional servers. The current embodiment would allow creation of a high performance network interface on the server motherboard.

Further the described runtime adaptable protocol processor or security processor or search processor or a combination of the foregoing architectures may also be used to provide additional capabilities or services beside protocol processing like socket layer security Transport layer security encryption decryption RDMA RDMA security application layer security virtualization or higher application layer processing which may further involve application level protocol processing for example protocol processing for HTTP HTTPS XML SGML Secure XML other XML derivatives Telnet FTP IP Storage NFS CIFS DAFS and the like . One embodiment could include TCP IP protocol processing using the dedicated protocol processor and XML acceleration mapped to a runtime adaptable processor such as that disclosed in this patent. The protocol processor may or may not provide RDMA capabilities dependent upon the system needs and the supported line rates. Security processing capabilities of this invention may also be optionally incorporated in this embodiment. The same architecture could also be used to provide security acceleration support to XML data processing on the runtime adaptable processor of this patent.

The server environment may also leverage the high performance IP storage processing capability of the described processor besides high performance TCP IP and or RDMA capabilities. In such an embodiment the processor may be a companion processor to a server chipset providing high performance network storage I O capability besides the TCP IP offloading from the server processor. This embodiment would allow creation of high performance IP based network storage I O on the motherboard. In other words it would enable IP SAN on the motherboard.

Similar to the Server embodiment described above this embodiment may also leverage the runtime adaptable processor of this patent to provide adaptable hardware acceleration along with protocol processing support in a server chipset. The runtime adaptable processor can be configured to provide storage services like virtualization security services multi pathing protocol translation and the like. Protocol translation may included for example translation to from FibreChannel protocol to IP Storage protocol or vice versa Serial ATA protocol to IP Storage or FibreChannel protocol or vice versa Serial Attached SCSI protocol to IP Storage or FibreChannel protocol or vice versa and the like.

Further the runtime adaptable search processor of this patent may provide content search acceleration capability to a server chipset similar to the server embodiment described above. The content search processor may be used for a vast variety of applications that use regular expressions for the search rules. The applications cover a spectrum of areas like bioinformatics application and network layer security proteomics genetics web search protocol analysis XML acceleration data warehousing and the like.

The processor may also be used as a companion of a chipset in a storage system which may be a storage array or some other appropriate storage system or subsystem controller which performs the storage data server functionality in a storage networking environment. The processor would provide IP network storage capability to the storage array controller to network in an IP based SAN. The configuration may be similar to that in a server environment with additional capabilities in the system to access the storage arrays and provide other storage centric functionality.

This embodiment may also leverage the runtime adaptable processor of this patent to provide adaptable hardware acceleration along with protocol processing support in a storage system chipset. The runtime adaptable processor can be configured to provide storage services like virtualization security services multi pathing protocol translation and the like. Protocol translation may included for example translation to from FibreChannel protocol to IP Storage protocol or vice versa Serial ATA protocol to IP Storage or FibreChannel protocol or vice versa Serial Attached SCSI protocol to IP Storage or FibreChannel protocol or vice versa and the like. The runtime adaptable processor may also be used to provide acceleration for storage system metadata processing to improve the system performance.

This embodiment may also leverage the runtime adaptable search processor of this patent to provide high speed search of storage traffic for various applications like confidential information policy enforcement virus malware detection high speed indexing for ease of searching virtualization content based switching security services and the like.

The IP processor may also be embedded in a server host adapter card providing high speed TCP IP networking. The same adapter card may also be able to offer high speed network security capability for IP networks. Similarly the adapter card may also be able to offer high speed network storage capability for IP based storage networks. The adapter card may be used in traditional servers and may also be used as blades in a blade server configuration. The processor may also be used in adapters in a storage array or other storage system or subsystem front end providing IP based storage networking capabilities. The adapter card may also leverage the runtime adaptable processors of this patent in a way similar to that described above.

The TCP IP processor may be embodied inside a processor chipset providing the TCP IP offloading capability. Such a configuration may be used in the high end servers workstations or high performance personal computers that interface with high speed networks. Such an embodiment could also include IP storage or RDMA capabilities or combination of this invention to provide IP based storage networking and or TCP IP with RDMA capability embedded in the chipset. The usage of multiple capabilities of the described architecture can be made independent of using other capabilities in this or other embodiments as a trade off of feature requirements development timeline and cost silicon die cost and the like. The processor chipset may also incorporate the runtime adaptable processors of this patent to offer a variable set of functions on demand by configuring the processor for the desired application.

The IP processor may also be used to create high performance low latency IP SAN switching system or other storage system or subsystem line cards. The processor may be used as the main processor terminating and originating IP based storage traffic to from the line card. This processor would work with the switching system fabric controller which may act like a host to transport the terminated storage traffic based on their IP destination to the appropriate switch line card as determined by the forwarding information base present in the switch system. Such a switching system may support purely IP based networking or may support multi protocol support allow interfacing with IP based SAN along with other data center SAN fabrics like Fibre channel. A very similar configuration could exist inside a gateway controller system that terminates IP storage traffic from LAN or WAN and originates new sessions to carry the storage traffic into a SAN which may be IP based SAN or more likely a SAN built from other fabrics inside a data center like Fibre channel. The processor could also be embodied in a SAN gateway controller. These systems would use security capabilities of this processor to create a distributed security network within enterprise storage area networks as well.

The runtime adaptable processor of this patent can be very effective in providing hardware acceleration capabilities individually or in combination as described above like protocol translation virtualization security bandwidth management rate limiting grooming policy based management and the like

The runtime adaptable search processor of this patent can be used to provide deep content inspection capability in this embodiment which can be used to created intelligent networking capabilities like content based switching application oriented networking and the like.

The processor may also be embedded in a network interface line card providing high speed TCP IP networking for switches routers gateways wireless access points and the like. The same adapter card may also be able to offer high speed network security capability for IP networks. This processor would provide the security capabilities that can then be used in a distributed security network.

The runtime adaptable processor of this patent may also be used in such embodiments offering services and capabilities described above as well as others like Wired Equivalent Privacy security capabilities RADIUS and like security features as needed by the environment. The runtime adaptable processor may also be used to provide dynamically changeable protocol processing capability besides TCP IP processing to support wireless protocols like Bluetooth HomeRF wireless Ethernet LAN protocols at various line rates 3GPP GPRS GSM or other wireless LAN or RF or cellular technology protocols or any combinations thereof.

The runtime adaptable search processor of this patent can be used to provide deep content inspection capability in this embodiment which can be used to created intelligent networking capabilities like content based switching application oriented networking and the like.

Storage networks management costs are increasing rapidly. The ability to manage the significant growth in the networks and the storage capacity would require creating special appliances which would be providing the storage area management functionality. The described management appliances for high performance IP based SAN would implement my high performance IP processor to be able to perform its functions on the storage traffic transported inside TCP IP packets. These systems would require a high performance processor to do deep packet inspection and extract the storage payload in the IP traffic to provide policy based management and enforcement functions. The security programmable classification and policy engines along with the high speed TCP IP and IP storage engines described would enable these appliances and other embodiments described in this patent to perform deep packet inspection and classification and apply the policies that are necessary on a packet by packet basis at high line rates at low latency. Further these capabilities can enable creating storage management appliances that can perform their functions like virtualization policy based management security enforcement access control intrusion detection bandwidth management traffic shaping quality of service anti spam virus detection encryption decryption LUN masking zoning link aggregation and the like in band to the storage area network traffic. Similar policy based management and security operations or functionality may also be supported inside the other embodiments described in this patent. The runtime adaptable processor of this patent can be used to dynamically support or accelerate one or more of the applications services. The services applications supported may be selected by the policies in existence under the influence or control of the user or the administrator. The runtime adaptable search processor of this invention can be used to perform high speed network line rate searches in this embodiment which can be used for several deep packet inspection content search and security applications discussed above.

Server systems are used in a clustered environment to increase the system performance and scalability for applications like clustered data bases and the like. The applications running on high performance cluster servers require ability to share data at high speeds for inter process communication. Transporting this inter process communication traffic on a traditional software TCP IP network between cluster processors suffers from severe performance overhead. Hence specialized fabrics like Fibre channel have been used in such configurations. However a TCP IP based fabric which can allow direct memory access between the communicating processes memory can be used by applications that operate on any TCP IP network without being changed to specialized fabrics like fibre channel. The described IP processor with its high performance TCP IP processing capability and the RDMA features can be embodied in a cluster server environment to provide the benefits of high performance and low latency direct memory to memory data transfers. This embodiment may also be used to create global clustering and can also be used to enable data transfers in grid computers and grid networks. The processor of this patent may also be used to accelerate local cluster data transfers using light weight protocols other than TCP IP to avoid the latency and protocol processing overhead. The runtime adaptable processor architecture can be leveraged to support such a light weight protocol. Thus the same processor architecture may be used for local as well as global clustering and enable data transfers in grid computers and grid networks. The programmable processor of this patent may also be used for similar purposes without burdening the runtime adaptable processor. The processor architecture of this patent can thus be used to enable utility computing. The runtime adaptable processors of this patent may also be used to provide the capabilities described in other embodiments above to the clustered environment as well.

The runtime adaptable TCP IP processor of this patent can also be used as a component inside a system or adapter card or as part of a chipset providing TCP IP protocol termination or XML acceleration or a combination thereof. As web services usage increases more and more web documents may start using XML or XML derivatives. The burden of processing XML on each web page access can be very significant on the host processors requiring additional hardware support. The runtime adaptable processor of this patent can be used in such an environment to provide acceleration to XML processing whereas transport protocol processing is handled by the dedicated protocol processor of this patent. XML documents may also need security support in which case the processor can be dynamically configured to provide security acceleration for secure XML documents. Similarly the runtime adaptable search processor of this invention can also be used in such an embodiment and provide high speed XML processing capability to meet 1 Gbps 10 Gbps and higher line rates. The search processor of this invention allows multiple thousand regular expression rules to be simultaneously evaluated for this application and can thus be used in networking systems also to provide intelligent networking and application oriented networking capabilities.

The processor of this patent can also be embedded inside voice over IP appliances like VoIP phones servers gateways handheld devices and the like. The protocol processor can be used to provide IP protocol processing as well as the transport layer protocol processing as needed in the VoIP environment. Further the runtime adaptable processor may be dynamically adapted to provide signal processing and DSP hardware acceleration capabilities that may be required for VoIP appliance and the applications running on the appliance. The runtime adaptable search processor of this application can be used for speech recognition applications beside the VoIP applications similar to the discussion above.

The processor of this patent may also be used to provide protocol processing hardware capability to processors or chipsets of handheld devices phones personal digital assistants and the like. The protocol processor along with the runtime adaptable processor may provide many of the capabilities described above for many of the embodiments. The processor of this patent may be used to create a secure protocol processing stack inside these devices as well as provide other services using hardware acceleration. The runtime adaptable processor may be used to enable the handheld devices to network in a wired or wireless manner. The device can then be dynamically adapted to work with a multitude of protocols like Bluetooth Wireless Ethernet LAN RF GPRS GSM CDMA CMDA variants or other 3G cellular technology or other wireless or cellular or RF technologies by using the protocol processor and the runtime adaptable processor of this patent. The runtime adaptable search processor of this patent may also be embodied in handheld devices to accelerate the search performance of the already low performance processors. The search processor of this application may be incorporated in a chipset or a processor of the handheld device to accelerate the search performance of the applications running on these platforms. The search processor may also be used as a multi purpose security processor offering cryptographic security network layer security functions through application layer security that may be embodied in a chipset or the host processor or as a companion processor in the handheld devices to protect against the emerging security threats for a variety of handheld devices.

The processor architecture can be partially implemented in software and partially in hardware. The performance needs and cost implications can drive trade offs for hardware and software partitioning of the overall system architecture of this invention. It is also possible to implement this architecture as a combination of chip sets along with the hardware and software partitioning or independent of the partitioning. For example the security processor and the classification engines could be on separate chips and provide similar functions. This can result in lower silicon cost of the IP processor including the development and manufacturing cost but it may in some instances increase the part count in the system and may increase the footprint and the total solution cost. Security and classification engines could be separate chips as well. As used herein a chip set may mean a multiple chip chip set or a chip set that includes only a single chip depending on the application.

The storage flow controller and the queues could be maintained in software on the host or may become part of another chip in the chipset. Hence multiple ways of partitioning this architecture are feasible to accomplish the high performance IP based storage and TCP IP offload applications that will be required with the coming high performance processors in the future. The storage engine description has been given with respect to iSCSI however with TCP IP and storage engine programmability classifier programmability and the storage flow controller along with the control processor other IP storage protocols like iFCP FCIP and others can be implemented with the appropriate firmware. iSCSI operations may also represent IP Storage operations. The high performance IP processor core may be coupled with multiple input output ports of lower line rates matching the total throughput to create multi port IP processor embodiment as well.

It is feasible to use this architecture for high performance TCP IP offloading from the main processor without using the storage engines. This can result in a silicon and system solution for next generation high performance networks for the data and telecom applications. The TCP IP engine can be augmented with application specific packet accelerators and leverage the core architecture to derive new flavors of this processor. It is possible to change the storage engine with another application specific accelerator like a firewall engine or a route look up engine or a telecom network acceleration engine along with the other capabilities of this invention and target this processor architecture for telecom networking and other applications.

The runtime adaptable search processor of this invention may also be embodied inside a variety of application specific systems targeted towards security or content search application or the like. For example the search processor may be embodied inside an email security appliance that provides capabilities like anti spam anti virus anti spyware anti worms other malware prevention as well as confidential information security and other regulatory compliance for regulations like Sarbanes Oxley Gramm Leach Bliley HIPAA and the like. The search processor may also be embodied in a system that may be used for extrusion detection and prevention systems to protect against confidential information leaks and other regulatory compliance like those listed above at an enterprise edge.

Ability to perform content search has become a critical capability in the networked world. As the network line rates go up to 1 Gbps 10 Gbps and higher it is important to be able to perform deep packet inspection for many applications at line rate. Several security issues like viruses worms confidential information leaks and the like can be detected and prevented from causing damage if the network traffic can be inspected at high line rates. In general content search rules can be represented using regular expressions. Regular expression rules can be represented and computed using FSAs. NFAs and DFAs are the two types of FSAs that are used for evaluation of regular expressions. For high line rate applications a composite DFA can be used where each character of the input stream can be processed per cycle of memory access. However this does have a limit on how fast the search can be performed dictated by the memory access speed. Another limiter of such approach is the amount of memory required to search even a modest number of regular expression rules. As discussed above NFAs also have their limitations to achieve high performance on general purpose processors. In general today s best of class microprocessors can only achieve less than 100 Mbps performance using NFAs or DFAs for a small number of regular expressions. Hence there is a clear need to create targeted content search processor hardware to raise the performance of the search to the line rates of 1 Gbps and 10 Gbps. This invention shows such a high performance content search processor that can be targeted for high line rates.

Storage costs and demand have been increasing at a rapid pace over the last several years. This is expected to grow at the same rate in the foreseeable future. With the advent of e business availability of the data at any time and anywhere irrespective of the server or system downtime is critical. This is driving a strong need to move the server attached storage onto a network to provide storage consolidation availability of data and ease of management of the data. The storage area networks SANs are today predominantly based on Fibre Channel technology that provide various benefits like low latency and high performance with its hardware oriented stacks compared to TCP IP technology.

Some system transport block storage traffic on IP designed to transport data streams. The data streams are transported using Transmission Control Protocol TCP that is layered to run on top of IP. TCP IP is a reliable connection oriented protocol implemented in software within the operating systems. A TCP IP software stack is slow to handle the high line rates that will be deployed in the future. New hardware solutions will accelerate the TCP IP stack to carry storage and network traffic and be competitive to FC based solutions.

The prevalent storage protocol in high performance servers workstations and storage controllers and arrays is SCSI protocol which has been around for 20 years. SCSI architecture is built as layered protocol architecture. illustrates the various SCSI architecture layers within an initiator block and target subsystems block . As used in patent the terms initiator and target mean a data processing apparatus or a subsystem or system including them. The terms initiator and target can also mean a client or a server or a peer. Likewise the term peer can mean a peer data processing apparatus or a subsystem or system thereof. A remote peer can be a peer located across the world or across the room.

The initiator and target subsystems in interact with each other using the SCSI application protocol layer block which is used to provide a client server request and response transactions. It also provides device service request and response between the initiator and the target mass storage device which may take many forms like a disk arrays tape drives and the like. Traditionally the target and initiator are interconnected using the SCSI bus architecture carrying the SCSI protocol block . The SCSI protocol layer is the transport layer that allows the client and the server to interact with each other using the SCSI application protocol. The transport layer must present the same semantics to the upper layer so that the upper layer protocols and application can stay transport protocol independent.

The IP based storage protocol like iSCSI can be layered in software on top of a software based TCP IP stack. However such an implementation would suffer serious performance penalties arising from software TCP IP and the storage protocol layered on top of that. Such an implementation would severely impact the performance of the host processor and may make the processor unusable for any other tasks at line rates above 1 Gbps. Hence we would implement the TCP IP stack in hardware relieving the host processor on which the storage protocol can be built. The storage protocol like iSCSI can be built in software running on the host processor or may as described in this patent be accelerated using hardware implementation. A software iSCSI stack will present many interrupts to the host processor to extract PDUs from received TCP segments to be able to act on them. Such an implementation will suffer severe performance penalties for reasons similar to those for which a software based TCP stack would. The described processor provides a high performance and low latency architecture to transport Storage protocol on a TCP IP based network that eliminates or greatly reduces the performance penalty on the host processor and the resulting latency impact.

Similarly a control command can use the transmit path whereas the received response would use the receive path. Similar engines can exist on the initiator as well as the target. The data flow direction is different depending on whether it is the initiator or the target. However primarily similar data flow exists on both initiator and target with additional steps at the target. The target needs to perform additional operations to reserve the buffers needed to get the data of a write command for instance or may need to prepare the read data before the data is provided to the initiator. Similar instances would exist in case of an intermediate device although in such a device which may be a switch or an appliance some level of virtualization or frame filtering or such other operation may be performed that may require termination of the session on one side and originating sessions on the other. This functionality is supported by this architecture but not illustrated explicitly in this figure inasmuch as it is well within the knowledge of one of ordinary skill in the art.

The block diagram in illustrates input queue and output queue blocks and as two separate blocks. The functionality may be provided using a combined block. The input queue block consists of the logic control and storage to retrieve the incoming packets from the MAC interface block. Block queues the packets as they arrive from the interface and creates appropriate markers to identify start of the packet end of the packet and other attributes like a fragmented packet or a secure packet and the like working with the packet scheduler and the classification engine . The packet scheduler can retrieve the packets from the input queue controller and passes them for classification to the classification engine. The classification block is shown to follow the scheduler however from a logical perspective the classification engine receives the packet from the input queue classifies the packet and provides the classification tag to the packet which is then scheduled by the scheduler to the processor array . . . . Thus the classification engine can act as a pass through classification engine sustaining the flow of the packets through its structure at the full line rate. The classification engine is a programmable engine that classifies the packets received from the network in various categories and tags the packet with the classification result for the scheduler and the other packet processors to use. Classification of the network traffic is a very compute intensive activity which can take up to half of the processor cycles available in a packet processor. This integrated classification engine is programmable to perform Layer 2 through Layer 7 inspection. The fields to be classified are programmed in with expected values for comparison and the action associated with them if there is a match. The classifier collects the classification walk results and can present these as a tag to the packet identifying the classification result as seen subsequently with respect to . This is much like a tree structure and is understood as a walk. The classified packets are then provided to the scheduler as the next phase of the processing pipeline.

The packet scheduler block includes a state controller and sequencer that assign packets to appropriate execution engines on the disclosed processor. The execution engines are the SAN packet processors block through including the TCP IP and or storage engines as well as the storage flow RDMA controller block or host bypass and or other appropriate processors depend on the desired implementation. For clarity the term when used to designate hardware components in this patent can mean and or as appropriate. For example the component storage flow RDMA controller can be a storage flow and RDMA controller a storage flow controller or an RDMA controller as appropriate for the implementation. The scheduler also maintains the packet order through the processor where the state dependency from a packet to a packet on the same connection session is important for correct processing of the incoming packets. The scheduler maintains various tables to track the progress of the scheduled packets through the processor until packet retirement. The scheduler also receives commands that need to be scheduled to the packet processors on the outgoing commands and packets from the host processor or switch fabric controller or interface.

The TCP IP and storage engines along with programmable packet processors are together labeled as the SAN Packet Processors through in . These packet processors are engines that are independent programmable entities that serve a specific role. Alternatively two or more of them can be implemented as a single processor depending on the desired implementation. The TCP IP engine of and the storage engines of are configured in this example as coprocessors to the programmable packet processor engine block of . This architecture can thus be applied with relative ease to applications other than storage by substituting removing for the storage engine for reasons of cost manufacturability market segment and the like. In a pure networking environment the storage engine could be removed leaving the packet processor with a dedicated TCP IP engine and be applied for the networking traffic which will face the same processing overhead from TCP IP software stacks. Alternatively one or more of the engines may be dropped for desired implementation e.g. for processor supporting only IP Storage functions may drop TCP IP engine and or packet engine which may be in a separate chip. Hence multiple variations of the core scalable and modular architecture are possible. The core architecture can thus be leveraged in applications beside the storage over IP applications by substituting the storage engine with other dedicated engines for example a high performance network security and policy engine a high performance routing engine a high performance network management engine deep packet inspection engine providing string search an engine for XML an engine for virtualization and the like providing support for an application specific acceleration. The processing capability of this IP processor can be scaled by scaling the number of SAN Packet Processor blocks through in the chip to meet the line rate requirements of the network interface. The primary limitation from the scalability would come from the silicon real estate required and the limits imposed by the silicon process technologies. Fundamentally this architecture is scalable to very high line rates by adding more SAN packet processor blocks thereby increasing the processing capability. Other means of achieving a similar result is to increase the clock frequency of operation of the processor to that feasible within the process technology limits.

The processor block diagram of also illustrates host interface block host input queue block and host output queue block as well as the storage flow RDMA controller block . These blocks provide the functions that are required to transfer data to and from the host also called peer memory or switch fabric. These blocks also provide features that allow the host based drivers to schedule the commands retrieve incoming status retrieve the session database entry program the disclosed processor and the like to enable capabilities like sockets direct architecture full TCP IP termination IP storage offload and the like capabilities with or without using RDMA. The host interface controller seen in greater detail in provides the configuration registers DMA engines for direct memory to memory data transfer the host command block that performs some of the above tasks along with the host interface transaction controller and the host interrupt controller. The host input and output queues provide the queuing for incoming and outgoing packets. The storage flow and RDMA controller block provides the functionality necessary for the host to queue the commands to the disclosed processor which then takes these commands and executes them interrupting the host processor on command termination. The RDMA controller portion of block provides various capabilities necessary for enabling remote direct memory access. It has tables that include information such as RDMA region access keys and virtual address translation functionality. The RDMA engine inside this block performs the data transfer and interprets the received RDMA commands to perform the transaction if the transaction is allowed. The storage flow controller of block also keeps track of the state of the progress of various commands that have been scheduled as the data transfer happens between the target and the initiator. The storage flow controller schedules the commands for execution and also provides the command completion information to the host drivers. The above can be considered RDMA capability and can be implemented as described or by implementing as individual processors depending on designer s choice. Also additional functions can be added to or removed from those described without departing from the spirit or the scope of this patent.

The control plane processor block of this processor is used to provide relatively slow path functionality for TCP IP and or storage protocols which may include error processing with ICMP protocol name resolution address resolution protocol and it may also be programmed to perform session initiation teardown acting as a session controller connection manger login and parameter exchange and the like. This control plane processor could be off chip to provide the system developer a choice of the control plane processor or may be on chip to provide an integrated solution. If the control plane processor is off chip then an interface block would be created or integrated herein that would allow this processor to interface with the control plane processor and perform data and command transfers. The internal bus structures and functional block interconnections may be different than illustrated for all the detailed figures for performance die cost requirements and the like and not depart from the spirit and the scope of this patent.

Capabilities described above for blocks with more detail below enable a packet streaming architecture that allows packets to pass through from input to output with minimal latency with in stream processing by various processing resources of the disclosed processor.

If the classifier is implemented before the scheduler as discussed above with respect to where the classification engine receives the packet from the input queue items and would be in the classifier or may not be needed depending on the particular design. The appropriate coupling from the classifier to from the scheduler blocks and may be created in such a scenario and the classifier coupled directly to the input queue block of .

In applications in which storage is done on a chip not including the TCP IP processor of by as one example an IP Storage processor such as an iSCSI processor of the TCP IP Interface would function as an interface to a scheduler for scheduling IP storage packet processing by the IP Storage processor. Similar variations are well within the knowledge of one of ordinary skill in the art viewing the disclosure of this patent.

The foregoing may also be considered a part of an RDMA capability or an RDMA mechanism or an RDMA function.

This is the high level flow primarily followed by most commands from the initiator to the target when the connection has been established between an initiator and a target.

Some of these steps are provided in more details in where a secure packet flow is represented where as the represents a clear text read packet flow. This flow and other flows illustrated in this patent are applicable to storage and non storage data transfers by using appropriate resources of the disclosed processor that a person with ordinary skill in the art will be able to do with the teachings of this patent.

The flow in illustrates clear text data transfer. If the data transfer needs to be secure the flow is similar to that illustrated in where the output data packet is routed through the secure packet as illustrated by arrows labeled and . The input R2T packet if secure would also be routed through the security engine this is not illustrated in the figure .

Prior to getting the session in the established state as in step 13 the control plane processor may be required to perform a full login phase of the storage protocol exchanging parameters and recording them for the specific connection if this is a storage data transfer connection. Once the login is authenticated and parameter exchange complete does the session enter the session establishment state shown in step 13 above.

The session establishment may also involve the login phase which is not illustrated in the . However the login phase and the parameter exchange occur before the session enters the fully configured and established state. These data transfers and handshake may primarily be done by the control processor. Once these steps are taken the remaining steps in the flow above may be executed.

The discussion above of the flows is an illustration of some the major flows involved in high bandwidth data transfers. There are several flows like fragmented data flow error flows with multiple different types of errors name resolution service flow address resolution flows login and logout flows and the like are not illustrated but are supported by the IP processor of this invention.

As discussed in the description above the perimeter security model is not sufficient to protect an enterprise network from security threats due to the blurring boundary of enterprise networks. Further a significant number of unauthorized information access occurs from inside. The perimeter security methods do not prevent such security attacks. Thus it is critical to have security deployed across the network and protect the network from within as well as the perimeter. The network line rates inside enterprise networks are going to 1 Gbps multi Gbps and 10 Gbps in the LANs and SANs. As previously mentioned distributed firewall and security methods require a significant processing overhead on each of the system host CPU if implemented in software. This overhead can cause increase in latency of the response of the servers reduce their overall throughput and leave fewer processing cycles for applications. An efficient hardware implementation that can enable deployment of software driven security services is required to address the issues outlined above. The processor of this patent addresses some of these key issues. Further at high line rates it is critical to offload the software based TCP IP protocol processing from the host CPU to protocol processing hardware to reduce impact on the host CPU. Thus the protocol processing hardware should provide the means to perform the security functions like firewall encryption decryption VPN and the like. The processor provides such a hardware architecture that can address the growing need of distributed security and high network line rates within enterprise networks.

The central manager works with each node in the security network to collect events or reports of enforcement statistics violations and the like using the event and report collection management engine . The event report collection engine works with the security monitoring engine to create the event and information report databases and which keep a persistent record of the collected information. The security monitoring engine analyzes the reports and events to check for any violations and may in turn inform the IT managers about the same. Depending on the actions to be taken when violations occur the security monitoring engine may create policy or rule updates that may be redistributed to the nodes. The security monitoring engine works with the security policy manager interface and policy update engine for getting the updates created and redistributed. The security policy manager interface provides tools to the IT manager to do event and information record searches. The IT manager may be able to develop new rules or security policy updates based on the monitored events or other searches or changes in the organizations policies and create the updates to the policies. These updates get compiled by the security policy compiler and redistributed to the network. The functionality of security policy manager interface and policy update engine may be provided by the security policy developer interface based on an implementation choice. Such regrouping of functionality and functional blocks is possible without diverging from the teachings of this patent. The security monitoring engine the security policy manager interface and the event report collection management interface may also be used to manage specific nodes when there are violations that need to be addressed or any other actions need to be taken like enabling a node for security disabling a node changing the role of a node changing the configuration of a node starting stopping deploying applications on a node or provisioning additional capacity or other management functions or a combination thereof as appropriate for the central manager to effectively manage the network of the nodes.

TCP IP packet processor engines of block are similar to the TCP IP processor engine of SAN packet processor of blocks through . The connection session memory block provides the functionality of IP session cache memory of block whereas the connection manager and control plane processor of block provide the session controller and control plane processor functionality similar to that of blocks and . The RDMA controller block provides RDMA functionality similar to the block . The memory controller block provides memory interface similar to that provided by memory controller of block . The TCP IP processor may have external memory which may be SRAM DRAM FLASH ROM EEPROM DDR SDRAM RDRAM FCRAM QDR SRAM Magnetic RAM or Magnetic memory or other derivatives of static or dynamic random access memory or a combination thereof. Host Fabric Network Interface block provides the interface to a host bus or a switch fabric interface or a network interface depending on the system in which this processor is being incorporated. For example in a server or server adapter environment the block would provide a host bus interface functionality similar to that of block where the host bus may be a PCI bus PCI X PCI Express or other PCI derivatives or other host buses like AMBA bus or RapidIO bus or HyperTransport or other derivatives. A switch or a router or a gateway or an appliance with a switch fabric to connect multiple line cards would have appropriate fabric interface functionality for block . This may include queues with priority mechanisms to avoid head of the line blocking fragmentation and defragmentation circuitry as needed by the switch fabric and appropriate flow control mechanism to ensure equitable usage of the switch fabric resources. In case of an environment like a gateway or appliance that connects to a network on ingress and egress the block would provide network interface functionality similar to the block .

The TCP IP processor illustrated in is a version of the architecture shown in and as is evident from the description above. The TCP IP processor engines of block may be substituted with SAN packet processors of block through and the two architectures would offer the same functionality. Thus can be looked at as a different view and or grouping of the architecture illustrated in and . The TCP IP processor engines may be augmented by the packet engine block of the SAN packet processors to provide programmable processing where additional services can be deployed besides protocol processing on a packet by packet basis. Block of illustrated as a dotted line around a group of blocks is called TCP IP processor core in this patent. The RDMA block is shown to be part of the TCP IP Processor core although it is an optional block in certain TCP IP processor core embodiments like low line speed applications or applications that do not support RDMA. Similarly the security engine may also not be present depending on the implementation chosen and the system embodiment.

The TCP IP processor core may provide a transport layer RDMA capability as described earlier. The TCP IP processor core may also provide security functions like network layer security transport layer security socket layer security application layer security or a combination thereof besides wire speed encryption and decryption capabilities. Thus the TCP IP processor core may also provide a secure TCP IP stack in hardware with several functions described above implemented in hardware. Even though the description of the adaptable TCP IP processor has been with the TCP IP processor core as illustrated in this application the TCP IP processor core may have various other architectures. Beside the architecture disclosed in this patent the TCP IP processor core could also be a fixed function implementation or may be implemented as a hardware state machine or may support partial protocol offloading capability for example support fast path processing in hardware where control plane processing as well as session management and control may reside in a separate control plane processor or host processor or a combination of various architecture alternatives described above. The TCP IP processor core architecture chosen may also include functions for security or RDMA or a combination thereof. Further the adaptable TCP IP processor architecture can be used for protocols other than TCP IP like SCTP UDP or other transport layer protocols by substituting the TCP IP processor core with a protocol appropriate processor core. This would enable creating an adaptable protocol processor targeted to the specific protocol of interest. The runtime adaptable processor of such a processor would be able to function similarly to the description in this patent and offer hardware acceleration for similar applications services by using its dynamic adaptation capabilities.

The runtime adaptable processor block provides a dynamically changeable hardware where logic and interconnect resources can be adapted programmatically on the fly to create virtual hardware implementations as appropriate to the need of the application service. The adaptation controller block may be used to dynamically update the RAP block. The adaptation controller may interface with the host processor or control plane processor or the TCP IP processor or a combination thereof to decide when to switch the configuration of RAP block to create a new avatar or incarnation to support needed hardware function s what function s should this avatar of RAP block perform where to fetch the new avatar how long is the avatar valid when to change the avatar as well as provide multiple simultaneous function support in the RAP block. The RAP block may be dynamically switched from one avatar to another avatar depending on the analysis done in TCP IP processor core. For instance the TCP IP processor core may have a programmed policy that will ask it to flag any data payload received that may contain XML data and pass the extracted data payload for processing through the RAP instead of sending it directly to the host processor. In this instance when a packet is received that contains XML data the TCP IP processor core may tag the data appropriately and either queue the packets in the external memory for further processing by RAP or pass the data in the data buffers of block for further processing by RAP. The TCP IP processor core may be coupled to a RAP interface block which may provide the functionality needed for the TCP IP processor core to interface with the RAP and the adaptation controller block. This functionality may be directly part of RAP or adaptation controller or the TCP IP processor core. The RAP interface would inform the adaptation controller in this instance of the arrival of XML traffic so the adaptation controller can fetch the appropriate configuration from configuration memory block which may be internal or external memory or a combination thereof. The adaptation controller can then provide the configuration to RAP block and enable it when the XML data is ready to be operated on and is ready in the data buffers or external memory for the RAP to fetch it. Similarly depending on the policies that may be programmed in the TCP IP processor core and the received network traffic the RAP block may get configured into a new hardware avatar to support the specific application service or function or a combination thereof dynamically based on the characteristics of the received traffic and the policies.

The TCP IP processor core may also choose to pass the received data to the host processor without passing it for further processing through the RAP depending on the policies and or the nature of the data received. Thus if hardware configurations for specific operations or functions or policies or applications have not been realized because the operations or policies or functions or applications may not be used often and hence do not cause performance issues or resources have not been assigned to develop the acceleration support for cost reasons or any other business or other reasons those operations may be performed on the host. As those operations are realized as a runtime adaptable configuration it may be provided to the adaptation controller so it can configure the RAP block for that operation as needed dynamically. The TCP IP processor would also be informed to then identify such operations and pass them through RAP block. Using such a technique over a period of time more applications can be accelerated without the need for changing or adding any hardware accelerators. The deployment of new policies or applications or services or operations on the runtime adaptable processor may be under the user or administrator control using very similar mechanisms as those shown for the security policy deployment and management. Thus a central administrator can efficiently deploy new configurations to systems with the runtime adaptable protocol processor of this patent as and when needed. Similarly the user or the administrator could remove or change the RAP supported functions or policies or applications as the need or the usage of the system changes. For example a system using runtime adaptable protocol processor of this patent may initially be used for XML traffic however its usage may change to support voice over IP application and XML acceleration may not be required but instead some other voice over IP acceleration is needed. In such an instance the user or the administrator may be able to change add or remove selectable hardware supported configurations from the specific system or systems. The central manager policy server flow central manager flow rule distribution flow control plane processor policy driver flows and the like illustrated in through are applicable to the management deployment change monitoring and the like for the runtime adaptable configurations as well with appropriate changes similar to those explained as follows. The runtime adaptable configuration creation flow may be added to the security policy creation flow for example. New configurations may become available from another vendor and the user may just need to select the configuration of interest to be deployed. The configuration distribution flow may be similar to the rule distribution flow where the policies for the support of the configuration s may be distributed to the TCP IP processor core blocks where as the configuration may be distributed to the adaptation controller of the system of interest or to a driver or an configuration control process on the host system or a combination thereof. Thus the runtime adaptable protocol processor systems may be integrated well into other enterprise management systems when used in that environment. The application or policy or service or operation configurations may be distributed by other means for example as a software update over the network or through mass storage devices or other means. The foregoing description is one way of providing the updates in one usage environment but there can multiple other ways to do the same for each embodiment and the usage environment as one skilled in the art can appreciate and hence should not be viewed as limited to the description above.

The adaptation controller may also be required to configure the RAP block to operate on data being sent out to the network. In such a case the RAP block may be required to operate on the data before it is passed on to TCP IP processor core to send it to the intended recipient over the network. For example it may be necessary to perform secure socket layer SSL operations on the data before being encapsulated in the transport and network headers by the TCP IP processor core. The host driver or the application that is sending this data would inform the adaptation controller of the operation to be performed on the data before being passed on to the TCP IP processor core. This can happen through the direct path from the host fabric interface to the adaptation controller . The adaptation controller can then configure RAP block or a part of it to perform the operation requested dynamically and let RAP operate on the data. Once RAP operation is completed it can inform the adaptation controller of the operation completion which can then work with the TCP IP processor core to send this data enroute to its destination after appropriate protocol processing header encapsulation and the like by the TCP IP protocol processor. RAP may pass the processed data to the TCP IP processor core through data buffers of block or by queuing them in memory using the memory interface block . Thus the runtime adaptable TCP IP processor of this patent can be configured to operate on incoming as well as outgoing data before or after processing by the TCP IP processor core.

Runtime adaptable processor may be restricted in size or capabilities by physical cost performance power or other constraints. RAP extension interface block may also be provided on the adaptable TCP IP processor to interface RAP block to one or more external components providing runtime adaptable processor functionality. Thus the solution can be scaled to bigger size or features or capabilities using the RAP extension interface . RAP extension interface comprises of all the necessary control routing data memory interface buses and connections as needed to seamlessly extend the RAP into one or more external components.

The runtime adaptable processor of can be configured such that computation array may be split into partial regions where each region may be configured to perform a specific hardware operation. For example clusters may form one region whereas clusters through M may form another region and some other clusters may form yet another region. Some of the regions may be interconnected as pipelined stages as may be required by the hardware function being mapped onto the runtime adaptable processor. Regions of the mapping may interconnect with each other or may operate on independent data or streams as may be appropriate for the operations mapped. The regions can all be dynamically adapted with the changing needs of the processing requirements. The regions can be very granular or may involve only partial compute clusters as well. Hence the runtime adaptable processor of this patent is dynamically adaptable to a very fine grain level to meet the demands of the required processing.

Security Solution comprises line cards which may incorporate the security processor SAN protocol processor TCP IP processor or runtime adaptable protocol processor or various other processors disclosed in this patent. The line card configuration and the architecture may vary with the specific system and the application. Three types of line card architectures a flow through b look aside and c accelerator card are illustrated in this patent to illustrate usage models for the processors of this patent. and illustrate these configurations using a security processor based system though it could also be based on other processors of this patent. Blocks and block illustrate two of these types of card configurations. The security processor illustrated in these cards is that disclosed in this patent. There are various different variations of the security processor that can be created depending on the functionality incorporated in the processor. Blocks and block illustrate two versions of such security processor. Block illustrates a security processor core comprising at least a content search and rule processing engine coupled with a runtime adaptable processor. This processor is similar to that illustrated in and is described in detail below. Block illustrates the security processor of block coupled with a TCP IP processor or a protocol processor to provide more functionality usable in a security node as a security processor. A reduced functionality security processor not illustrated may also be created by removing runtime adaptable processor and associated logic from block to provide a content search and rules processing engine based security processor. The choice of the security processor may depend on the system in which it is being deployed the functionality supported by the system the solution cost performance requirement or other reasons or a combination thereof. The security processor may use one or more ports to connect to external memories block which may be used to store rules information or other intermediate data or packets or other information as necessary to perform various functions needed for security processing. The memories may be of various types like DRAM SDRAM DDR DRAM SRAM RDRAM FCRAM QDR SRAM DDR SRAM Magnetic memories Flash or a combination thereof or future derivates of such memory technologies. The inventions disclosed in this patent enable many variations of the architectures illustrated and may be appreciated by those skilled in the art that changes in the embodiments may be made without departing from the principles and spirit of the invention.

Similarly there may be many other categories of application layer rules. For example there may be rules defined to manage digital rights of the owners of the electronic documents or media or content as may be appropriate. Such rules may also be defined similar to the signature or pattern matching or string of character matching rules above. These rules may flag matches to a specific digital rights signature inside a content which can then be used to refer to a digital rights database that may indicate if such an access or usage of the digital content is permitted to the owner. The digital rights ownership data base may reside in the memories associated with the security processor and a control processor like block or block described below can refer to that database to decide if valid ownership exists or not and if it does not exist what specific action should be taken based on the defined rule. The digital rights confirmation may be done by some other device or processor in the specific node which is performing the digital rights signature matching. The decision of where to perform such analysis functionality may depend on the specific system usage model and the system design choices. A set of rules for digital rights management also be created as part of the application layer rules for the security processor.

Instant messaging IM has gained tremendous success in its usage by individuals as well as corporations. Instant messaging may be regulated for various industries like the financial industry to preserve for future reference and is also subject to spam like other modes of communication like email. Thus some organizations may create rules specifically targeted towards instant messaging to protect against ensuing liabilities in case of wrongful usage or protect the users from unwanted spam or for other reasons as deemed appropriate by the organization. One of the issues with instant messaging is that any level of policing has to be done in stream without creating delays in the communication. Thus a hardware based security enforcement of this patent may be needed to monitor IM. These rules are similar to other application layer rules discussed above and may be created using similar means like defining the message search strings using regular expressions.

Recent surveys by FBI and others have found that over 70 of attacks on information technology are from within an organization. Thus there is a need for a class of security devices and rules that need to be developed to protect from the damaging effects of such attacks. These rules are defined as extrusion detection rules. The extrusion detection rules may be created to detect intentional or unintentional disclosure of confidential or proprietary or sensitive information of the organization using the network from going outside the perimeter of the organization. For example a software company may need to guard its core software source code from accidental or malicious disclosure to people or entities unauthorized to get it. A set of rules may thus be created by the organization that may search for specific strings or paragraphs or code modules or other appropriate information within all outbound messages and flag them or prevent them from being sent. Such rules may also be compiled using the security compiler flow and distributed to the appropriate node or nodes. For example a rule may be defined to search for a Top Secret phrase in any message being sent that is outbound from the organization and flag such a message for further review by the IT manager or to drop such connection and inform the user or other responsible person. A regular expression rule . Top Secret may be defined to search for the term anywhere in a message. Such rules may also be created as application layer rules that may then be compiled and distributed to appropriate nodes for detection and enforcement of extrusion detection security functionality.

The IT manager may be able to create classes of rules from the application layer rules or network layer rules or SAN rules or application specific rules or other rules and deploy a class of rules to a class of security nodes and a different class of rules to another set of security nodes. For example the manager can create certain application layer rules like anti spam or anti virus rules and network layer rules that are deployed to the switches and routers of the network that are security enabled with the teaching of this patent and another set of rules like extrusion detection rules and network layer rules for sensitive servers holding critical top secret information. It may be possible to create different sets of rules that may be deployed depending on the functions within an organization. For example security nodes that are deployed within a manufacturing department may get one set of rules while those in an engineering department may get a different set of rules. Creating a different set of rules for different types of devices or different device users or node specific rules or a combination thereof can be used as a process to create a pervasive and layered security within an organization.

Similarly there may be application layer rules that detect or flag access to specific web address or URL s or other confidential information like customer information comprising their credit card numbers or health information or financial reports or the like which may be used to create a different set of application rules as shown in block . With an increase in usage of voice over IP solutions within organizations and over the internet security threats are also increasing. It may then be necessary to create rules specific to VOIP for example rogue connections may need to be detected and flagged or VOIP traffic may not be allowed to go outside an organization s boundary or detect for viruses entering the organization through VOIP connections or create confidentiality of VOIP traffic by encrypting it or the like. The VOIP rules may also be created using the same application layer rules engines and detect matches to the rules at appropriate nodes in the network. The runtime adaptable processor block described below may be used to provide encryption or decryption services to VOIP traffic when such traffic is detected by the VOIP rule match. Similarly other application specific rules may also be developed and provided in the central manager modules to be programmed compiled and distributed to the secure nodes in the network using the compiler flow illustrated in .

Network layer rules block may comprise various rules targeted at the network and transport layers of the network. These rules are similar to those illustrated in . These rules may include IP level address rules protocol port rules protocol specific rules connection direction oriented rules and the like. These rules may be described in a special language or using regular expressions. In TCP IP based networks these are primarily TCP and IP header fields based rules where matches may be defined on source address or destination address or an address range or port numbers or protocol type or a combination thereof. Similarly there may be rules targeted specifically to storage area networks which may transport critical information assets of an organization. This is shown as a different category of rules but may comprise storage network s network layer rules application layer rules or the like. There may be rules targeted to specific logical unit numbers LUNs or zones groups of source destination addresses or logical or physical block addresses or the like. These rules may also be represented in a specific language or as strings of characters or data patterns using regular expressions.

The secure solution compiler of allows an IT manager to create security rules of different types as discussed above and enable them to create a layered and or pervasive security model. The compiler flow would be provided with the characteristics of the specific nodes like the security capability presence the rules communication method the size of the rule base supported the performance metrics of the node deployment location e.g. LAN or SAN or other or the like. The compiler flow then uses this knowledge to compile node specific rules from the rule set s created by the IT manager. The compiler comprises a rules parser block for parsing the rules to be presented to the lexical analyzer generator block which analyzes the rules and creates rules database used for analyzing the content. The rule parser may read the rules from files of rules or directly from the command line or a combination depending on the output of the rule engines. The rules for a specific node are parsed to recognize the language specific tokens used to describe the rules or regular expression tokens. The parser then presents the tokens to the lexical analyzer generator. The lexical analyzer processes the incoming tokens and generates non deterministic finite automaton NFA which represents rules for parsing the content. The NFA is then converted in deterministic finite automaton DFA by the lexical analyzer generator to enable deterministic processing of the rule states. The process of creating NFAs and DFAs is well understood by modern compiler developers. However the lexical analyzer generator creates various tables that represent DFA states and the state transition tables for the rules that are used by a hardware lexical analyzer instead of generating lexical analysis software as is done for compilers. One way to view the rules is that they define a language to recognize the content. These tables are used by a lexical analyzer hardware or content search and rule processing engine block described below to analyze the stream of data being presented to the security processor of this patent. The regular expression rules can be viewed as defining a state transition table. For example if a string help is being searched using a regular expression help then each character of the regular expression can be viewed to represent a state. There may be a start state s and character specific states s s s and s where s x represent a state for a character x. There may also be error states like s err which may be entered upon terminating a search when appropriate transition conditions are not met. As the input stream is being analyzed by the hardware lexical analyzer this state machine is activated when a first h is encountered and the state machine reaches s. Now if the next character in the stream is an e then the state machine transitions to s. Thus if a string help is encountered the state machine will reach state s. States s through s are accepting states meaning they continue the search to the next state. State s for this string is marked by the lexical analyzer generator as a terminal state. These states are marked as accepting or terminal states in the accept tables. When a comparison reaches a terminal state a match with the specific rule may be indicated. Any action that needs to be taken based on matching of a rule is created in a match action table as an action tag or instruction that is then used by the content search and rule processing engine block to take specific action or forward the match and action information to control processor block to take appropriate rule specific action. However if there is only a partial rule match e.g. if the input content includes string her then the rule processing hardware will enter state s having encountered he however as soon as r is analyzed an error is indicated to mean that there is no rule match and processing of the input stream starts from that point forward from the initial state s. Though the above description is given with regards to using single character match per state it is be possible to analyze multiple characters at the same time to speed up the hardware analysis. For example the lexical analyzer generator may create tables that enable transition of 4 characters per state there by quadrupling the content search speed. The lexical analyzer generator creates character class tables block next state look up tables block state transition tables block accept states block and match action tables block which are then stored in the compiled rules database storage block . The character class tables are created by compressing the characters that create a similar set of state transition into a group of states for compact representation. The state transition tables comprise of rows of states in a DFA table with compressed character class as the columns to look up the next state transitions. The next state table are used to index to the next state from the current state in the state machine represented by the DFA. These tables are stored in on chip and off chip memories associated with security processors of this patent. The compiler of this patent uses the node characteristics and connectivity database to create the rules on a node by node basis. The compiler indicates an error to the IT manager if certain rules or rule sizes do not match the capabilities of the specific nodes so they may be corrected by the manager. This information is retrieved from a node characteristics and connectivity database as illustrated by block .

Rules distribution engine block follows the central manager and rules distribution flow illustrated in and . The security rules may be distributed to the host processor or a control plane processor as illustrated in or to a control processor and scheduler block described below or a combination thereof as appropriate depending on the node capability. The rules may be distributed using a secure link or insecure link using proprietary or standard protocols as appropriate per the specific node s capability over a network.

The control processor and scheduler block communicates with the rules distribution engine block to receive appropriate data tables prior to starting the content inspection. It stores the received state information into their respective dedicated memories. The character class table from block is stored in the memory block . The state transition and accept tables block and are stored in their respective memories represented by block . Block may also be two or more separate memories for performance reasons but are illustrated by one block in the figures. The next state look up tables from block are stored in the next state memory block . The match action tables from block are stored in their memory block . These tables may be larger than the memory available in the security processor on chip and may be stored in external memory or memories that are accessed by the memory controller block . There may be multiple ports to memory to speed up access to data tables stored in external memories. These memories may be of various types like DRAM SDRAM DDR DRAM SRAM RDRAM FCRAM QDR SRAM DDR SRAM Magnetic memories Flash or a combination thereof or future derivatives of such memory technologies. For most applications next state table and action tables may need to be off chip whereas the other tables may be maintained on chip dependent on the size and number of the rules. Once the rules distribution engine provides the tables to the control processor and scheduler block and they are setup in their respective memories the security processor is ready to start processing the data stream to perform content inspection and identify potential security rule matches or violations. The security processor state configuration information is received via a coprocessor host interface controller. The security processor of this patent may be deployed in various configurations like a look aside configuration illustrated in or flow through configuration illustrated in or an accelerator adapter configuration illustrated in as well others not illustrated which can be appreciated by persons skilled in the art. In a look aside or an accelerator adapter configuration the security processor of this patent is under control of a master processor which may be a network processor or a switch processor or a TCP IP processor or classification processor or forwarding processor or a host processor or the like depending on the system in which such a card would reside. The control processor and scheduler receives the configuration information under the control of such master processor that communicates with the rule engine to receive packets that contain the configuration information and passes it on to the security processor. Once the configuration is done the master processor provides packets to the security processor for which content inspection needs to be performed using the coprocessor or host interface. The coprocessor or the host interface may be standard buses like PCI PCI X PCI express RapidIO HyperTransport or LA 1 or SRAM memory interface or the like or a proprietary bus. The bandwidth on the bus should be sufficient to keep the content search engine operating at its peak line rate. The security processor may be a memory mapped or an IO mapped device in the master processor space for it to receive the packets and other configuration information in a look aside or accelerator configuration. The security processor may be polled by the master processor or may provide a doorbell or interrupt mechanism to the master to indicate when it is done with a given packet or when it finds a match to the programmed rules. The control processor and scheduler block and the block work with the master processor to provide the above functionality. The control processor and scheduler stores incoming packets to the packet buffer block and schedules the packets for processing by the content search and rule processing engines as they become available to analyze the content. The scheduler maintains the record of the packets being processed by the specific engines and once the packets are processed it informs the master processor. The content search and rule processing engines of block inform the control processor and the scheduler when they have found a match to a rule and the action associated with that rule as programmed in the match action table. This information may in turn be sent by the control processor to the master processor where the master processor can take specific action for the packet indicated by the rule. The actions may be one from a multitude of actions like dropping the packet or dropping a connection or informing the IT manager or the like as discussed earlier. When the security processor includes a runtime adaptable processor like block the control processor and scheduler may schedule operations on the packet through block . The control processor would work with the adaptation controller block to select the specific avatar of the processor for the needed operation. For example a packet that needs to be decrypted before being analyzed may be scheduled to the adaptable processor before being analyzed by the content search engines. Once the packet has been decrypted by the adaptable processor it is then scheduled by block to block . However runtime the adaptable processor may operate on a packet once a match has been found by the content search engines or the packet has been processed by the search engine without any issues. For example the packet data may need to be encrypted once no issues have been found. The control processor and scheduler schedules the packets to the runtime adaptable processor in the appropriate order as defined by the needs of the operation. The runtime adaptable processor block adaptation controller block and configuration memory block is similar to those illustrated in and . The runtime adaptable processor and the associated block provide similar functionality with appropriate logic enhancements made to couple to the control processor and scheduler of the security processor. The runtime adaptable processor may be used to provide compression and decompression service to the packets if the appropriate adaptation configurations are deployed. The runtime adaptable processor may also be used for VOIP packets providing relevant hardware acceleration service to those packets like DSP processing or encryption or decryption or the like.

The security processor may also need to provide inspection ability across multiple packets in a connection between a source and a destination. The control processor and scheduler block provides such functionality as well. The control processor may store the internal processing state of the content search and security processing engine in a connection database which may be maintained in the on chip memory in the control processor or in the off chip memory. The control processor and scheduler looks up the execution or analysis state for a given connection when a packet corresponding to the connection is presented to it by the master processor or in the incoming traffic in a flow through configuration described below. The connection ID may be created by the master processor and provided that to the security processor with the packet to be inspected or the security device may derive the connection association from the header of the packet. The connection ID may be created in the IP protocol case by using a 5 tuple hashing derived from the source address destination address source port destination port and the protocol type. Once the connection ID is created and resolved in case of a hash conflict by the control processor and scheduler it then retrieves the state associated with that connection and provides the state to the search engines block to start searching from that state. This mechanism is used to create multi packet searches per connection and detect any security violations or threats that span packet boundaries. For example if there is a rule defined to search for Million US Dollars and if this string appears in a connection data transfer in two separate packets where Million U appears in one packet and S Dollars appears in another packet then if a connection based multi packet search mechanism of this patent is not present the security violation may not be detected since each packet individually does not match the rule. However when the multi packet search is performed no matter how far apart in time these two packets arrive at the security node the state of the search will be maintained from one packet to another for the connection and the strings of two packets will be detected and flagged as a continuous string Million US Dollars .

As discussed earlier the security processor of this patent may also be deployed in a flow through configuration. For such a configuration the security processor may include two sets of media interface controller ports as illustrated by blocks and . The security processor illustrated in is very similar to that in however it has multiple media interface controller ports as against the host or coprocessor interface block like block . The number of ports may depend on the line rate per port and the performance of the security processor. The sum of incoming ports line rate should be matched with the processing performance of the security processor to provide security inspection to substantially the entire incoming traffic. A conscious choice could be made to use a higher line rate sum than the processors capability if it is known that not all the traffic needs to be inspected for security purposes. The decision of the traffic that must be inspected may depend on the connection or the session as programmed in the processor from the central manager. The security processor of may thus be used to provide flow through security inspection to the traffic and may be used in a flow through configuration like that illustrated by . A flow through configuration may be created for various types of the systems like a switch or a router line card or a host server adapter or a storage networking line card or adapter or the like. In a flow through configuration the security processor is directly exposed to the traffic on the network. Thus the central manager and the rules distribution engine may directly communicate to the control processor and scheduler block or block of the security processor. Security processor of block is similar to the one illustrated in without the runtime adaptable processor incorporated in it. One of the issues in a flow through configuration that needs to be addressed is the latency introduced in the traffic by the security processor. The network switches or routers for example are very sensitive to latency performance of the system Hence in such a configuration a deep packet inspection can add significant latency to the detriment of the system performance. Hence the security processors for flow through configuration of this invention provide a cut through logic illustrated by block that is used to pass the data traffic from the input of the security processor to its output incurring a minimal latency to support the overall system performance needs. The control processor and scheduler block of provides the cut through logic and is not illustrated separately. In a flow through configuration once a match has been found the security processor may create special control packets internal to the system where the system s switch processor or a network processor or other processors may interpret these messages and perform appropriate action on the packets that utilize the cut through mode before those packets are allowed to exit the system. Such a protocol may be a proprietary protocol within a system or may utilize a standard protocol as may be appropriate for the system incorporating a flow through security configuration.

As described earlier the security processor of this invention may be embedded in systems with many different configurations dependent on the system environment system functionality system design or other considerations. This patent illustrates three such configurations in and . As discussed above illustrates the security processor in a network line card or an adapter providing flow through security. In this configuration the security processor may reside next to the media interface as illustrated or after block closer to the host or back plane interface block . Such decisions are system design decisions and are not precluded from the usage of the security processor of this patent. In a scenario where the security processor incorporates TCP IP or protocol processing capability the block may not be required in some systems. illustrates a look aside security configuration for a network line card or an adapter. In such a configuration there exists a master processor which may be a switch processor network processor forwarding engine or classification engine or other processor illustrated by block . The master processor communicates with the central manager of as described earlier to receive the rules and to provide events back to the central manager working with the security processor. The master processor may also incorporate functions illustrated by blocks and . The master processor could also be a TCP IP processor or other IP processor variations that are feasible from the processors of this patent as well.

The security processor of or may also be used to perform content searches on documents or digital information and be used to create indexes that may be used for accelerated searches like web search capability provided by Google or their competitors. Using security processor of this invention for such a task can provide significant performance improvement to indexing and searches compared to that done using a general purpose processor based software. For such an application the control processor and scheduler of the security processor may utilize the content search and rules processing engines to perform key phrase searches in data presented to it and get the match indexes. These results can then be used to create a master search index by a process that may run on the control processor and scheduler or another processor of the system that is servicing the content search request from end users. This master index may then be referred to provide quick and comprehensive search results.

The security processor of described above may be coupled with elements of the processor of or to provide security capabilities to different versions of protocol processing architectures of this patent. For example block illustrates one such variation where the TCP IP protocol processor is coupled with the processor of or to create another security processor with TCP IP processing. Similar versions may be created by including IP storage protocol processing capability with the security processor or coupling TCP IP processor with RDMA capability with the security processor of or or a combination thereof. The security processor of or may also be used in place of the classification engine block shown in more detail in as described above when the security processor is programmed to search for the classification fields used in block .

As described earlier regular expression can be represented using FSA like NFA or DFA. illustrates Thompson s construction for the regular expression xy y yx. Thompson s construction proceeds in a step by step manner where each step introduces two new states so the resulting NFA has at most twice as many states as the symbols and operators in the regular expression. An FSA is comprised of states state transitions and symbols that cause the FSA to transition from one state to another. An FSA comprises at least one start state and at least one accept state where the start state is where the FSA evaluation begins and the accept state is a state which is reached when the FSA recognizes a string. Block represent the start state of the FSA while block is an accept state. Block represents state and represents state . The transition from state to state is triggered on the symbol x and is represented as a directed edge between the two states. Thompson s NFA comprises of transitions which are transitions among states which may be taken without any input symbol.

The and illustrate generic four state NFAs where all the transitions from each state to the other are shown based on the left biased or right biased construct characteristics. However not all four state NFAs would need all the transitions to be present. Thus if a symbol is received which would require the FSA to transition from the present state to the next state when such transition on the received input symbol is not present the NFA is said to not recognize the input string. At such time the NFA may be restarted in the start state to recognize the next string. In general one can use these example four state NFAs to represent any four state RE in a left biased LB or right biased RB form provided there is a mechanism to enable or disable a given transition based on the resulting four states NFA for the RE.

Similarly also illustrates states and transitions for a right biased NFA. The figure illustrates a right biased NFA with a state A which has incoming transitions from state B state C and state D on receiving input symbols S S and S respectively. However the transitions from each of the states B C and D to state A occur only if the appropriate state dependent control is set besides receiving the appropriate input symbol. The state dependent control for transition from state B to state A is Vwhile those from states C and D to state A is Vand Vrespectively. Transition to the next state A is dependent on present state A through the state dependent control V. Thus transition into a state A occurs based on the received input symbol and if the state dependent control for the appropriate transition is set. Thus one can represent any arbitrary four states right biased NFA by setting or clearing the state dependent control for a specific transition. All state transition controls for a given state form a state dependent vector SDV which is comprised of V V V and Vfor the example in for the left biased and the right biased NFAs.

If there are n states supported in the search engine discussed later each SLB needs n bit SDV which can be stored as a register comprised of flip flops 2 bits allocated to start flag and accept flags 1 bit for LB RB m bit action storage and t bit tag storage. Thus if n 16 and m 6 and t 4 then the total storage used per SLB would be a 29 bit register equivalent which is a little less than 4 bytes per state.

The search engine receives as inputs the FSA control and data buses FSACtrl and FSAdata respectively the input symbol on the bus InChar and the substitution control and data on the bus SubBus . The other standard inputs like the power clock reset and the like which may be required are not illustrated in this figure or other figures in this disclosure to not complicate the illustrations. Use of such signals are documented in ample in basic hardware design textbooks and is well understood by any one with ordinary skills in the art. Though the description is with respect to the inputs as illustrated the information may be received on multiplexed busses or other parallel busses and hence this description is not to be taken as a limitation of the invention. These signals or busses are illustrated for the ease of discussion. The search engine also comprises an application state memory which holds application specific NFA rule context for multiple applications like HTTP SMTP XML and the like. The application state memory holds information like SDV initial state vector ISV accept state vector ASV LB RB Actions Tags number of rules and the like for each application NFA. The application state control block is used by the search processor s global configuration controller described later to configure the appropriate NFA parameters in the application state memory for each of the supported applications. Then the configuration controller can instruct the application state control to program the SLBs using a particular application NFA for performing search for that application. There may be multiple REs or rules per application in the search engine if the number of states for the REs together is less than that supported by the search engine. The application state control can retrieve the appropriate application context from the application state memory and program each SLB with the appropriate information to implement the NFA programmed for the application. The parameters being configured in each SLB comprises the SDV start state accept state LB RB action or tag. The application state controller also configures the symbol detection array block using the symbols for the NFA retrieved from the application state memory. Appropriate counter initialization may also be done by the application state control for the NFA before enabling the search engine to start processing input symbols received on InChar . The state outputs of the SLBs are concatenated together to form the CSV. The RSV is the output from the symbol detection array. The RSV and CSV are fed into each SLB. The SLBs then perform the state transition evaluation based on their configuration until an accept state is reached or the NFA reaches a null state which indicates that the input string is the one recognized by the NFA or rejected by the NFA respectively. The symbol detection array comprises of an array of computation engines which in the most simplistic form can be comparators and memory for symbols being searched. The input symbol received on InChar is compared simultaneously with all the symbols and then the appropriate signals of the RSV are set if the symbol being searched matches the received symbol. The symbol detection array may also interact with the counters block to enable rules that detect occurrence of symbols before taking a specific transition. The accept state and the action fields from each SLB is used by the match and accept state detection logic block to detect if an NFA has reached an accept state and it then registers the action for the NFA to provide to the global configuration controller discussed later. The action or tag may provide an indication that a RE rule has matched but may also provide an action that may be taken on the input stream. For example the action may indicated that the input stream contains a computer virus being searched and hence the input stream should be quarantined for instance or it may detect that confidential information being searched is part of the stream and hence the transmission of this stream should be stopped and the like. The action or tag may also comprise of a pointer to a code routine that the global controller or the control processor discussed later may perform. The code routine may cause the configuration of the search engine to change to search for a different set of rules. For example if the NFA was part of a rule set discussed earlier and if it triggers then the action to be taken may be to activate set of rules and put the set on hold for instance. There are innumerable type of tasks that may be accomplished using the invoked code routine as a result of the RE rule being matched as would be obvious to one skilled in the art. Complex set of actions can be taken on the input stream or other control functions may be activated or management code may be activated or the like once an NFA reaches an accept state if the associated action or the tag is programmed for such an action to occur. The indication of a match as well as the rule ID being matched may be output on the M bus by the match logic for the cluster controller discussed later or the global configuration controller to take appropriate action. The action tag carries the associated action or tag to the cluster controller or the global configuration controller which may follow actions as described above. The match logic is set up to support M simultaneous rule evaluation by the search engine.

There are instances of regular expressions where a certain number of events need to be counted before a state transition occurs. For example a RE rule may be created like 3 5 abc which indicates that the RE recognizes a string that has 3 to 5 leading a s followed by b followed by c . If the input string is aabc or abc or aaaaaabc then it would not be recognized by this RE whereas an input string aaaabc or aaabc or aaaaabc would be recognized by this RE. To enable such REs there are counters provided in the search engine as part of the counters block . There may be L sets of counters to support multiple rule execution simultaneously. If the some rules do not need to use the counters then those counters may be allocated for use by other rules. The choice of L may be a function of the search engine resource availability and the need of counters for applications being supported using the search engine. The counter values may be read or written to by the application state controller on the configuration of an application state into the SLBs if the NFA configured in the search engine needs counters. The output from the counters block is provided to the SLBs for them to make decisions on the counter values as necessary. The counters may also be read and written by the symbol detection array for it to support RE rules that depend on counting symbols occurring for a certain number of times like the one described above. The counter block also holds counters to keep track of the string match beginning and the end for each of the M rules. Begin and end counts indicate to the cluster controller or the control processor the start marker for the string match in the input stream and the end marker in the input stream respectively.

The FSA extension logic block is used to extend the number of NFA states that can be supported for a RE. Thus if there are REs that need more states then n supported by an individual search engine it is possible to connect multiple search engines into groups together to support the RE. The FSA extension logic block provides the input and output signals that enable a search engine to know the state of other search engines of the group. The signals may be fed into the SLBs as control signals though are not explicitly illustrated in or or .

The search engine may also comprise of a pattern substitution control which may hold pattern s to be used for substituting the input string as the search engine progresses through NFA. This can support finite state transducer functionality which is well understood and documented in literature. This block may evaluate the transitions taken through the NFA by keeping track of two consecutive CSVs and making decision about the transitions enabled and the associated transducer symbols to be generated for substituting the received input string.

A search engine with n 16 states would require 16 4 64 Bytes of storage for SLBs plus 16 bytes for 16 symbols and M counters where if M 4 then there may be 8 bytes for counters if each counter is 16 bit. Thus total storage required for an application state context discussed below is around 90 bytes for n 16.

There are N application state contexts ASC stored in the application state memory . ASC is illustrated to be one of the RE rules for anti spam application whereas ASCn is illustrated to be one of the RE rules for XML application. Each ASC may be further organized into NFA parameters and SLB parameters . The SLB parameters are state specific parameters like Start Flag Accept Flag SDV Action and the like that are applied to the specific SLB. These could simply be a 4 byte memory location. Similarly the NFA parameters are those that are applicable for the entire NFA like symbols number of rules per NFA substitution table and the like. RB LB information may be common to the entire NFA or may be stored in a per state basis.

The benefit of ASCs is that when the control processor or global configuration controller or cluster controller discussed later need to apply the rules for a different application than that currently configured in the search engine it can make the transition to rules for the new application rapidly by informing the application state control of each search engine. Thus in an array of search engines which may be used to perform evaluation of a large number of REs in parallel the switching to an application context can be very rapid unlike if the configuration information has to be retrieved from a global memory or external system memory and provided to each search engine. In the example of n 16 states it may be possible to switch the application context for each search engine within 20 to 25 clocks. If there are 1000 search engines being used in parallel to do 1000 RE rule searches and if the application based context has to be loaded into each of them from a global memory or external memory it can take potentially 20 000 to 25 000 clock cycles which can bring down the performance of the search processor significantly compared to using ASCs per search engine as in this invention.

Even though the above discussion is with respect to different applications it may be possible to use different REs for the same applications in different ASC. This can allow the search processor to be able to support a larger number of REs for an application though there would be performance impact if all the REs have to be applied to the input stream. In such a case the input stream may be presented multiple times to the search engine each time switching the ASC which in fact then applies a new RE for the application to the stream. Another usage model of the ASC can be for the same applications nested RE rules. For example as illustrated in the RE rules are nested rules where only a limited set is being applied to the input stream in parallel. In such a case one rule from each level of nesting could be set up in individual ASCs and the action programmed could indicated that when the current rule reaches accept state the context needs to be switched to another ASC. For example R R and R could each be stored in three independent ASCs with Rule R being used as the initial context. Now if the input string matches R then the context can be switched to bring in rule R which then looks for the FONT string. If R reaches an accept state then the context can be switched to R and so on. Thus a nested RE rule set for any given application can be easily supported in the search engine of this invention. Thus one can create a runtime adaptable finite state automaton architecture using various runtime configurable and programmable features of the NFA based search engine described above.

As discussed earlier a regular expression can be converted to NFA and DFA using well known methods. Further it is also well known that an NFA and a DFA for the same RE are both equivalent. However there are differences in terms of the performance metrics of an NFA vs. a DFA as discussed earlier. DFAs can result in a large storage requirement for the states in a worst case compared to NFAs. However in practice such is not the case and many times a DFA may have fewer states than a NFA. Thus a content search processor using only NFA based search engines may not realize the most efficient implementation. One can use a combination of NFAs and DFAs to evaluate a set of regular expressions. Further if the DFA implementation can be more efficient than the NFA then more rules can be supported on a single integrated search processor compared to the one that purely uses NFAs. DFAs may have transitions to multiple states from a state each transition taken on a different input symbol. Thus to process a state one would need to detect multiple symbols and based on the input symbol choose the appropriate transition. In DFAs only one state is active at a time by the deterministic nature of DFAs. Thus it may be possible to select DFAs that are like the one illustrated in where the number of states in the DFA are less then or comparable to those in an NFA and the number of transition from a state are also limited in this case an upper bound of two and program them for evaluation using a DFA based search engine like that illustrated in . As long as the number of states of the DFA are not more than that can be held in the DFA instruction and state flow memory described below the DFA could be chosen instead of the NFA for evaluation.

The fields of the instructions above are op first operation S symbol T IP Target op second operation S symbol T IP Target and other fields like Default target init start count init end count action tag and the like . For evaluation of the DFA illustrated in such a code may be stored in the DFA instruction and state flow memory from where each instruction is retrieved and processed. The column with the header State IP is representing the corresponding state of the DFA and the instruction pointer IP value for the corresponding instruction. There may be a preamble code to that illustrated that may need to be executed before the DFA code to setup appropriate counters error state processing and the like. However in the illustration of the example above the start state A and its IP is used as the default target if the input symbols are not those expected by the DFA. The DFA evaluation starts in the start state at IP 0. The instruction at IP 0 is first fetched and evaluated against the input symbol. If the input symbol is a y then the DFA is expected to transition from state A to state C however if the received symbol is x then the transition is to state B . Assuming that the input symbol is a y then the instruction evaluation of the first instruction results in an affirmation of the comparison of symbol S with y and hence the associated target IP of 2 is selected as the next instruction to fetch and evaluate against the next symbol. The IP 2 corresponds to state C which is expected to be the transition that is defined by the DFA of . The DFA search engine now processes instruction at IP 2 and then follows the appropriate flow as defined by the instructions. When the execution reaches IP 3 which corresponds to the accept state D the DFA search engine outputs a match flag and associated tag or action to indicate that a string matching the current DFA has been found. The DFA engine also records the end count to indicate where the string match ends in the input stream. The DFA engine with multi way execution like the one illustrated above may also provide priority to the multiple operations being performed simultaneously where the first operation s affirmative result may get a higher priority over the other operations. In general since the DFAs are deterministic only one of the listed operations of the instruction should result in affirmative result though such a mechanism may be created to avoid execution ambiguity.

Though the example above used only two types of operations there are many other operations that DFA operations engine can perform as illustrated in . There may be RANGE operation defined which enables one to search for a symbol S to fall within the range of ASCII codes for two symbols with X indicating the lower bound of the range and Y indicating the upper bound of the range. X and Y in this case may be any 8 bit or 16 bit extended ASCII codes. Similarly a set of operations are defined that enables the DFA code to loop in a given state until a certain condition is met or if a certain number of occurrences of a certain symbol occurs. There are other ways to realize these operations which would be within the scope of this invention as may be obvious to one skilled in the art.

The DFA based search engine of is essentially an application specific processor which operates on targeted instructions like those illustrated in . The DFA search engine operates by having the DFA context control fetch the instructions from the DFA Instruction and state flow memory which are than evaluated by DFA operations engine by performing the operation indicated by the instruction which may typically involve evaluating the input symbol received on InChar with those expected by the instruction and then decide the next instruction to fetch based on the result of the evaluation. The DFA operations engine provides the information about the next instruction to the DFA state and context control blocks and which in turn provide the next instruction address to the DFA instruction and state flow memory . The performance of this processor can reach the same level or higher as that of state of the art microprocessors using advanced process technologies and there by achieve line rate performance of 1 Gbps to 10 Gbps and higher. Thus one can create a runtime adaptable finite state automaton architecture using various runtime configurable and programmable features of the DFA based search engine described above.

A DFA search engine as illustrated in may be implemented in 50 to 200 logic elements in today s FPGA depending on the supported DFA operations and multi way execution support. The instruction and flow state memory may by 512 bytes per search engine or less depending on the number of application contexts that get supported and the number of states that need to be supported. This can allow over a thousand DFAs to be included in a single FPGA which can have close to thousand logic elements. Thus DFA and NFA based content search processor may be implemented in today s best of class FPGAs supporting one to two thousand REs in one FPGA which may be operated at the clock rate achievable in the FPGA. Higher number of REs can be supported in an FPGA format when multiple application rules are taken into account which can drive the number into multiple thousands of rules. This invention thus supports creating content search solutions using FPGAs beside ASICs ASSP and other forms of integrated circuits which may be useful when the time to market is important and the volumes are not high enough to justify the expense of creating ASICs ASSPs or other ICs. When an NFA and DFA search engines of this invention are implemented on the best of class process technologies like 90 nm or 65 nm that are used in today s leading edge microprocessors the performance of these engines can be taken to the same or higher frequencies and thus can enable very high line rate processing of regular expressions unlike today s microprocessors. Unlike below 100 Mbps performance of a 4 GHz processor for evaluating a few hundred NFAs or DFAs as described earlier a thousand RE content search processor with NFA and DFA search engines of this invention could achieve clock rates similar to the leading microprocessors and evaluate around one input symbol per clock which may be a single character or more characters thereby achieving well above 10 Gbps performance. Further with the device densities growing with small geometry process technologies like 90 nm 65 nm and lower the number of REs that can be incorporated on an IC can be in multiple thousands depending on the choice of the mix of NFAs and DFAs. By using a mix of NFAs and DFAs on the content search processor it would be possible to create a series of products that offer different numbers of both in a chip for example one product can offer 25 DFAs and 75 NFAs while another could offer 75 DFAs and 25 NFAs for instance. Thus it may become possible to meet varying market needs. If an application requires more REs than those provided on a single content search processor multiple processors can be used in parallel to increase the number of REs. Thus it is possible to use this invention to support search applications requiring thousands of REs without the need to grow the memory usage like that would be needed in a composite DFA that make it uneconomical for general use.

The search processor of may be used as a coprocessor in a system which needs acceleration for content search of local content remote content or content received in network packets or the like. The configuration in which this processor may be used is similar to that illustrated in and where the security processor of the blocks and may be replaced with the search processor of or other variations of the search processor as described above. The search processor may be incorporated on an add in card which may act as an acceleration card in a system as illustrated in where the search processor may be that of or other variations of the search processor as described above. The search processor may also be configured in a form illustrated by where the runtime adaptable processor block may be substituted with runtime adaptable search processor array block . Such search processor can have the benefit of allowing the number of rules the processor can support to grow beyond the number supported by the runtime adaptable search processor array. Thus if this search processor array is able to support a couple thousand REs then if the user need to grow the number of REs beyond that they may be supported as a composite DFA that may be implemented on the content search rule processing engines . Thus the user may be able to grow the number REs upto a point that the external memories can support without needing to add another search processor. The content search processor of this application may also be used as a security processor in applications that require deep packet inspection or require screening of the application content like anti spam anti virus XML security web filtering firewalls intrusion detection prevention and the like. Thus all figures in this application that refer to security processor may also incorporate a search processor.

The search processor receives the content it needs to search from the master processor in a coprocessor configuration like that of or or it may receive the content from the network when coupled with the network interfaces like those in or . The control processor and scheduler deposits data or information content to be searched into data packet buffers for scheduling them into the search processor array when the processor array is ready to receive it. In general the search processor can meet the network line rates from below 100 Mbps 1 Gbps to 10 Gbps and higher and hence the packets may not need to be buffered. However it may be necessary to buffer the packets if the packets or content comes into the search processor at a rate higher than what it can handle. It may also be necessary to schedule the packets that belong to the same flow as in the same transport layer connection for instance to be processed by the search processor in the correct sequential order. The packets of such flows may also get stored in the data packet buffers until packets in the flow before them get processed. The search processor may also need to provide inspection ability across multiple packets in a connection between a source and a destination. The control processor and scheduler block provides such functionality as well. The control processor may store the internal processing state of the runtime adaptable search processor array in a connection database which may be maintained in the configuration and the global memory or in the off chip memory. The control processor and scheduler looks up the execution or analysis state for a given connection when a packet corresponding to the connection is presented to it by the master processor or the incoming traffic in a flow through configuration. The connection ID may be created by the master processor and provided that to the search processor with the packet to be inspected or the search processor may derive the connection association from the header of the packet. The connection ID may be created in the IP protocol case by using a 5 tuple hashing derived from the source address destination address source port destination port and the protocol type. Once the connection ID is created and resolved in case of a hash conflict by the control processor and scheduler it then retrieves the state associated with that connection and provides the state to the search processor array block to start searching from the state of the connection. This mechanism is used to create multi packet searches per connection and detect any search strings or security violations or threats that span packet boundaries. For example if there is a rule defined to search for Million US Dollars and if this string appears in a connection data transfer in two separate packets where Million U appears in one packet and S Dollars appears in another packet then if a connection based multi packet search mechanism of this patent is not present the security violation may not be detected since each packet individually does not match the rule. However when the multi packet search is performed no matter how far apart in time these two packets arrive at the search processor the state of the search will be maintained from one packet to another for the connection and the strings of two packets will be detected and flagged as a continuous string Million US Dollars .

The control processor and scheduler schedules the packets or data to the runtime adaptable search processor array in the appropriate order. The runtime adaptable search processor array block adaptation controller block and configuration and global memory block are similar to those illustrated in and . The runtime adaptable search processor array and the associated blocks provide similar functionality with appropriate logic enhancements made to couple to the control processor and scheduler of the search processor. The runtime adaptable search processor array and its components are described below. The search processor array is presented with each character of the incoming packet or data which it then examines for string match with the RE rules programmed in them. W hen a string is matched the search processor array provides that indication along with other information like the RE rule identification the action to be taken the tag associated with this rule the start count of the match the end count of the match and the like to the control processor and scheduler which may then take appropriate action depending on the rule or rules that have been matched. The controllers inside the search processor array may also take appropriate action s as required as discussed below.

Rules distribution engine block follows the central manager and rules distribution flow illustrated in and with appropriate changes to communicate to the right configuration of the search processor. The search rules may be distributed to the host processor or a control plane processor as illustrated in or to the control processor and scheduler block or a combination thereof as appropriate depending on the node capability. The rules may be distributed using a secure link or insecure link using proprietary or standard protocols as appropriate per the specific node s capability over a network.

The control processor and scheduler block communicates with the rules distribution engine block to receive appropriate compiled rule tables prior to starting the content inspection. It programs the received rules into the appropriate NFA or DFA search engines described earlier working with the adaptation controller . There may be multiple rules being stored in each search engine dependent on the number of application contexts supported by the search processor. Once the rules distribution engine provides the compiled rules to the control processor and scheduler and they are setup in their respective engines the search processor is ready to start processing the data stream to perform content inspection. The search processor state configuration information is received via the coprocessor host interface controller or a media interface controller not illustrated. The search processor of this patent may be deployed in various configurations like a look aside configuration illustrated in or flow through configuration illustrated in or an accelerator adapter configuration illustrated in as well others not illustrated which can be appreciated by persons skilled in the art. In a look aside or an accelerator adapter configuration the search processor of this patent is under control of a master processor which may be a network processor or a switch processor or a TCP IP processor or classification processor or forwarding processor or a host processor or the like depending on the system in which such a card would reside. The control processor and scheduler receives the configuration information under the control of such master processor that communicates with the rule engine to receive packets that contain the configuration information and passes it on to the search processor. Once the configuration is done the master processor provides packets or data files or content to the search processor for which content inspection needs to be performed using the coprocessor or host interface. The coprocessor or the host interface may be standard buses like PCI PCI X PCI express RapidIO HyperTransport or LA 1 or SRAM memory interface or the like or a proprietary bus. The bandwidth on the bus should be sufficient to keep the content search engine operating at its peak line rate. The search processor may be a memory mapped or an IO mapped device in the master processor space for it to receive the content and other configuration information in a look aside or accelerator configuration. The search processor may be polled by the master processor or may provide a doorbell or interrupt mechanism to the master to indicate when it is done with a given packet or content or when it finds a match to the programmed rules. The control processor and scheduler block and the interface controller block work with the master processor to provide the above functionality.

The control processor and scheduler stores incoming packets to the packet buffer block and schedules the packets for processing by the search processor array block as they become available to analyze the content. The scheduler maintains the record of the packets being processed by the specific engines and once the packets are processed it informs the master processor. The search processor array informs the control processor and the scheduler when it has found a match to a rule and the action associated with that rule. This information may in turn be sent by the control processor to the master processor where the master processor can take specific action indicated by the rule for the packet. The actions may be one from a multitude of actions like dropping the packet or dropping a connection or informing the IT manager or the like as discussed earlier.

The search engines in a search cluster are configured by the cluster configuration controller which interacts with the global configuration controller . The global configuration controller is used for global configuration control and interacts with the adaptation controller control processor and scheduler configuration and global memory and the memory controller to receive store and retrieve various hardware configuration information as required by the rules being configured by the user or an IT manager using the search compiler flow illustrated in . The search compiler compiles the rules as per the capabilities of a specific search processor of search device and then interacts with the control processor and scheduler through a host or master processor or otherwise as described earlier to configure the search processor array with these rules. These rules are communicated by the control processor interacting with various blocks listed above to the cluster configuration controller. The cluster configuration controller then configures the application state memory for an NFA based search engine and DFA Instruction and State flow memory for a DFA based search engine. Once all the search engines in all the clusters are configured with appropriate rules and the application contexts the content search may start. As described earlier there may be a need for the search processor to maintain the information of flows or sessions to continue searching of content inside packets of the same flow that may arrive at the search processor at different times. Similarly in solutions that require searching of messages or files or other content which may be sent to the search processor by a host processor there may be a need to maintain the context if the content is sent in chunks of bytes or pages or segments or the like. The flow context may be maintained in the global memory of the search processor or may be stored in the memory coupled to the memory controller . When the number of search engines is large the amount of information that may need to be stored for a given context can cause a performance issue. The DFA and NFA search engines of this patent may need to save a minimum of one 32 bit word per context. Hence for example if the number of search engines are 1024 distributed amongst 64 clusters with 16 search engines each then one would need to store and retrieve up to 1024 32 bit words from global memory. This can be a significant performance barrier for the search processor that may need to support 1 Gbps to 10 Gbps line rate. Hence for applications that need high line rate performance this invention enables the clusters to locally store the flow context information in the cluster memory . For the current example if the search engines are organized in a 4 4 array and there are four ports into the local memory then on a per cluster basis four 32 bit words of flow context need to be stored and retrieved for the flow context switch. Since all the clusters can access their local memories in parallel the flow context switch can thus be accomplished in 8 clock cycles compared to 2048 clock cycles when the flow information has to be stored and retrieved from the global memory. The loading of a new context would require 4 clock cycles which would be the minimum time needed to switch the context where the storing of the context being swapped out could be done in background while the search of the new context begins. Thus the patent of this invention can solve a major performance bottleneck that may exist in architectures that require the context to be stored in the global memory.

The configuration illustrated in may be used for email security or instance message security or outbound security or extrusion detection or HIPAA compliance or Sarbanes Oxley compliance or Gramm Leach Bliley compliance or web security or the like or a combination thereof. The security capabilities listed may comprise anti spam anti virus anti phishing anti spyware detection prevention of directory harvest attacks detection prevention of worms intrusion detection prevention firewalls or the like or detection prevention of leaks of confidential information health care information customer information credit card numbers social security numbers or the like or a combination thereof. The content search processor or processors in such device may be configured with a set of security rules for one or more of the applications listed above and provide acceleration for content search for information incoming or outgoing from the device. Since the content search processor may be used inline with the traffic the device may be deployed at any place in the network like close to a router or a switch or gateway of an organization s networks or at a departmental level or within a datacenter or a combination and provide high speed content inspection to incoming or outgoing traffic flow of the network.

The processors of this invention may be manufactured into hardware products in the chosen embodiment of various possible embodiments using a manufacturing process without limitation broadly outlined below. The processor may be designed and verified at various levels of chip design abstractions like RTL level circuit schematic gate level layout level etc. for functionality timing and other design and manufacturability constraints for specific target manufacturing process technology. The processor design at the appropriate physical layout level may be used to create mask sets to be used for manufacturing the chip in the target process technology. The mask sets are then used to build the processor chip through the steps used for the selected process technology. The processor chip then may go through testing packaging process as appropriate to assure the quality of the manufactured processor product.

While the foregoing has been with reference to particular embodiments of the invention it will be appreciated by those skilled in the art that changes in these embodiments may be made without departing from the principles and spirit of the invention.

