---

title: System and method for data transfer using a recoverable data pipe
abstract: A system for data transfer using a recoverable data pipe includes a data producer, one or more data consumers, a storage device and a data pipe manager. The data producer may be configured to append updated data blocks to the storage device via a producer storage input/output (I/O) channel. The data consumers may be configured to read data blocks from the storage device via consumer storage I/O channels. The data pipe manager may be configured to maintain metadata identifying data blocks of the storage device that have been read by each data consumer, and to release backing storage corresponding to the data blocks that have been read by all data consumers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07293145&OS=07293145&RS=07293145
owner: Symantec Operating Corporation
number: 07293145
owner_city: Cupertino
owner_country: US
publication_date: 20050216
---
This application is a continuation in part of prior application Ser. No. 10 966 791 entitled System and Method For Loosely Coupled Temporal Storage Management filed Oct. 15 2004.

This invention relates to computer systems and more particularly to data transfer in computer systems.

Many business organizations and governmental entities rely upon applications that access large amounts of data often exceeding many terabytes of data for mission critical applications. A variety of different storage devices potentially from multiple storage vendors with varying functionality performance and availability characteristics may be employed in such environments. Numerous data producers i.e. sources of new data and updates to existing data may need to transfer large amounts of data to consumers with different sets of storage access requirements. In some enterprise environments hundreds or thousands of data producers and data consumers may be operating at any given time. Sustained update rates on the order of tens to hundreds of gigabytes per hour may need to be supported in large enterprise data centers with spikes of even higher levels of I O activity. In some environments furthermore access patterns may be skewed towards the most recently updated data that is instead of being uniformly spread over an entire data set a relatively large proportion of write and read requests may be directed at a working set of recently modified data.

As the heterogeneity and complexity of storage environments increases and as the size of the data being managed within such environments increases providing a consistent quality of service for data transfer operations may become a challenge. Quality of service requirements may include the ability to predictably sustain data transfers at desired rates between data producers and data consumers data integrity requirements and the ability to recover rapidly from application host and or device failures. At the same time advanced storage features such as replication and archival capabilities may also be a requirement for enterprise level storage environments.

Some traditional data sharing mechanisms such as applications that may utilize NFS Network File Systems or FTP File Transfer Protocol may rely on data transmission over networking protocols such as one or more underlying protocols of the Transmission Control Protocol Internet Protocol TCP IP family. Such traditional mechanisms may sometimes provide unpredictable levels of performance especially in response to error conditions and or network congestion. In addition many traditional networking protocols may have been originally designed for short messages and may not scale well for large data transfers. Networking software stacks within operating systems and or service specific daemons or service processes may add to dispatch latency and processing load at data producers and data consumers. Simultaneous transfer of data between a single data producer and multiple data consumers may require the use of relatively inefficient broadcast or multicast protocols. Some data sharing mechanisms that rely on file systems may also result in additional overhead for file management.

Other traditional data sharing mechanisms such as various types of clustered systems may require data producers and data consumers to be tightly coupled e.g. configured within a single cluster even if application requirements may demand other configurations of producers and consumers such as producers and consumers in different clusters or within un clustered servers. Some clustering solutions may also require extensive and cumbersome configuration management. In addition traditional data transfer mechanisms may provide inadequate support for efficiently re using resources such as disk space or memory dedicated to data transfer for example insufficient support may be provided to re use storage for the data that has already been consumed while a long data transfer continues. The requirements for sustained high performance predictability and improved data integrity during data transfers may place a high burden on system managers.

Various embodiments of a system and method for data transfer using a recoverable data pipe are disclosed. According to a first embodiment the system includes a data producer one or more data consumers a storage device and a data pipe manager. The data producer may be configured to append updated data blocks to the storage device via a producer storage input output I O channel. That is in an embodiment where the storage device presents a linear address space the data producer may be configured to add blocks containing updated data at a higher address within the linear address space than was used for previous updates. The one or more data consumers may be configured to read data blocks from the storage device via consumer storage I O channels. Portions of the storage device that are currently in use e.g. portions that have been written by the data producer but have not yet been read or discarded by at least one data consumer may be backed at persistent storage thus allowing the in use portions of the storage device to be recovered in the event of a failure.

In some embodiments data consumers may read data blocks sequentially i.e. generally in the same order as the order in which the data blocks were written. Each data consumer may read data independently of other data consumers and of the data producer e.g. with minimal direct interactions between different data consumers and the data producer. The data pipe manager may be configured to maintain metadata identifying data blocks of the storage device that have been read by each data consumer and to release backing storage corresponding to the data blocks that have been read by all data consumers. For example in one implementation the metadata may include a data low watermark pointing to an address representing the latest data block that has been read by all data consumers. The metadata may also comprise additional information in some embodiments such as a storage low watermark and a storage high watermark identifying an address range within the storage device that is currently backed at persistent storage and or a data high watermark identifying the latest data block written by the data producer.

Data producers may write data independently of the rate at which data consumers read data in some embodiments. For example a data consumer may be reading a first data block of the storage device e.g. a block at an address X while a data producer may concurrently be appending or writing a second data block e.g. a block at address X Y . In other embodiments the data pipe manager may release a given data block concurrently with reads to other data blocks from one or more data consumers and or concurrently with data block append operations from the data producer. Thus the operations of data producers data consumers and the data pipe manager may be autonomous independent and asynchronous with respect to each other. The data pipe manager may be configured to allocate released backing storage to store additional data blocks appended by the data producer in some embodiments. For example after a range of data blocks have been read and processed by all data consumers the data pipe manager may be configured to release the backing storage corresponding to the range of data blocks e.g. into a pool of free storage and to re use the released backing storage for new updates from the data producer.

In one embodiment the storage device may comprise a logical volume. The logical volume may be striped and or mirrored across multiple physical storage devices in some embodiments. The portion of the storage device that is currently backed by persistent storage may be recoverable in the event of a failure such as a system crash. In some embodiments updates to the storage device may be applied in batches e.g. more than one data block may be appended to the storage device in a single transaction. The metadata for the storage device may include one or more commit records in some embodiments where each commit record corresponds to a batch of updates and where each commit record includes storage and data watermarks. The metadata may be stored within the storage device in one embodiment. Components of the data pipe manager may be incorporated with one or more support servers and or within a data producer or one or more data consumers in some embodiments. In some embodiments the data pipe manager may support one or more interfaces such as an application programming interface API supporting functions such as open close read and write which may be similar to functions typically supported by file systems. Such APIs may allow end user applications and or storage management software such as volume managers to access the storage device.

While the invention is susceptible to various modifications and alternative forms specific embodiments are shown by way of example in the drawings and are herein described in detail. It should be understood however that drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed but on the contrary the invention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.

As shown in data producer may be pictured as adding data at one end of storage device i.e. the end with the blocks with the highest currently allocated addresses within the linear address space while data consumers may be pictured as reading data from the other end of storage device i.e. from lower addresses of the linear address space . Storage device may also be referred to herein as a data pipe. Only one data producer may be granted write access to storage device in some embodiments. Data pipe manager may be configured to maintain metadata identifying data blocks of storage device that have been read by each data consumer . At any point in time at least a portion of storage device may be backed or stored within one or more backing physical storage devices . For example in some embodiments backing physical storage devices may implement one or more mirrored and or striped logical volumes thus allowing the backed portion of storage device to be recovered in the event of a failure. Data pipe manager may also be configured to release and re use backing storage corresponding to data blocks of storage device that have already been read by each data consumer .

The metadata maintained by data pipe manager may include a number of pointers to specific regions or addresses of storage device in different embodiments. For example in one embodiment one pointer may identify an un backed region L consisting of data blocks of storage device that are no longer backed at physical storage devices . Another pointer may identify a discarded region A containing data blocks whose backing storage may be released by data pipe manager . In one embodiment each data consumer may be configured to designate data blocks of storage device for which reading and or processing has been completed as being appropriate for discarding. In such an embodiment data blocks that have been designated by every data consumer as being appropriate for discarding may form region A and data pipe manager may be configured to release backing storage corresponding to region A as needed. Another pointer within the metadata may identify a region B containing data blocks that have been read by at least one data consumer but not read by all data consumers in some embodiments. Region C consisting of data blocks that may have been written by data producer but may not yet have been read by any data consumer may be identified by another pointer in one embodiment. Region D may consist of blocks that have not yet been written by data producer . In some embodiments portions of regions A and D and all of regions B and C may be backed at physical storage devices at any given time.

Producer application may be any application that generates data that may be consumed or processed by a data consumer application . For example in one embodiment a producer application may be configured to extract data from a database repository e.g. based on a specific criterion such as a data insertion date range or based on the parameters of a query and transfer the extracted data through storage device for analysis by a data mining consumer application . Data producer and data consumers may each comprise any device or system capable of supporting applications such as producer application and consumer applications such as a computer server including one or more processors one or more memories and various peripheral devices.

Producer storage I O channel A and consumer storage I O channels B may each include a storage interconnect that allows shared access to backing physical storage devices from data producer and data consumers . In some embodiments for example a producer or consumer storage I O channel may include one or more fibre channel links over which data may be transferred using a storage protocol such as Fibre Channel Arbitrated Loop Protocol FCAL e.g. without relying on TCP IP . In other embodiments a version of a Small Computer System Interface SCSI protocol allowing shared access may be employed for data transfer over the producer and consumer storage I O channels. In some embodiments a combination of protocols may be used e.g. encapsulated SCSI commands over fibre channel . In one embodiment the type of interconnect hardware implementation and protocol used by data producer may differ from the interconnect hardware implementation and protocol used by one or more data consumers . It is noted that multiple data consumers may share at least part of a consumer storage I O channel B in one embodiment. Backing physical storage devices may include any of a variety of physical storage devices in different embodiments such as individual disks disk arrays various levels of RAID Redundant Arrays of Inexpensive Disks devices non volatile random access memory NVRAM etc. In some embodiments a backing physical storage device may include one or more levels of storage cache for example a disk array may include a front end NVRAM cache .

As described below in further detail the update operations of the data producer and the read operations of data consumers may be performed independently of each other in some embodiments. For example at a given point of time data producer may be appending a data block at address X of storage device data consumer A may be reading a data block at address X A data consumer B may be reading a data block at address X B and so on. The use of independent storage I O channels by data producer and data consumers may support such independent and autonomous reading and writing operations with minimal interference between data producer and data consumers . To consumer applications and producer applications storage device may appear to consist of locally connected storage for which no locking or intermediary servers are required. In addition the operations of data pipe manager such as identifying data blocks of storage device whose backing storage may be released and or re used and releasing or re using such data blocks may also be performed independently and autonomously with respect to data producer and or data consumers in some embodiments. For example in one embodiment data pipe manager may be configured to release backing storage for a particular data block at address Y of storage device while data producer may be concurrently appending a data block at address Y A and a data consumer A may be concurrently reading a data block at address Y C where C is less than A . In some embodiments one or more components of data pipe manager may be incorporated within data producer data consumers and or additional support servers. Additional details about constituent components of data pipe manager and the functions that may be performed by data pipe manager data producer and data consumers in different embodiments are provided below.

A data consumer may be configured to wait to receive a next read request from a consumer application block of . When the next read request is received data consumer may be configured to read the desired data block or blocks from storage device via a consumer storage I O channel B block provide the contents of the data block to the consumer application and again wait for the next read request block . In some embodiments multiple consumer applications may be supported within a single data consumer . A plurality of consumer applications or a plurality of instances of a consumer application may be configured to share access to the same storage device in some embodiments. In one embodiment a component of data pipe manager may be incorporated within a data consumer and may be configured to process the read requests generated by one or more consumer applications . It is noted that in some embodiments consumer applications and or producer applications may be incorporated within one or more remote servers i.e. at servers other than data consumers and or data producer . For example in one embodiment a data producer may be configured to receive and process update requests from a producer application running on another server or servers and a data consumer may be configured to process read requests generated from consumer applications at a different server or servers. In some embodiments a producer application and or a consumer application may be a distributed or clustered application running on a number of servers or cluster nodes.

Data pipe manager may be configured to maintain metadata identifying blocks of storage device that have been read and or processed by each data consumer as shown in block of . In addition as described in further detail below in conjunction with the description of the metadata may also contain one or more pointers which may be termed high watermarks or low watermarks to various regions of the address space of storage device . In some embodiments data pipe manager may be configured to periodically identify blocks of storage device whose backing storage may be released and or re used. In other embodiments data pipe manager may be configured to identify such blocks on demand e.g. when requested by data producer a data consumer or an administrator in addition to or instead of performing periodic identifications. When such blocks are identified as detected in decision block data pipe manager may be configured to release the corresponding backing storage block and to continue to maintain the metadata as before block . If no data blocks are identified that are suitable for a release of backing storage data pipe manager may also be configured to continue to maintain the metadata until the next scheduled or requested identification occurs. The metadata may be updated for each batch of updates appended by data producer in some embodiments as described below and may include one or more commit records that may be stored within storage device .

As indicated by the separate flow diagrams for data producer data consumer and data pipe manager in at least some of the operations of data producers data consumers and data pipe managers may be performed autonomously e.g. without tight coupling between data producers and data consumers in some embodiments. Read operations from a data consumer for example may be performed independently and or asynchronously with respect to write or append operations performed by a data producer and with respect to concurrent read operations from other data consumers . Similarly in some embodiments at least some backing storage allocation release and or reuse operations may be performed by data pipe manager independently and or asynchronously with respect to read and write operations. It is noted that the semantics of data flow between data producers and data consumers may result in the imposition of some constraints on the operations of data producer data consumer and data pipe manager in some embodiments. For example a data consumer may be prevented from reading past the last updated block of storage device in one embodiment e.g. an error message may be returned if such a read is attempted and a data producer may be prevented from writing to an address for which backing storage has not yet been allocated. As noted above in some embodiments agents or components of data pipe manager may be incorporated within data producers data consumers and or additional support servers.

For example in the embodiment illustrated in data pipe manager may be configured to maintain a data low watermark DLW which points to the least recently written data that has not been discarded by all data consumers and a data high watermark DHW which points to the most recently written data. The blocks of storage device that lie between DLW and DHW e.g. blocks in regions B and C of at a given point of time may thus represent data that has been written by data producer but for which reading and or processing has not been completed by at least one data consumer . In some embodiments a given data consumer or a component or agent of data pipe manager at a given data consumer may also be configured to maintain its own local data low watermark or discard pointer as described in further detail below. In such embodiments data pipe manager may derive DLW from the local data low watermarks maintained for each data consumer e.g. by determining the lowest address indicated by any data consumer s local data low watermark and may allow data consumers to view the derived or global DLW .

In addition in the embodiment shown in data pipe manager may be configured to maintain a storage low watermark SLW and a storage high watermark SHW respectively identifying the low end and high end block address for the region of the storage device that is currently backed at physical storage devices . Thus for example at least a portion of regions A B C and D of the address space of storage device may be backed at physical storage devices at the point of time illustrated in . In embodiments where redundant storage organizations are used to back storage device for example via mirroring data blocks between SLW and SHW may be recoverable in the event of certain kinds of failures. Backing storage for regions of the address space of storage device with lower addresses than SLW e.g. region L may already have been released or de allocated by data pipe manager . Backing storage for regions of the address space of storage device with higher addresses than SHW may not yet have been allocated. Backing storage for blocks whose addresses lie between SLW and DLW may be released or purged by data pipe manager and may be re used or reallocated for example for new updates to storage device or for additional storage devices or data pipes .

In general data pipe manager may be configured to move each of the watermarks SLW DLW DHW and SHW in a forward direction i.e. towards higher logical addresses within storage device as data producer appends new data data consumers read and discard data and data pipe manager releases and re uses backing storage. Ordering restrictions may be imposed on the various watermarks in some embodiments for example SLW may not be moved past DLW i.e. to a higher logical address than DLW otherwise a particular data consumer may be unable to read data it has not yet discarded. Similarly DHW may not move past SHW because a write to a block for which backing storage has not yet been allocated may fail.

In the embodiment depicted in data pipe manager may be configured to maintain metadata including for example the watermarks shown in within storage device itself e.g. within a metadata region . The metadata region may be mapped to a special metadata mini volume M of logical volume . At a given point of time a portion of a discarded region A of storage device may be mapped to one or more allocated mini volumes A while backing storage for another portion region of discarded region A may already have been released or reclaimed by data pipe manager . Regions of storage device that have been written to by data producer but not yet been discarded by all data consumers e.g. regions B and C may also be mapped to portions of one or more allocated mini volumes such as A B and C. A portion of an allocated mini volume C may extend beyond the currently written region e.g. to not yet written region D of storage device . As more backing storage becomes needed e.g. as data producer fills up mini volume C with new updates data pipe manager may be configured to allocate new mini volumes by reusing backing storage from released reclaimed region or using new backing storage that has not previously been used for storage device .

Any of a number of different allocation policies may be used when allocating additional storage for storage device in different embodiments. For example in one embodiment mini volumes may each contain the same number of data blocks i.e. backing storage may be allocated in fixed size units. In another embodiment an allocation unit size may be selected dynamically based for example on the rate at which data producer appends data. In such an embodiment for example if data pipe manager detects a rapid exhaustion of allocated storage which may be caused either by rapid updates or by a mismatch between an update rate and read rates data pipe manager may dynamically increase the size of new mini volumes e.g. if a given mini volume comprised 128 Megabytes of storage the next mini volume may comprise 256 Megabytes and so on . In some embodiments the allocation unit size and or an allocation unit determination policy may be configurable. An allocation triggering policy i.e. a policy specifying the conditions under which an additional mini volume should be allocated may also be configurable in some embodiments. In one embodiment for example allocation of a new mini volume may be triggered when data high watermark DHW approaches within a specified number of blocks of storage high watermark SHW . In another embodiment released backing storage may be allocated automatically e.g. as soon as data pipe manager reclaims the backing storage for a new mini volume unless for example data producer has indicated to data pipe manager that no new updates to storage device are forthcoming.

Commit records A and B may be updated alternately in some embodiments e.g. so that an update of one commit record does not overwrite the contents of the other. Such alternate updating of commit records may support an improved level of error recovery e.g. in case of system crashes or device recording errors. If for example a crash or other error occurs while commit record A is being updated recovery may be started using the contents of commit record B. In some embodiments additional entries may be included within commit records such as validation keys security keys one or more checksum values that may be used to validate data blocks associated with the commit record or to validate the commit record itself etc. In one embodiment metadata region may include metadata information other than the commit records illustrated in such as device specific information on backing physical storage devices performance or recovery specifications and or statistics etc. It is noted that in some embodiments a single commit record may be maintained while in other embodiments more than two commit records may me included in the metadata for storage device .

A number of different commit protocols may be used in different embodiments to synchronize a commit record and its associated data blocks e.g. to ensure that commit record A is consistent with its associated data block region C . In one embodiment a serial commit protocol may be used to ensure that updated data blocks for a transaction are saved to persistent storage before a commit record for the transaction is written. In a serial commit protocol the write latency for an update of a data block e.g. as seen by a requesting producer application may include the latency for writing the data block or blocks as well as the latency for writing the commit record. Successful writing of the commit record in such a protocol may indicate that all the corresponding data blocks have also been written. During a recovery operation for example after a system crash both commit records A and B may be read and the most recently written commit record e.g. as identified by a sequence number may be used as the starting point for continued processing.

In contrast to the use of a serial commit protocol the use of a concurrent commit protocol may allow data blocks and commit records to be written in parallel in some embodiments. It is noted that in concurrent commit protocols as in serial commit protocols an update requester may not be notified of a completion of a requested update until the corresponding commit record is written to persistent storage. By allowing commit records and data blocks to be written in parallel rather than sequentially write latency as seen by a requesting producer application may be reduced relative to the write latency that may have been seen if a serial commit protocol were used instead potentially at the cost of slightly reduced recoverability. In some embodiments the use of a concurrent commit protocol may however require more overall processing than the use of a serial commit protocol e.g. resulting in higher processor utilization relative to embodiments where a serial commit protocol is used .

In one embodiment for example a checksum may be generated from the data blocks to be written and may be stored in the commit record as part of a concurrent commit protocol. The checksum may be derived from a sample or subset of the data in each sector of the updated data blocks in some embodiments. In some implementations the sample size i.e. the amount of data for which the checksum is generated may be configurable e.g. the checksum may be generated from a small fraction of the data such as a few bytes sampled from each sector or from as large a fraction of the data blocks as desired. In one implementation a checksum generated from the contents of the commit record may also be included within the commit record in addition to the checksum derived from the data blocks. During a subsequent system recovery the most recently written commit record A may be read and validated and then the data blocks corresponding to the most recently written commit record A may be read and validated against the checksum. If the user data cannot be validated against the checksum the state of storage device may be rolled back to the previous transaction and the previous commit record.

In some embodiments a technique employing pre formatted physical storage may be employed to aid in recovery. For example each sector of the underlying storage e.g. at storage devices may be pre formatted to include a magic pattern i.e. a known string of bit values in one embodiment. During recovery in such an embodiment the most recently written commit record A is retrieved and all data block sectors associated with the record A are scanned for the magic pattern. If the magic pattern is detected the data for the most recent transaction may not have been successfully written to the underlying storage device and the previous commit record B may be used as the starting point for continued processing. In other embodiments a higher level recovery service such as a service embedded within a database log manager or a replication manager may be configured to provide recovery for the data being transferred via storage device .

In some embodiments as noted previously updated data blocks may be written to storage device in batches. is a flow diagram illustrating aspects of the operation of data pipe manager in such an embodiment. Data pipe manager may be configured to wait for a next update request from a producer application block of . When the next update request is received data pipe manager may enqueue the next update request within a current batch of updates for batched processing block . When sufficient updates have been accumulated in the current batch as detected in decision block data pipe manager may be configured to update metadata e.g. a commit record and to perform the physical writes for the current batch and the updated metadata block . If enough updates for the current batch have not been accumulated data pipe manager may resume waiting for the next update. Similarly after the current batch has reached a desired size data pipe manager may initialize a new batch and resume waiting for new update requests. In some embodiments data blocks and metadata for a given batch may be written to persistent storage as a single transaction and the update requester such as a data producer application may be notified of a completion of the batched update only after the data blocks and the metadata have been written to persistent storage. The size of a batch i.e. the amount of data that may be written as one transaction may be configurable in some embodiments e.g. as specified in one or more configuration files by a system administrator. In other embodiments an adaptive batch sizing policy may be employed where for example batch size may be dynamically adjusted by data pipe manager in response to changing conditions such as measured latencies update rates and the like . In many implementations fairly large batch sizes such as several megabytes may be used without causing substantial I O delays under normal operating conditions.

Each data consumer may be configured to read blocks of storage device independently of and concurrently with other data consumers in some embodiments as noted earlier. Data pipe manager may be configured to allow data consumers or consumer applications to connect to and disconnect from a given storage device and to keep track of the set of data consumers that are correctly connected to the storage device. A connected data consumer may read from the storage device autonomously e.g. without interacting with other data consumers or data producer . In embodiments where components of data pipe manager are incorporated within a support server a data consumer may require only infrequent interactions with the support server for example to connect to or disconnect from storage device or to handle read errors or other failures. Although data retrieval by a data consumer may normally be in sequential order i.e. data consumer may read data blocks of storage device in the order in which the data blocks were written by data producer a data consumer may re read any data block that it has not explicitly designated for discarding. Further details on how a data consumer may designate data blocks to be discarded are provided below in conjunction with the description of . The rate at which a given data consumer reads data may be limited only by the transfer rate of the consumer storage I O channel B being used for the data transfer e.g. no locking or intermediary data transfer agents may be required. A data consumer may be configured to periodically read metadata for storage device in some embodiments e.g. to obtain a version of DLW and DHW . Data consumer may read blocks between its versions of DLW and DHW . Upon reaching its version of DHW i.e. after having read data blocks with addresses lower than the address pointed to by its version of DHW data consumer may retrieve the latest version of DHW e.g. by re reading the latest commit record . In some embodiments if the latest DHW is the same as the local version of the DHW previously obtained by data consumer indicating that data consumer has read all available data blocks data consumer may wait or perform other functions before again retrieving the latest available DHW . In some embodiments an adaptive algorithm to adjust the interval between successive retrievals of the latest DHW may be used for example increasing the interval each time until a changed DHW is retrieved.

In some embodiments each data consumer may be configured to keep track of data blocks of storage device that have been read and processed by the data consumer and to which access from the data consumer may no longer be required. is a block diagram illustrating an embodiment where a discard pointer may be maintained for each data consumer to identify such data blocks. For example discard pointer A may point to data block A indicating that data consumer A is willing to discard data blocks of storage device A with addresses lower than A. Similarly discard pointer B associated with data consumer B may point to data block B indicating that data consumer B no longer requires access to data blocks with addresses lower than B and discard pointer N may point to data block N indicating that data consumer N no longer requires access to data blocks with addresses lower than N. A discard pointer may represent a local version of a data low watermark for a given data consumer. A given data consumer may not read data at a lower address than the address pointed to by its associated discard pointer but may read data blocks at higher addresses that may have been discarded by other data consumers . For example in data consumer A may read data blocks between A and DHW but data consumer B may not read data blocks between A and B. In some embodiments a central purger incorporated within data pipe manager may coordinate the release of backing storage from region i.e. a region consisting of data blocks that have been read and discarded by all data consumers. Each data consumer or an agent or component of the data pipe manager at the data consumer may be configured to provide the latest version of its discard pointer to the central purger. Further details about central purgers and their interactions with other components of data pipe manager are provided below in conjunction with the description of . Such a central purger may for example derive the DLW after aggregating information from the discard pointers for each data consumer and coordinate the release of backing storage for addresses lower than DLW . The released backing storage may be re allocated for data blocks at addresses beyond SHW .

Producer DPMCs may be configured to provide a number of functions to help the management of storage device in different embodiments. For example in one embodiment a producer DPMC may be configured to connect to and disconnect from a storage device by communicating with support server DPMC enqueue updates for batched writes to the storage device transmit enqueued data blocks over a producer storage I O channel A and or update commit records and other metadata for the storage device . In addition a producer DPMC may be configured to release and reallocate backing storage corresponding to discarded data blocks in coordination with central purger in some embodiments. In such an embodiment central purger may be configured to issue data purge and re allocation requests for data blocks at addresses lower than DLW to a local allocator within a producer DPMC . That is while the central purger may be configured to perform the processing required to identify the data blocks whose backing storage may be released and reused the actual release and reallocation may be performed by the local allocator within a data producer .

Consumer DPMCs may be configured to provide a number of consumer side functions related to the management of one or more storage devices in various embodiments. In one embodiment a consumer DPMC may be configured to connect to a storage device by communicating with support server DPMC obtain local versions of metadata elements such as DLW and DHW at the data consumer read data blocks over consumer storage I O channel B and or notify support server DPMC when a discard pointer for the associated data consumer is updated.

In some embodiments a storage device may grow by concatenating mini volumes at one end of the data address space of the storage device and may shrink as a result of a release or reclamation of mini volumes at the other end of the data address space. It is noted that in one embodiment backing storage for metadata mini volume M may not be released or re used during normal operation. The released backing storage may be re used for the same storage device in some embodiments and or may be recycled among multiple storage devices in other embodiments. The local allocator within a given producer DPMC may be responsible for managing release and reallocation of backing storage for the storage device associated with the corresponding data producer while central allocator at support server may be responsible for redistributing backing storage among different storage devices . For example in some embodiments a local pool of released and reusable backing storage which may be termed a local recycling bin may be maintained by a local allocator at each data producer while a global pool of released backing storage or global recycling bin may be maintained by central allocator at support server . If sufficient released storage can be found in its local recycling bin local allocator may allocate new mini volumes from the local recycling bin. If not enough released storage is found in the local recycling bin local allocator may request additional storage from central allocator . In some embodiments a local allocator may be configured to maintain a minimum supply of reusable storage within its local recycling bin for example in case the central allocator is inoperable or unreachable when additional storage is needed for a storage device . Central allocator may also be configured to maintain a minimum supply of reusable storage within the global recycling bin and to request or obtain additional free storage from one or more local allocators as needed in one embodiment. In other embodiments central allocator may also be configured to take over the backing storage release and allocation functions for a given local allocator in the event that the local allocator or its associated data producer becomes inoperable.

As described above in some embodiments central purger may be configured to aggregate discard notifications e.g. notifications including updated discard pointers from consumer DPMCs and to notify a local allocator as the DLW for a given storage device advances allowing the local allocator to release backing storage for addresses lower than the DLW . Connection manager may provide a number of support functions for data producers and data consumers in various embodiments. For example in one embodiment connection manager may allow one or more data consumers and a single data producer to connect to a given storage device ensure that only one data producer has write access to a given storage device and verify that backing storage devices are accessible from the connected data producer and data consumers. In other embodiments connection manager may also be configured to ensure that connectivity between central purger and consumer DPMCs is maintained e.g. over a TCP IP connection and or that connectivity between central allocator and local allocators is maintained. During reconfigurations of a logical volume corresponding to a storage device connection manager may also be configured to suspend access from the data producer and from data consumers connected to the storage device.

When a data consumer disconnects normally from a given storage device e.g. if the disconnection is requested by the data consumer rather than being caused by an error or failure in one embodiment connection manager may be configured to designate all the data that has been read by the data consumer as discarded even if the discard pointer corresponding to the data consumer has not been updated to reflect all the data read. If the data consumer reconnects to the same storage device after a normal disconnection connection manager may be configured to set the data consumer s discard pointer to the next block after the blocks that were discarded at the last disconnection. For abnormal disconnections e.g. disconnections that may have been forced by errors connection manager may not adjust discard pointers .

In some embodiments a support server may be configured for failover e.g. an alternate support server may be configured to take over the functionality of a primary support server in the event of a failure. In other embodiments a data producer or a data consumer may be configured to take over the support server s functions if a failure occurs. It is noted that one or more components of support server DPMC may be incorporated within a data producer or within a given data consumer in some embodiments. For example in a minimal configuration a support server may be omitted a central purger may be incorporated within a data consumer and central allocator and connection manager may be incorporated within a data producer . In one specific embodiment central allocator may be omitted entirely and release and allocation of backing storage may be managed entirely by local allocators .

Data pipe manager may provide one or more interfaces to allow access to storage device in different embodiments. In one embodiment for example data pipe manager may support an application programming interface API that may allow producer applications and consumer applications to access storage device using functions or methods similar to typical file system functions. For example in such an embodiment functions such as open close read and write may be provided to producer and consumer applications. In some implementations the API to access storage device may include at least a subset of a file system API conforming to a standard such as POSIX Portable Operating System Interface . The semantics of various API options such as options to open specifying read only access versus read write access or exclusive write access may differ for data producers and data consumers in some embodiments. For example in one implementation an invocation of an open function with a particular option e.g. an O TRUNC or truncate option may be used allocate a new mini volume. In addition to an API allowing user level applications to access a storage device in some embodiments an internal API may also be provided to allow lower level interactions with storage devices such as interactions at a volume manager level.

A number of different types of end user applications as well as internal storage management application may utilize the data transfer capabilities supported by storage devices in different embodiments. For example as noted earlier a data producer may be configured to extract contents of a database repository and transfer the extracted data through a storage device for analysis by a data mining consumer application . Data producer may begin writing extracted data to the storage device as soon as the data extraction process begins and data consumer may being reading and analyzing data from the storage device as soon as the data producer commences writing. The asynchronous and autonomous nature of reading and writing to storage device may lead to reductions in overall data transfer time compared to other solutions which may for example require the data to be fully extracted first then fully transferred e.g. via FTP before reading may commence. Data replication and archival applications may also be configured to utilize the data transfer techniques implemented at storage devices as described above especially in scenarios where at least part of the processing and I O load for data transfer is offloaded to one or more support servers instead of being handled at servers where production applications are executed. Internal storage management operations potentially involving large data transfers such as frozen image services that may be embedded within volume managers or operating systems may also be implemented using storage devices in some embodiments.

In general a data producer may comprise any hardware and or software device capable of generating data for transfer to a data consumer for example on behalf of one or more producer applications and a data consumer may comprise any hardware and or software device capable of receiving or reading data transferred by a data producer for example on behalf of one or more consumer applications . In one embodiment for example a first computer server may be a data producer a second computer server may be a data consumer and a third computer server may be a support server . Each computer server may comprise one or more processors coupled to one or more memories as well as one or more I O interfaces such as storage adapters allowing access to storage I O channels network interface cards a mouse keyboard and other peripherals. The one or more processors may be implemented using any desired architecture or chip set such as the SPARC architecture from Sun Microsystems or the x86 compatible architectures from Intel Corporation Advanced Micro Devices etc. The one or more memories may be implemented using any appropriate medium such as any of various types of RAM e.g. DRAM SDRAM RDRAM SRAM etc. . In some embodiments the operating system in use at a data producer may differ from the operating system in use at a data consumer or the operating system in use at a support server . Backing physical storage devices may include any of a number of different types of physical storage devices such as individual disks disk arrays optical devices such as CD ROMs CD RW drives DVD ROMs DVD RW drives flash memory devices various types of RAM and the like.

Although the embodiments above have been described in considerable detail numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.

