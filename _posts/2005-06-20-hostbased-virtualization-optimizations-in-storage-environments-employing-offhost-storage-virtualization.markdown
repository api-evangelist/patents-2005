---

title: Host-based virtualization optimizations in storage environments employing off-host storage virtualization
abstract: A system for host-based virtualization optimizations in storage environments employing off-host virtualization may include a host, one or more physical storage devices, and an off-host virtualizer such as a virtualizing switch. The off-host virtualizer may be configured to aggregate storage within the one or more physical storage devices into a virtual storage device such as a logical volume, and to provide control data for the virtual storage device to the host. The host may be configured to use the control data to perform a function in response to an I/O request from a storage consumer directed at the virtual storage device, and to use a result of the function to coordinate one or more I/O operations corresponding to the I/O request.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07669032&OS=07669032&RS=07669032
owner: Symantec Operating Corporation
number: 07669032
owner_city: Mountain View
owner_country: US
publication_date: 20050620
---
This application is a continuation in part of U.S. patent application Ser. No. 10 722 614 entitled SYSTEM AND METHOD FOR EMULATING OPERATING SYSTEM METADATA TO PROVIDE CROSS PLATFORM ACCESS TO STORAGE VOLUMES filed Nov. 26 2003 and is a continuation in part of Application Number PCT US2004 039306 filed Nov. 22 2004 in the PCT.

This invention relates to computer systems and more particularly to storage environments employing off host virtualization of storage devices.

Many business organizations and governmental entities rely upon applications that access large amounts of data often exceeding a terabyte of data for mission critical applications. Often such data is stored on many different storage devices which may be heterogeneous in nature including many different types of devices from many different manufacturers.

Configuring individual applications that consume data or application server systems that host such applications to recognize and directly interact with each different storage device that may possibly be encountered in a heterogeneous storage environment would be increasingly difficult as the environment scaled in size and complexity. Therefore in some storage environments specialized storage management software and hardware may be used to provide a more uniform storage model to storage consumers. Such software and hardware may also be configured to present physical storage devices as virtual storage devices to computer hosts and to add storage features not present in individual storage devices to the storage model. For example features to increase fault tolerance such as data mirroring snapshot fixed image creation or data parity as well as features to increase data access performance such as disk striping may be implemented in the storage model via hardware or software. The added storage features may be referred to as storage virtualization features and the software and or hardware providing the virtual storage devices and the added storage features may be termed virtualizers or virtualization controllers . Virtualization may be performed within computer hosts such as within a volume manager layer of a storage software stack at the host and or in devices external to the host such as virtualization switches or virtualization appliances. Such external devices providing virtualization may be termed off host virtualizers. Off host virtualizers may be connected to the external physical storage devices for which they provide virtualization functions via a variety of interconnects such as Fiber Channel links Internet Protocol IP networks and the like.

In some storage environments employing virtualization off host virtualizers such as virtualization switches may be capable of performing certain virtualization related functions but may not be flexible enough to handle other virtualization related functions efficiently. For example if an I O request from a host or storage consumer maps to blocks within two different back end physical storage devices e.g. if a write request may require blocks at two different back end SCSI disk arrays to be updated some virtualizing switches may serialize the back end operations instead of performing them in parallel. If the I O request were pre partitioned into multiple I O requests such that each request was limited to a single back end physical storage device the switch may schedule the multiple I O requests in parallel instead of serializing them. In addition for certain types of I O operations where for example more than one network path from the host to a target storage device is available the use of one path by the host may result in more efficient I Os e.g. by traversing fewer network hops or switches than the use of other paths. Information on the multiple paths may be available from the off host virtualizer. A mechanism that allows cooperation between off host virtualizers and host based virtualization software to optimize I O operations may therefore be desirable.

Various embodiments of a system and method for host based virtualization optimizations in storage environments employing off host virtualization are disclosed. According to a first embodiment a system may include a host one or more physical storage devices and an off host virtualizer which may include for example one or more virtualizing switches. The off host virtualizer may be configured to aggregate storage within the one or more physical storage devices into a virtual storage device such as a logical volume and to provide control data for the virtual storage device to the host. In various embodiments the control data may be provided in band e.g. as part of a response to an I O request or out of band e.g. in a message generated according to a custom I O management protocol or using a combination of in band and out of band communication. The host may be configured to use the control data to perform a function in response to an I O request from a storage consumer directed at the virtual storage device and to use a result of the function to coordinate one or more I O operations corresponding to the I O request. The function performed by the host may result in a more efficient response to the I O request e.g. a faster response or a response that requires fewer resources than may have been possible if the function were not performed. Thus the off host virtualizer may be configured to cooperate with the host e.g. with an optimization driver layer of a host software stack to enhance the I O efficiency of the system.

In one specific embodiment the one or more physical storage devices may include a first and a second physical storage device. The control data provided by the off host virtualizer may include a layout or geometry of the virtual storage device allowing the host to identify the physical storage device or devices to which any given logical block of the virtual storage device is mapped. In such an embodiment the host may be configured to detect whether a given I O request from a storage consumer may require physical I O at more than one physical storage device e.g. at both the first and second storage devices. If physical I Os at multiple physical storage devices are required the host may be configured to partition the I O request to multiple physical I O requests such that no single physical I O request requires access to more than one physical storage device. The host may then send the partitioned physical I O requests to the off host virtualizer thereby allowing the off host virtualizer to complete the physical I O requests without requiring I O splitting at the off host virtualizer.

In another embodiment multiple network paths may be available from the host to a given physical storage device. The control data provided by the host may include network path information. The host may be configured to utilize the control data to select a preferred network path to use when accessing a given physical storage device. For example if several virtualizing switches can provide services for the same virtual storage device that is backed by the given physical storage device the host may select the switch that may be most efficient for each block and or each physical I O based on a variety of I O path characteristics and or switch characteristics. The characteristics that may be used to select the preferred network path may include the number of network hops or switches included the load at one or more devices or path links or the specific performance and or functional characteristics of the switches and or host bus adapters included. In some embodiments the host may be further configured to respond to a triggering condition such as a failure or a load imbalance by selecting a different preferred network path. The off host virtualizer may be configured to provide a special error code to the host when a path failure is detected in such an embodiment allowing the host to retry attempts to access a storage device and or to send subsequent I O requests using an alternative network path.

While the invention is susceptible to various modifications and alternative forms specific embodiments are shown by way of example in the drawings and are herein described in detail. It should be understood however that drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed but on the contrary the invention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.

In one embodiment physical storage devices may include physical block devices. Generally speaking a physical block device may comprise any hardware entity that provides a collection of linearly addressed data blocks that can be read or written. For example in one embodiment a physical block device may be a single disk drive configured to present all of its sectors as an indexed array of blocks. In another embodiment a set of individual disk drives may be organized as a single physical storage device in the form of a hardware disk array. A hardware disk array may present data from its constituent disk drives as a single contiguous storage address range to a storage consumer. It is contemplated that any suitable type of storage device may be configured singly or as part of a collection or an array as a physical block device such as fixed or removable magnetic media drives e.g. hard drives floppy or Zip based drives writable or read only optical media drives e.g. CD or DVD tape drives solid state mass storage devices or any other type of storage device. Physical storage devices such as disks or tape drives may be accessible via some form of the Small Computer System Interface SCSI protocol though other protocols and interfaces are possible and contemplated.

In the context of storage management the term virtualization refers to a process of creating or aggregating logical or virtual storage devices out of one or more underlying physical or logical storage devices and making the virtual storage devices accessible to consumers which may be termed virtual device clients for storage operations. When the underlying physical storage devices which may be termed backing storage devices and the aggregated virtual storage devices present a block interface i.e. a collection of linearly addressable blocks the process may be termed block virtualization . For example in one embodiment of block virtualization one or more layers of software may rearrange blocks from one or more physical block devices such as disks and add various kinds of functions. The resulting rearranged collection of blocks may then be presented to a storage consumer such as a database application or a file system as one or more aggregated devices such as logical volumes with the appearance of one or more basic disk drives. That is the more complex structure resulting from rearranging blocks and adding functionality may be presented as if it were one or more simple arrays of blocks or logical block devices. In some embodiments multiple layers of virtualization may be implemented. Thus one or more block devices may be mapped into a particular virtualized block device which may be in turn mapped into still another virtualized block device allowing complex storage functions to be implemented with simple block devices.

Block virtualization may be implemented at various places within a storage environment in both hardware and software. In host based block virtualization a volume manager such as the VERITAS Volume Manager from VERITAS Software Corporation may be implemented within a storage software stack at host . The volume manager may for example form a layer between a disk driver layer and a file system layer. In some storage environments virtualization functionality may be added to host bus adapters HBAs i.e. to devices that provide an interface between the storage software stack at a host and the storage network . Block virtualization may also be performed outside the host e.g. in a virtualization appliance a virtualizing switch or at some other device dedicated to virtualization services. Such external devices providing block virtualization i.e. devices that are not incorporated within host may be termed off host virtualizers or off host virtualization controllers. In some storage environments block virtualization functionality may be implemented by an off host virtualizer in cooperation with a host based virtualizer. That is some block virtualization functionality may be performed off host and other block virtualization features may be implemented at the host. In addition in some embodiments multiple devices external to the host may be configured to cooperate to provide block virtualization services e.g. off host virtualizer functionality may be distributed across a number of cooperating virtualizing switches virtualizing appliances or a combination of virtualizing switches and appliances. Each of the various devices that collectively form the off host virtualizer may be connected to the host by a separate path e.g. a separate set of links in some embodiments.

In various embodiments off host virtualizer may implement numerous different types of storage functions using block virtualization. For example in one embodiment a virtual block device such as a logical volume may implement device striping where data blocks may be distributed among multiple physical or logical block devices and or device spanning in which multiple physical or logical block devices may be joined to appear as a single large logical block device. In some embodiments virtualized block devices may provide mirroring and other forms of redundant data storage the ability to create a snapshot or point in time image of a particular block device at a point in time and or the ability to replicate data blocks among storage systems connected through a network such as a local area network LAN or a wide area network WAN for example. Additionally in some embodiments virtualized block devices may implement certain performance optimizations such as load distribution and or various capabilities for online reorganization of virtual device structure such as online data migration between devices. In other embodiments one or more block devices may be mapped into a particular virtualized block device which may be in turn mapped into still another virtualized block device allowing complex storage functions to be implemented with simple block devices. More than one virtualization feature such as striping and mirroring may thus be combined within a single virtual block device in some embodiments creating a hierarchical virtual storage device.

In one embodiment off host virtualizer may also be configured to present a virtual storage device in the form of a virtual SCSI LUN logical unit to a disk driver within a storage software stack at host . That is off host virtualizer may be configured to emulate metadata within the virtual storage device in a manner that allows a disk driver at host to detect or recognize the virtual storage device as a LUN. In addition off host virtualizer may be configured to map one or more logical volumes to one or more respective address ranges within a virtual LUN VLUN and to provide configuration information for the logical volumes to the host e.g. to a second driver such as optimization driver layered above the disk driver accessing the VLUN to allow I O operations to be performed on the logical volumes. Such a mapping of logical volumes to VLUNs may be termed volume tunneling . The format and location of the emulated metadata within the VLUN may vary with the operating system in use at host i.e. in an environment where multiple operating systems are supported off host virtualizer may need to generate metadata according to a variety of different specifications.

The off host virtualizer either alone or in cooperation with one or more other virtualizers may provide functions such as configuration management of virtualized block devices and distributed coordination of block device virtualization. For example after a reconfiguration of a logical volume accessed from multiple hosts e.g. when the logical volume is expanded or when a new mirror is added to the logical volume the off host virtualizer may be configured to distribute a volume description indicating the reconfiguration to each of the multiple hosts. In one embodiment once the volume description has been provided to the hosts the hosts may be configured to interact directly with various storage devices according to the volume description i.e. to transform logical I O requests into physical I O requests using the volume description . Distribution of a virtualized block device as a volume to one or more virtual device clients such as hosts may be referred to as distributed block virtualization.

In some embodiments off host virtualizer may be configured to distribute all defined logical volumes to each virtual device client present within a system. Such embodiments may be referred to as symmetric distributed block virtualization systems. In other embodiments specific volumes may be distributed only to respective virtual device clients such that at least one volume is not common to two virtual device clients. Such embodiments may be referred to as asymmetric distributed block virtualization systems.

As noted above in some embodiments off host virtualizer may include one or more virtualizing switches such as intelligent fibre channel switches. In addition to providing the interconnection functionality provided by a standard switch device a virtualization switch may typically include one or more ASICs Application Specific Integrated Circuit or other circuitry programmed to provide desired virtualization features. In some embodiments a virtualization switch may include one or more memory modules in which virtualization related data structures such as volume layout information may be stored. In one embodiment a virtualizing switch may provide an application programming interface API to control aspects of the virtualization functionality provided e.g. to load desired volume layout information from an external volume configuration database. The specific virtualization features supported and the processing and or memory capacity of a given virtualizing switch may differ from one switch product to another and from vendor to vendor. Some virtualization switches may be relatively inflexible in the virtualization features supported for example a switch may support mirroring but may limit the maximum number of mirrors that a virtual storage device may include e.g. to four mirrors . Typically virtualizing switches may be designed to pass on I O requests to back end physical storage devices as soon as possible after the I O request is received and similarly to pass the results of I O operations back to the requesting front end client or host as soon as possible. That is the switches may typically not be configured to store the data corresponding to an I O request e.g. the data blocks written or read which may result in complicated error handling under some circumstances as described below in further detail.

In many storage environments employing off host virtualizers especially when the off host virtualizer includes virtualization switches with relatively limited processing and or memory capabilities as described above certain storage operations may be performed more efficiently if the off host virtualizer is configured to cooperate with optimization driver at the host . The term optimization driver as used herein refers to any software module or layer within a host that is configured to receive control data for virtual storage device from off host virtualizer to use the control data to perform a function or operation in response to an I O request from a storage consumer and to coordinate one or more I O operations to complete a response to the I O request using a result of the function. For example the optimization driver may be a layer of software above a disk driver and below a file system within a storage software stack at host in one embodiment. In another embodiment the optimization driver may be incorporated within a volume manager. The I O operations performed using the results of the function may result in the I O request being satisfied in a more efficient manner e.g. faster and or with a lower level of resource utilization than if the function had not been performed. In various embodiments the control data may be provided to the host in band e.g. as part of a response to an I O request or out of band e.g. in a message generated according to a custom I O management protocol or using a combination of in band and out of band communication.

The content of the control data provided to optimization driver may differ in different embodiments. In one embodiment the control data may include the layout of the virtual storage device including a description of the stripe geometry and the number of mirrors and the mapping of stripes and or mirrors to physical storage devices. Using the control data the host i.e. optimization driver at the host may detect that a single I O request from a storage consumer may require access to multiple physical storage devices. For example in one configuration of a striped virtual storage device blocks numbered 0 999 of the virtual storage device may be stored as part of a first stripe on a first disk device while blocks numbered 1000 1999 may be stored in a second stripe on a second disk device. If an I O request from a storage consumer is directed to block through block and the stripe geometry of the virtual storage device is provided to the host by the off host virtualizer host may be configured to detect that two physical I Os may be required one at each of the first and second disk devices. In some embodiments the off host virtualizer may not provide efficient support for splitting an I O request that spans multiple physical I O devices and delivering the split requests to the physical I O devices for example some virtualizing switches may have to serialize the split I O requests rather than executing them in parallel. In contrast if the host splits up the spanning I O request and sends the split requests to the off host virtualizer such that each I O request received by the off host virtualizer requires access to a single physical I O device the off host virtualizer may be able to hand off the split requests to the physical storage devices more efficiently e.g. in parallel .

In order to complete a response to the I O request from the storage consumer in one embodiment host may be configured to pass on the physical I O requests to off host storage virtualizer and to ensure that each physical I O request has successfully completed before returning a response to the storage consumer. For example in an embodiment where off host virtualizer is a virtualization switch and the I O request has been partitioned host may send each of the split or partitioned I O requests to the virtualization switch. As none of the partitioned I O requests may require access to more than one target physical storage device the virtualization switch may be configured to hand off the multiple partitioned I O requests to the physical storage devices in parallel and to send completion indications or results to host when the corresponding physical I Os are complete. Host may be configured to gather the results which may include data blocks read in response to a read I O request from the storage consumer and to send a single I O response to the storage consumer after consolidating the results corresponding to all the partitioned physical I O requests. In other embodiments for example where host may be configured to access one or more physical storage devices through a network path that does not include off host virtualizer host may be configured to issue the partitioned physical I O requests and gather the results without further interaction with off host virtualizer .

In one embodiment multiple paths from host to a given physical storage device backing a virtual storage device may be available with one path providing faster access or utilizing fewer intermediate hops than another. In such an embodiment the control data provided to host by off host virtualizer may include both volume layout information and network path information allowing host to make a selection of a preferred network path to be used to access a given block of data.

As noted above the control data for virtual storage device provided to host by off host virtualizer in embodiments such as those depicted in may include layout information and network path information. When an I O request from a storage consumer is received the layout information may allow host to identify the specific physical storage device or devices at which a physical I O operation corresponding to the I O request may be required. The network path information which may include a list of the alternate paths available to access the identified physical storage device may allow host to select a preferred network path including a preferred switch. That is the control data dependent function performed by host may be a selection of a preferred network path. Host A may for example be able to choose between switches A and B when accessing a block at physical storage device A. In some embodiments the host may be operable to select only the first device such as switch A or B along an I O path while one or more devices along the path may in turn be operable to select the remaining hops to the physical storage device. In other embodiments hosts may be provided more complete I O path information allowing them to select additional hops links or switches. It is noted that in the subsequent description the term switch may be used generically to refer to virtualizing switches as well as to non virtualizing switches i.e. to switches that are not configured to provide virtualization functionality.

Any of a number of different criteria may be used by optimization driver at host to select the preferred network path in different embodiments. For example in one embodiment the preferred network path may be selected based on the number of links or hops within the path that is optimization driver may be configured to select the path with the fewest links among the available paths. If no single path contains fewer links than all other paths any one of the paths with the fewest links may be selected. In another embodiment switches may differ from one another in performance capabilities and or functional characteristics and host may be provided specifications describing the differences between the switches . For example switch B may be faster or provide higher aggregate bandwidth than switch A. In such embodiments the criterion used by optimization driver to select the preferred network path may include a selection of a preferred switch based on the known performance capabilities and or functional characteristics of the different switches .

For some I O requests as noted above multiple physical I O operations may be required e.g. if virtual storage device is a striped or mirrored device a logical write request may require multiple physical write operations . In one embodiment each physical I O operation may require a selection of a preferred network path. After a preferred network path has been selected using the control data provided by off host virtualizer optimization driver may be configured to generate a request for the physical I O operation and to route the physical I O operation request via the preferred network path. When the physical I O request or requests complete successfully the optimization driver may be configured to receive the results and to pass the data for a read or completion status for a write to the requesting storage consumer.

In some embodiments the physical storage devices backing a given virtual storage device may be accessible via more than one HBA for example a first subset of physical storage devices may be accessible via a first HBA and a second subset of physical storage devices may be accessible via a second HBA. is a block diagram illustrating an exemplary storage system configuration where host includes a first HBA A and a second HBA B according to one embodiment. In the illustrated embodiment host may access physical storage devices A C via HBA A and physical storage devices C and D via HBA B. Virtual storage device may be backed by physical storage devices A D. In such embodiments the control data provided by off host virtualizer may include a mapping between HBAs and physical storage devices as well as layout information for the virtual storage device allowing optimization driver to select a preferred HBA to use in responding to an I O request from a storage consumer. That is the control data dependent function performed by optimization driver at host may be a selection of a preferred HBA. In some embodiments a preferred HBA may be selected from among two or more alternatives using criteria similar to those described earlier for selecting preferred switches or preferred network paths e.g. performance or functional capabilities.

In some embodiments where multiple switches may be used to access a given physical storage device backing virtual storage device special handling for write operations may be implemented especially in environments that support write intensive applications. In such environments read operations may be directed to any of the switches that may have access to the targeted physical storage device . In contrast for write operations it may be desirable to divide the address space of the virtual storage device into one or more partitions and to designate a different switch as a write coordinator responsible for performing all write operations targeted at the partition. That is all write operations targeted at a given partition may be directed to the switch serving as the write coordinator for the partition. Such a technique may provide several advantages such as improved write load balancing across switches and a reduction in the amount of communication required for concurrency control i.e. a reduction in the amount of locking related network traffic that may be required when multiple concurrent storage consumers attempt to update a given data block . is a block diagram illustrating an embodiment where off host virtualizer may be configured to divide the address space of virtual storage device into two partitions P and P and to designate a respective switch as a write coordinator for each partition switch A for partition P and switch B for partition P .

The address range of virtual storage device may be divided into partitions for write coordination using a variety of techniques. In one embodiment a modulo technique may be employed that is logically successive ranges of blocks of virtual storage device may be placed in successive partitions. For example blocks may be placed in partition P blocks in partition P blocks in partition P etc. In another embodiment the address space of virtual storage device may simply be divided into N consecutive sub ranges for distribution among N partitions e.g. if virtual storage device comprises blocks numbered 0 99999 and N 2 blocks may be placed in partition P and blocks in partition P. In general any number of partitions of the virtual address space may be created and distributed among different switches for write coordination as desired.

The control data provided by off host virtualizer to optimization driver in such embodiments may include the write coordinator map identifying different partitions and associated write coordinators. When a write request is received from a storage consumer optimization driver may use the write coordinator map to identify the appropriate switch to which a corresponding physical write request may be directed. Thus the control data dependent function performed by a host in such embodiments may include the selection of a target write coordinator. As write requests targeted at different blocks of the address space of virtual storage device are received optimization driver may distribute them to the appropriate write coordinator switches thereby distributing the write workload across the switches. In some embodiments the write coordinators may also be configured to implement mirrored writes without requiring a host to generate multiple write requests. That is if a virtual storage device includes multiple mirrors a single request to update a given block may be generated by optimization driver and sent to the appropriate write coordinator switch the write coordinator may then generate physical write operations for all the mirrors. Multiple hosts such as hosts A and B may be provided the write coordinator map and each of the hosts may be configured to direct write operations to the appropriate write coordinator switch that is writes to a particular partition that may have originated at any of the multiple hosts may be directed to the same write coordinator switch in some embodiments.

When multiple concurrent write requests directed at a given block or overlapping range of blocks of virtual storage device are received by a host all such write requests may be directed to a single write coordinator. The write coordinator may then serialize the corresponding physical write operations thereby ensuring that concurrent write operations do not overwrite each other s data or otherwise cause data corruption. Since each block of virtual storage device may be associated with only one write coordinator switch no locking related messages e.g. distributed locks may need to be exchanged between different write coordinators to provide concurrency control using this technique. For certain long lasting atomic operations such as a mirror recovery operation that may require a large number of writes to be completed in one embodiment a special interface may be implemented to allow write coordinator switches to hold a lock on a given block until the entire long lasting atomic operation completes. It is noted that while the technique of employing write coordinators has been described above in the context of storage environments employing switches in other embodiments devices other than switches such as additional hosts intelligent disk array devices or intelligent storage appliances may also be used as write coordinators.

In some embodiments a host and off host virtualizer may be configured to cooperate to provide additional virtualization functions such as enhanced error recovery capabilities. As noted above in one embodiment where off host virtualizer includes a virtualizing switch the switch may be designed to pass on I O requests to back end physical storage devices as soon as possible after the I O request is received and to pass the results of I O operations back to the requesting front end client or host as soon as the results become available from the physical storage device. That is the switch may typically not be configured to store the data corresponding to an I O request e.g. the data blocks written or read . In such environments cooperation between the off host virtualizer and host may allow an improved response to certain kinds of failures and or errors such as read errors at mirrored virtual storage devices and network path errors.

In one embodiment where the virtual storage device is a mirrored logical volume including two or more mirrors i.e. where two or more copies of the data of the logical volume may be stored at separate physical storage devices the off host virtualizer may be configured to send a particular error code to host upon the detection of a read error at a first mirror. The particular error may indicate that an alternate redundant copy of the targeted data blocks i.e. the blocks of volume intended to be read is available at the second mirror. The host may be configured to send a second read request directed at the second mirror upon receiving the particular error code instead of for example passing an error indication on to a requesting storage consumer. In another embodiment the particular error code may simply indicate that the read error occurred at a mirrored logical volume without specifying that an alternate copy is available or which physical storage devices include the alternate copy. In such an embodiment the host may be configured to send a query to the off host virtualizer upon receiving the particular error code requesting an indication that an alternate copy of the targeted data blocks is available. In response to the query the off host virtualizer may be configured to verify that a redundant alternate copy of the data blocks is available and accessible by the host e.g. at the second mirror. The off host virtualizer may then send a response to the query e.g. including an identification the second mirror and the host may send a request to read the redundant copy.

A similar technique may be used in the presence of network path failures in embodiments where redundant paths may be available to the same physical storage device . For example if a network link between a switch and a physical storage device fails preventing a successful completion of a physical I O request in one embodiment switch may be configured to send a particular error code to host . The particular error code may identify the nature of the error as well as the specific link that failed or a set of links that were part of the network path on which the write operation was sent. In response to receiving the particular error code in one embodiment host may be configured to select an alternative network path and re issue the I O request along the selected alternative network path instead of passing an error indication on to a requesting storage consumer. In another embodiment the particular error code may simply indicate that a network path error has occurred and the host may be configured to send a query to off host virtualizer requesting an indication of an alternate network path if such a path exists and is functioning. The off host virtualizer may send a response to the query including an identification of one or more alternate paths and the host may then re issue the request using one of the specified alternate paths. In one embodiment the error recovery operation for a write operation may include the host simply re issuing or retrying the failed write request up to a specified number of times allowing the switch to attempt the write operation again e.g. over alternate paths. It is noted that while some traditional host based disk drivers may also be configured to attempt up to a fixed number of retries for failed operations the disk drivers may often be unaware of the number of mirrors or alternate paths that may be currently available. If for example the traditional disk driver is configured to retry an I O up to two times but there are three or more alternate mirrors or three or more alternate paths to the data the number of retries attempted by the disk driver may be insufficient to complete the I O e.g. the disk driver may stop attempting retries after two attempts even though a third attempt may have succeeded . In contrast the error code or other control data provided to optimization driver by off host virtualizer may allow the host to overcome the kinds of restrictions typically found in traditional disk drivers e.g. by allowing the host to retry the I O based on the current number of alternative paths or mirrors available.

The technique of selecting a second or alternative network path using control data provided by off host virtualizer to perform a desired I O operation in response to a detection of a failure may also be applied in response to other triggering conditions in other embodiments. is a flow diagram illustrating aspects of the operation of system according to one embodiment where multiple paths are available between a host and a physical storage device and the host is configured to switch from using a first network path to the physical storage device to using a second network path. As shown in block off host virtualizer may be configured to provide control data on virtual storage device to host including network path information indicating the multiple paths allowing access to a given physical storage device . Using the control data host may select a first preferred network path block for example using one of the selection criteria described above such as a minimum number of hops or links . I O operations may then be performed using the first preferred network path until a condition triggering a change to a second network path is detected by the host block .

The triggering condition may differ in different embodiments for example in one embodiment only an error or a failure may trigger a change in the preferred network path while in another embodiment a load imbalance may also trigger the change. An indication of a load imbalance may be detected using any suitable technique e.g. using statistics periodically collected from one or more switches or using sniffers or other devices to measure traffic along links . The criteria used to determine whether load has become unbalanced and algorithms to select a particular alternate network path to use if multiple alternate paths are available may be specified as part of a load balance policy implemented within system . Various implementation details related to load balancing such as the metrics used to measure load and the amount by which load over one network path or switch has to differ from the load over an alternate path for a load imbalance to be detected etc. may be included in the load balance policy in different embodiments. Once the triggering condition has been detected the host may switch to a second preferred network path block for I Os directed at the physical storage device . In one embodiment host may also be configured to send an indication e.g. via an out of band message or as a parameter or flag associated with an I O request of the second preferred network path to off host virtualizer and off host virtualizer may be configured to use the second preferred network path in subsequent I O operations directed at the physical storage device .

In one embodiment a transition from using one network path to another may require cache cleanup at one or more switches . is a block diagram illustrating one embodiment where each switch in storage network includes a respective read cache e.g. switch A includes read cache A switch B includes read cache B etc. Any given read cache may be used to store data read over a network path including the corresponding switch . For example if a block B of a virtual storage device is read from physical storage device A over a network path including switches A and C a copy of block B may be stored in cache A and or cache C with a goal of reducing the latency required to access block B if it is later re read. In embodiments employing read caches in this manner a change to a second network path may potentially result in stale data being read. For example if block B is initially cached at read cache C and if host chooses a second network path including switches B and D as a preferred network path because of a load imbalance the copy of data block B that is cached at read cache C may become out of date or stale if block B is updated after the change of preferred network path. If later cache C becomes part of a preferred network path again e.g. if a second change in network path is triggered by a second load imbalance or by a failure and block B remains within cache C the stale version may inadvertently be returned to a storage consumer. In such embodiments as shown in block of host may be configured to coordinate a cache cleanup along the initial network path. E.g. in when host changes the preferred network path from a first path including switches A and C to physical storage device A to a second path including switches B and D host may issue one or more cache invalidation requests to switches A and C so that blocks of data of physical storage device A may be invalidated in or purged from caches A and C. In one embodiment the cache invalidation requests may be sent over a network other than storage network e.g. over an IP Internet Protocol network to which host and switches may be linked. It is noted that in some embodiments not all switches of the storage network may include read caches so that cache cleanup may not be required at all switches on a particular network path.

In other embodiments host may be configured to perform additional functions to assist off host virtualizer in responding to I O requests from storage consumers. is a block diagram illustrating an embodiment where configuration information for virtual storage device is maintained in an external configuration database . In the illustrated embodiment configuration information required to perform physical I O operations such as volume layout information may be stored outside off host virtualizer which may comprise a single switch A for example because of limited memory space available at memory at the off host virtualizer. The control data provided to host by off host virtualizer may include an identification of the configuration database and the virtual storage device allowing host to communicate with configuration database . In such an embodiment the function performed by host corresponding to block of may include a loading a portion of the configuration information into memory at off host virtualizer . That is in response to receiving an I O request from a storage consumer host may be configured to ensure that configuration information needed to perform the physical I O corresponding to the I O request is loaded at off host virtualizer e.g. over a network link . Configuration database may be maintained at a separate host in some embodiments and or may be incorporated within host in other embodiments.

In one specific embodiment host may also be configured to maintain a Dirty Region Log or DRL associated with a virtual storage device . Dirty region logging is a technique that may be used to reduce the amount of time taken to resynchronize the mirrors of a mirrored virtual storage device e.g. after a failure that leaves the mirrors inconsistent with each other. In embodiments where virtual storage device comprises one or more mirrors the address space of the virtual storage device may be logically divided into a number of regions. Before a write is performed to a block of the virtual storage device the region corresponding to the block may be marked dirty in the DRL. If the mirrors become inconsistent only those regions that are marked dirty in the DRL may be copied from one designated source mirror to the remaining mirrors instead of copying the entire virtual storage device. Control data on the layout of the virtual storage device provided by off host virtualizer to host may allow host to construct the DRL e.g. as a bitmap with one bit representing each region. By maintaining and updating the DRL at host instead of at the off host virtualizer itself the tasks required from off host virtualizer for a write operation to a mirrored virtual storage device may be simplified e.g. off host virtualizer may not need to block or enqueue physical write operations while the DRL is updated. In such embodiments the control data dependent function performed at host may be an update of the DRL. While the embodiment illustrated in includes a DRL and a configuration database it is noted that these two features may be implemented independently of one another i.e. in general embodiments including a configuration database need not include a DRL and embodiments including a DRL need not include a configuration database .

It is noted that while the embodiments described above have included a single virtual storage device in other embodiments multiple virtual storage devices may be used. In such embodiments off host virtualizer may be configured to aggregate storage from physical storage devices into a plurality of virtual storage devices . Further control data for one or more of the aggregated virtual storage devices may be provided to more than one host either symmetrically where each host gets the same control data or asymmetrically where different hosts are provided different sets of control data. A host may be any desired computer system capable of executing optimization driver including one or more processors one or more memories peripheral devices such as monitors keyboards mice and I O interfaces such as network adapters disk adapters and the like. A host may also include locally attached physical storage devices such as disks. A storage consumer may be any type of application or device that requires data access such as a file system or an application. A storage consumer may be incorporated within host in one embodiment while in other embodiments the storage consumer may be executed at a remote device or a client computer system other than host and may interact with host over a network such as a local area network LAN or wide area network WAN . In additional embodiments a storage consumer may be an embedded system configured to use application specific integrated circuit ASIC or field programmable gate array FPGA technology to execute operations whereby a given storage device may be accessed.

Storage network may be implemented using any suitable technology capable of supporting the functionality described above such as Fibre Channel Internet SCSI and the like. Off host virtualizer may be implemented using any desired device such as a virtualizing switch a virtualization appliance or additional hosts configured to provide virtualization functionality. A virtualizing switch may be an intelligent fiber channel switch configured with sufficient processing capacity to perform desired virtualization operations in addition to supporting fiber channel connectivity. A virtualization appliance may be an intelligent device programmed to perform virtualization functions such as providing mirroring striping snapshot capabilities etc. Appliances may differ from general purpose computers in that their software is normally customized for the function they perform pre loaded by the vendor and not alterable by the user. In some embodiments multiple devices or systems may cooperate to provide off host virtualization e.g. multiple cooperating virtualization switches and or virtualization appliances may form a single off host virtualizer . Numerous other configurations of off host virtualizers storage network and hosts are possible and contemplated.

Although the embodiments above have been described in considerable detail numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.

