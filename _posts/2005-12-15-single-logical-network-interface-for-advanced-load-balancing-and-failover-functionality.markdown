---

title: Single logical network interface for advanced load balancing and fail-over functionality
abstract: The invention sets forth an approach for aggregating a plurality of NICs in a computing device into a single logical NIC as seen by that computing device's operating system. The combination of the single logical NIC and a network resource manager provides a reliable and persistent interface to the operating system and to the network hardware, thereby improving the reliability and ease-of-configuration of the computing device. The invention also may improve communications security by supporting the 802.1X and the 802.1Q networking standards.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08572288&OS=08572288&RS=08572288
owner: Nvidia Corporation
number: 08572288
owner_city: Santa Clara
owner_country: US
publication_date: 20051215
---
Embodiments of the present invention relate generally to the field of computer networking and more specifically to a single logical network interface with advanced load balancing and fail over functionality.

A network computing device oftentimes has two or more network interface cards to increase the computing device s communication bandwidth beyond what a single NIC is able to provide. Such a plurality of NICs is commonly referred to as a team of NICs. Typically the team shares a common Internet Protocol IP address while maintaining unique Media Access Control MAC addresses for each NIC within the team. One aspect of using this team configuration is that network traffic between the computing device and other computing devices in the network may be distributed among the NICs in the team such that the overall throughput of the team may be maximized. This type of operation is referred to as load balancing. Another aspect of using a team configuration is that network traffic may be migrated from a nonfunctional or unreliable NIC within the team to a functional or more reliable NIC within the team. This type of operation is referred to as fail over. The combination of load balancing and fail over in a single implementation is typically referred to as LBFO. 

For an LBFO implementation to set up and migrate network connections between NICs to optimize the computing device s network communications throughput and reliability the LBFO implementation may monitor operational parameters at the NIC level such as the error rate on each NIC and the amount of data exchanged through each network connection. Typical LBFO implementations locate their LBFO software in the operating system and request the NIC level operational parameters from the NIC hardware through one or more device drivers. The resulting architectures are inherently inefficient because they place the LBFO decision making and management functionalities several software levels away from where the operational parameters are monitored requiring a substantial amount of ongoing communication between software levels. Further to decrease the communication required between software levels and or because the LBFO implementations in operating systems are not fully optimized for LBFO the LBFO decision making elements in prior art LBFO implementations may not utilize all operational parameters available at the NIC level. Thus LBFO implementations residing in operating systems may be lower performing due to excessive software communications between software levels and LBFO decision making based on limited NIC level operational information.

In addition for an LBFO implementation to be reliable and to maintain high performance its configuration should not be subject to change by either the operating system or some third party such as a user or an application program. Typical LBFO implementations expose each NIC in the team to the operating system giving the operating system a substantial degree of control over the NICs including the ability to reconfigure any of the NICs. In such implementations the operating system may be able to change the IP address associated with the team of NICs or change the security settings or communications parameters for a particular NIC. Further to the extent the NICs are also exposed to third parties as a result of their exposure to the operating system a third party may be able to similarly reconfigure the NICs. For example a user may be able to mistakenly disable or reconfigure a reliable NIC or mistakenly enable an unreliable NIC. As a general matter a team of NICs is configured to optimize a particular LBFO architecture. Unilaterally reconfiguring a NIC once that NIC has been configured for a team may very well undermine that optimization thereby lowering the performance and or reliability of the computing device s network communications.

Further for a LBFO implementation to be most useful in modern networks the implementation should account for additional security features that may be in use in the network. One such security feature is the Institute of Electrical and Electronics Engineers IEEE security credentials protocol hereafter referred to as 802.1X which improves network security by requiring a NIC to request authentication from a security credential server before the NIC may communicate with a switch. Another security feature is the IEEE Virtual LAN protocol hereafter referred to as 802.1Q which may also improve network security by allowing a network administrator to configure a range of IP addresses as a virtual LAN VLAN and selectively assign machines to the VLAN thereby enabling communications between machines in the VLAN and preventing communications between machines inside the VLAN and those outside the VLAN. Each of these protocols imposes additional constraints on the LBFO implementation. Consequently one drawback of existing LBFO implementations is that they do not always implement LBFO within the constraints of the aforementioned security 802.1X and 802.1Q protocols.

As the foregoing illustrates what is needed in the art is an LBFO architecture that addresses one or more of the drawbacks of existing LBFO implementations set forth above.

One embodiment of the invention sets forth a method for configuring a computing device having an operating system and a first network interface card NIC and a second NIC configured as a team to support one or more network connections using load balancing and fail over techniques. The method includes the steps of creating a first function driver associated with the first NIC and a second function driver associated with the second NIC coupling the first function driver to a software stack within the operating system through a first software binding coupling the second function driver to the software stack through a second software binding creating a virtual function driver associated with the team of NICs and coupling the virtual function driver to the software stack through a third software binding.

One advantage of the disclosed method is that the resulting architecture exposes a single logical NIC to the operating system by using a bus architecture in a novel and efficient manner to create and manage function drivers software bindings and software handles. The single logical NIC and a network resource manager together perform load balancing fail over and fail back independent of the operating system which improves the reliability and networking throughput of the computing device. The single logical NIC also supports the 802.1X and 802.1Q networking standards which may improve the security of communications between the computing device and other machines in a computer network.

One embodiment of the present invention is a computer readable medium storing instructions for causing a computing device having an operating system and a first network interface card NIC and a second NIC configured as a team to configure itself to support one or more network connections using load balancing and fail over techniques by performing the steps of creating a first function driver associated with the first NIC and a second function driver associated with the second NIC where the first function driver is configured to be coupled to a software stack within the operating system through a first software binding and to a network resource manager through a first software handle and the second function driver is configured to be coupled to the software stack through a second software binding and to the network resource manager through a second software handle and creating a virtual function driver associated with the team of NICs where the virtual function driver is configured to be coupled to the software stack through a third software binding and to the network resource manager through third software handle.

The operating system includes a TCP IP stack an interface object shown as App OS sockets interface object and a software interface object shown as network stack interface object NDIS among other software components that have been omitted from for clarity. The TCP IP stack is coupled to the interface object through a software interface and is coupled to the interface object through a software interface .

The single logical NIC includes a virtual function driver VFD a function driver FD an FD and an FD . The NRM includes a NRM application programming interface API a NRM network function software and a hardware abstraction layer HAL . The NRM network function software is coupled to the HAL through a software interface and is coupled to the NRM API through a software interface . The multi NIC device includes an external hardware interface shown as interface to chip internals to which the NIC the NIC and the NIC are coupled through hardware interfaces and respectively. The hardware interface is coupled to the HAL of the NRM through a hardware software interface .

The bus driver is coupled to each of the NRM the VFD the FD the FD and the FD through software handles and respectively. As described in greater detail below for each NIC a software handle and as the case may be establishes a communication channel between the function driver for that NIC and a software object in the NRM representing that NIC. Similarly a software handle establishes a communication channel between the VFD and a software object in the NRM representing the team of NICs. The VFD is coupled to the TCP IP stack through software binding .

The computer network further includes a switch and a remote machine . The switch is coupled to the remote machine through a network interface and is coupled to NICs and through network interfaces and respectively.

As set forth in greater detail herein the present invention provides a single logical NIC to the operating system . This single logical NIC and the NRM together perform load balancing fail over and fail back independent of the operating system which improves the reliability and networking throughput of the computing device . The single logical NIC also supports the 802.1X and 802.1Q networking standards which may improve the security of communications between the computing device and other machines in the network . Additionally the relevant device drivers and hardware components within the computing device are configured to automatically generate and persistently maintain the single logical NIC . The resulting configuration is automatically and persistently maintained to ensure consistent and reliable configuration upon system reboot.

Typically an operating system communicates with hardware devices through one or more device drivers that directly control their corresponding hardware devices. Device drivers may also communicate with hardware devices through one or more intermediate device drivers to indirectly control their corresponding hardware devices. One example of such indirect control is when a device driver controls a hardware device on a hardware bus through a coupling to an intermediate device driver for the hardware bus such as a digital camera device driver controlling a digital camera on a Universal Serial Bus USB through a coupling to a device driver for the USB. The USB device driver directly controls the communications on the bus and the digital camera device driver through its communications with the USB device driver indirectly controls the communications with the digital camera over the bus. Such an architecture often creates intricate software and hardware hierarchies that must be properly managed to achieve the desired interactions between the operating system and the hardware devices within the system. The technical complexity of such software and hardware hierarchies prompted the development of different software architectures to manage software hardware communications in a consistent object oriented manner.

One such software architecture is called the bus architecture which provides generic software objects for creating a hierarchical device driver architecture that is well suited for indirectly controlling hardware devices on a hardware bus. The bus architecture typically provides a generic bus driver software object for controlling the hardware bus and a generic function driver software object for indirectly controlling each device on the hardware bus. The generic bus driver and generic function driver are typically supplemented with additional device specific software provided by a hardware manufacturer that allows the generic bus driver or generic function driver to communicate with device specific hardware features associated with the hardware bus or device on the hardware bus respectively. Such supplemental software creates a device specific bus driver or a device specific function driver referred to herein simply as a bus driver or a function driver. Additionally the bus driver includes software that identifies which hardware devices are coupled to the hardware bus creates function drivers for those hardware devices and couples the function drivers to the bus driver through software interfaces. Once created each function driver either couples itself to its corresponding hardware device through a software handle to the hardware device or couples itself to an intermediate software object representing the hardware device through a software handle to the intermediate software object. The function driver also couples itself to the operating system through a software binding to the operating system provided by a software interface object in the operating system such as the NDIS . Since the function driver is either configured with the specific functionality it needs to manage its associated hardware device or is able to access that functionality from another entity within the system e.g. through network oriented calls to the operating system the function driver is able to act as an interface object indirectly coupling the operating system and its corresponding hardware device.

Importantly the degree of control the operating system has over a given function driver determines the operating system s degree of control over the hardware device corresponding to that function driver. Since the bus driver controls the creation of the different function drivers within the system and the function drivers create the software interfaces between themselves and the operating system the bus architecture may be used to control the operating system s interactions with different hardware devices by directly controlling which function drivers are created and by indirectly controlling how those function drivers are coupled to the operating system. Further once configured the bus architecture persistently maintains the configuration of the bus driver the function driver s and their couplings by storing this information in the registry or an external data file to ensure that the hierarchical driver configuration is reliably recreated when the computing device reboots. The methods for storing configuration information in the registry or in an external data file are well known to those skilled in the art. Thus the bus architecture provides a sophisticated software architecture for creating and maintaining hierarchical persistent device drivers and the software architecture may be used to control the operating system s interactions with the hardware devices corresponding to the device drivers.

The present invention utilizes the bus architecture in a novel manner to represent a team of NICs to the operating system as a single logical NIC. The single logical NIC includes a function driver for each NIC in the team and an additional function driver called a virtual function driver for handling conventional networking communications between the operating system and the team of NICs. By configuring all conventional network traffic and configuration communications to flow between the operating system and the virtual function driver rather than between the operating system and the individual function drivers or NICs the operating system is effectively prevented from reconfiguring the individual NICs or team of NICs and from attempting to distribute network traffic between the individual NICs within the team. As described in greater detail herein this configuration is accomplished by selectively removing all bindings between the function drivers and the operating system by a user level process and through the introduction of an intermediate software object the NRM to intelligently control the configuration and communications of the NICs.

Referring again to the NRM enables the configuration and management of the team of NICs and and the intelligent provision of load balancing fail over and fail back functions to improve the networking throughput and reliability of the computing device . The NRM provides a handle representing the team of NICs and to the virtual function driver thereby allowing conventional network traffic and configuration communications between the VFD and the NRM . Within the NRM the network traffic may be distributed across the individual NICs and . The NRM may also transmit status information related to the team of NICs and such as an aggregate communications throughput or an aggregate link status indication to the operating system through the VFD . Additionally as described in greater detail below the NRM may provide handles to the individual NICs and to enable advanced networking features that require NIC level communications such as 802.1X. Thus the single logical NIC which includes the VFD the FD the FD and the FD works together with the NRM to create a virtual interface between the operating system and the team of NICs and that limits the ability of the operating system to interact with individual NICs and . For example as the different NICs and either fail or return to operation the NRM may reconfigure network traffic to recognize the removal or addition of a particular NIC without any user or operating system visibility into the reconfiguration. Further this structure allows the NRM to intelligently configure the NICs and and efficiently manage their respective network connections to optimize load balancing fail over and fail back functions as the case may be.

When setting up the architecture illustrated in the bus driver creates the FD the FD and the FD and then couples itself to each of these function drivers through the software handles and respectively. Initially each of the FD the FD and the FD couples itself through a software binding to the TCP IP stack provided by the NDIS . Further the FD couples itself to a software object within the NRM that represents the NIC not shown through the handle provided by the NRM API thereby creating a communication channel between the FD and the NIC the FD couples itself to a software object within the NRM that represents the NIC not shown through the handle provided by the NRM API thereby creating a communication channel between the FD and the NIC and the FD couples itself to a software object within the NRM that represents the NIC not shown through the handle provided by the NRM API thereby creating a communication channel between the FD and the NIC . Each of these communication channels may be used in instances where the operating system communicates directly with the individual NICs and instead of the team of NICs such as when 802.1X security credential requests are communicated as described in greater detail below.

Additionally the bus driver creates the VFD to provide the interface to the operating system for the single logical NIC . Further the VFD couples itself to a software object within the NRM that represents the team of NICs and not shown through the handle provided by the NRM API and to the TCP IP stack through the binding provided by the NDIS thereby creating a communication channel between the TCP IP stack and the team of NICs. This communication channel may be used in instances where the operating system communicates directly with the team of NICs such as when conventional TCP IP communications are managed and processed by the TCP IP stack within the operating system .

Importantly a user level process removes the bindings between the TCP IP stack and each of the FD the FD and the FD after the bus driver creates these function drivers. Consequently as shown in the figure the only binding between the operating system and the single logical NIC is the binding . Thus the binding is the single software interface between the operating system and the single logical NIC for conventional networking communications and for configuration and status information communications. Removing the bindings between the operating system and the FD the FD and the FD ensures that the operating system and third parties are prevented from unilaterally reconfiguring the NICs and . As described earlier the resulting configuration is persistently stored by the underlying storage mechanism incorporated in the bus architecture software. On reboot the configuration of the computing device is read from storage and the bus driver VFD the FD the FD and the FD the binding and the couplings and are recreated.

The NRM contains the NRM network function software which includes a local TCP IP stack not shown and performs NIC optimization functions and all dedicated networking functions performed by the NRM . Thus TCP IP connections may be managed and processed either locally by the NRM or conventionally by the TCP IP stack within the operating system . However as persons skilled in the art will recognize incorporating this type of functionality into the NRM enables the NRM to improve the communications throughput of the multi NIC device by optimizing its configuration and managing network connections across the NICs and better than the operating system could through a conventional device driver architecture. For example the NRM has information available to it that is not normally available to the operating system such as the amount of traffic being transmitted through specific network connections. This type of information enables the NRM to make more informed network management decisions than the operating system especially in the context of load balancing and fail over. Further the NRM includes the hardware abstraction layer which isolates the hardware of the multi NIC device from the NRM network function software thereby insulating the NRM network function software against subsequent hardware changes to the multi NIC device .

As previously described herein the NRM also is configured to report status information for the single logical NIC to the operating system through the VFD . For example the single logical NIC may report a throughput to the operating system that represents the aggregate throughput of the individual NICs and within the team. Thus if the individual throughput of each of the NICs and were 100 megabits per second Mbps then the throughput of the single logical NIC reported to the operating system would be 300 Mbps. The NRM is configured to adjust the aggregate throughput of the single logical NIC as NICs are added or removed from the team. For example if a fourth NIC also having a throughput of 100 Mbps were added to the team the throughput of the single logical NIC would be reported to the operating system as 400 Mbps. Likewise if NIC were to fail then the throughput of the single logical NIC would be 200 Mbps. Further the NRM may report an aggregate link status indication to the operating system indicating whether one or more of the NICs and is enabled. For example if one or more of NICs and were active then the aggregate link status reported to the operating system would indicate that the single logical NIC is enabled. However if all of the NICs were disabled then the aggregate link status report would indicate that the single logical NIC is disabled. Providing status information to the operating system such as aggregate throughput and aggregate link status allows the operating system to report status information about the single logical NIC to users or network monitoring software.

In sum the disclosed architecture has the following advantages over prior art LBFO implementations. First LBFO functions are implemented in the NRM where NIC level operational parameters are monitored thereby minimizing the amount of communication that occurs between software levels when load balancing fail over and fail back operations are performed. Second the NRM has access to network traffic information and other NIC level operational parameters not necessarily available to the operating system which allows the NRM to make more informed load balancing fail over and fail back decisions. Third the NRM network function software may be specially tailored for complex LBFO management and operations thereby improving LBFO related performance relative to prior art implementations. Finally a single logical NIC is seen by the operating system and the user that effectively limits the ability of the operating system or the user to reconfigure the individual NICs and or the team of NICs. Further the resulting configuration is created and persistently maintained.

NIC authentication in 802.1X typically begins with the NIC requesting an 802.1X credential from the switch which forwards the 802.1X request to the credential server. If the 802.1X request is approved the credential server transmits an 802.1X credential to the switch which forwards the credential to the requesting NIC. Once the NIC has a valid credential the NIC is authorized to communicate with the switch until the credential expires or the network connection between the NIC and the switch is interrupted e.g. if the network cable were disconnected or the NIC were disabled . The networking protocol for a NIC to request and receive a credential from an 802.1X compatible switch is well known to those skilled in the art.

In one embodiment of the invention an 802.1X security credential request to authenticate a specific NIC originates from 802.1X software in the operating system and is communicated from the 802.1X software through that NIC s function driver to the NRM which forwards the credential request through the NIC to the switch . Unlike conventional TCP IP communications from the TCP IP stack to the VFD which transmits the TCP IP traffic to the NRM where the traffic is distributed between the different NICs and in the team the credential request is transmitted through the actual NIC being authenticated. For example if the NIC is being authenticated the 802.1X software transmits an 802.1X security credential request to the FD which subsequently transmits the credential request to the NIC through the handle . Once received by the NIC the credential request is forwarded from the NIC to the switch .

In one embodiment of the invention the computing device may have one or more VLAN assignments that are common to each of NICs and in the team. Each such VLAN assignment represents an IP address for the computing device within the IP address range defined for that VLAN. Further for each VLAN there is a separate binding between the single logical NIC and the TCP IP stack for exchanging TCP IP communications associated with the VLAN. The procedure for configuring a range of IP addresses as a VLAN and assigning machines to the VLAN is well known to those skilled in the art. Reconfiguring the computing device to support a first VLAN assignment requires the bus driver to add a VLAN attribute to the existing VFD and adjust the binding between the VFD and the TCP IP stack to couple the VLAN attribute and the TCP IP stack . Each additional VLAN assignment requires the bus driver to add another virtual function driver to the single logical NIC and to request a software binding from the NDIS to couple the additional virtual function driver to the TCP IP stack for exchanging TCP IP communications associated with the additional VLAN. The additional virtual function driver is also coupled to the NRM similarly to the way the VFD is coupled to the NRM to enable communications between the TCP IP stack and the team of NICs and . Further the binding between the 802.1X software program in the operating system and the additional virtual function driver that is initially created when the bus driver creates the additional virtual function driver is removed by the user level process just as the 802.1X binding is removed for the VFD . In this configuration the computing device may communicate with computing devices assigned to any of the VLANs associated with any of the virtual function drivers included in the single logical NIC and the computing device is prevented from communicating with computing devices not assigned to those VLANs thus supporting the 802.1Q network standard.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof. For example in various embodiments the team of NICs may include any number of NICs. In an embodiment where the computing device includes one NIC there would be one function driver for the NIC and a software binding would couple the function driver to the TCP IP stack for handling conventional network communications between the NIC and the operating system . Further if applicable another software binding would couple the function driver to the 802.1X software program for authenticating the NIC using 802.1X. To support a first VLAN assignment a first virtual function driver would be created and the software binding between the TCP IP stack and the function driver would be transferred coupling the first virtual function driver and the TCP IP stack to enable VLAN traffic to be communicated between the NIC and the TCP IP stack . For each additional VLAN assignment another virtual function driver would be created along with a software binding that would couple the additional virtual function driver to the TCP IP stack to enable network traffic associated with the additional VLAN to be communicated between the NIC and the TCP IP stack . In addition to the foregoing the software components and hierarchy shown in are exemplary and other functionally equivalent software components or other hierarchies may be implemented without departing from the scope of the invention. Further the computing device may be a desktop computer server laptop computer palm sized computer personal digital assistant tablet computer game console cellular telephone or any other type of similar device that processes information.

