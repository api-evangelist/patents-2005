---

title: Method and apparatus to interface an offload engine network interface with a host machine
abstract: A system that includes a host including at least one per-connection data structure and at least one per-processor data structure, wherein the at least one per-connection data structure is associated with a connection, and an offload engine operatively connected to the host. The engine includes offload engine connection registers and functionality to update the at least one per-connection data structures in the host, wherein the offload engine is configured to send and receive network data on the connection, wherein the host and the offload engine communicate using the at least one per-processor data structure, and wherein the offload engine communicates a status of the connection to the host using the offload engine connection registers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07647436&OS=07647436&RS=07647436
owner: Sun Microsystems, Inc.
number: 07647436
owner_city: Santa Clara
owner_country: US
publication_date: 20050429
---
Network traffic is transmitted over a network such as the Internet from a host e.g. a device capable of receiving data over a network to another host. Each host uses a specialized piece of hardware commonly referred to as a network interface NI to access the network. The NI is a piece of hardware found in a typical computer system that includes functionality to send and receive network traffic. Typically network traffic is transmitted in the form of packets where each packet includes a header and a payload. The header contains information regarding the source address destination address size transport protocol used to transmit the packet and various other identification information associated with the packet of data. The payload contains the actual data to be transmitted from the network to the receiving system. The contents and transmission of the aforementioned packets on the network are typically governed by Transmission Control Protocol TCP and Internet Protocol IP .

Processing TCP IP traffic requires significant host resources. To decrease the amount of processing required by the host TCP offload engines TOE have been developed. TOEs typically correspond to specialized hardware in chare of executing TCP IP. The use of a TOE enables the host to offload the execution of TCP IP thereby freeing CPU cycles on the host to improve host performance.

In general in one aspect the invention relates to a system comprising a host comprising at least one per connection data structure and at least one per processor data structure wherein the at least one per connection data structure is associated with a connection and an offload engine operatively connected to the host comprising offload engine connection registers and functionality to update the at least one per connection data structures in the host wherein the offload engine is configured to send and receive network data on the connection wherein the host and the offload engine communicate using the at least one per processor data structure and wherein the offload engine communicates a status of the connection to the host using the offload engine connection registers.

In general in one aspect the invention relates to a method for receiving data comprising allocating an anonymous receive buffer in a host wherein the anonymous receive buffer is associated with a descriptor sending the descriptor to an offload engine operatively connected to the host using a request queue wherein the request queue is located on the host and is accessible by the offload engine transferring ownership of the anonymous receive buffer to the offload engine receiving the data from a network interface card by the offload engine storing the data in the anonymous receive buffer placing an event on an event queue wherein the event queue is located on the host and the event comprises a reference to a connection upon which the data was received a reference to the anonymous receive buffer and an amount of data stored in the anonymous receive buffer issuing an interrupt to the host by the offload engine transferring ownership of the anonymous receive buffer from the offload engine to the host upon receiving the interrupt notifying a processor on the host associated with the connection that data has been received on the connection and consuming data in the anonymous receive buffer.

In general in one aspect the invention relates to a method comprising placing the data in a send buffer wherein the send buffer is located on a host posting a send request on the request queue wherein the send request comprises a descriptor of the send buffer wherein the request queue is located on the host notifying an offload engine of the send request by updating a last descriptor posted register in the offload engine wherein the offload engine is operatively connected to the host upon receiving the notification obtaining the descriptor from the host by the offload engine programming a network interface card to send the data directly from the send buffer to a receiver using the descriptor sending the data to the receiver by the network interface card receiving notification of receipt of the data from the receiver by the offload engine and updating a byte send count register in the offload engine.

Other aspects of the invention will be apparent from the following description and the appended claims.

Exemplary embodiments of the invention will be described with reference to the accompanying drawings. Like items in the drawings are shown with the same reference numbers.

In an embodiment of the invention numerous specific details are set forth in order to provide a more thorough understanding of the invention. However it will be apparent to one of ordinary skill in the art that the invention may be practiced without these specific details. In other instances well known features have not been described in detail to avoid obscuring the invention.

In general embodiments of the invention relate to a method and apparatus for interfacing a host with a TCP offload engine TOE . More specifically embodiments of the invention provide per processor and per connection data structures to facilitate communication between the host and the TOE as well as to decrease the overhead associated with using the TOE. Further embodiments of the invention use the per processor data structures to control the flow of data between the host and the TOE to notify the host of events e.g. receiving data from the network a new connection has been established etc. occurring on the TOE and to communicate error conditions from the TOE to the host. In one or more embodiments of the invention the host and the TOE use a direct memory access DMA mechanism to communicate and transfer data. Further embodiments of the invention provide software and hardware constructs to interface a host machine to a TOE to support legacy application programming interfaces APIs for sockets.

Continuing with the discussion of the host memory and the TOE memory maintain various data structures used by the host and the TOE i.e. the combination of the TOE CPU and the TOE memory to send and receive data from the network . More specifically in one embodiment of the invention the host memory includes per processor data structures and per connection data structures . As the names suggest there is one set of per processor data structures for each host CPU . Similarly there is one set of per connection data structures for each connection. In one embodiment of the invention the connection corresponds to a communicate channel across the network between the host and a receiver. The connection may be established using a socket e.g. Berkley Software Distribution BSD sockets . The socket enables the host and the receiver to engage in inter process communication IPC over the network. Those skilled in the art will appreciate that a given system implementing embodiments of the invention may include any number of host CPU s and corresponding host memory and support any number of connections. The specific per processor data structures and per connection data structures are described below in .

Continuing with the discussion of the TOE memory includes the following data structures an incoming packet buffer TOE registers and TOE connection registers . In general the incoming packet buffer is used to buffer data received via the NI prior to forwarding the data to the host memory . The TOE registers and the TOE connection registers are described below in . Those skilled in the art will appreciate that while only one host CPU is shown in the network system may include multiple host CPUs.

In one embodiment of the invention each of the send buffer s is associated with a descriptor not shown . Further when the send buffer s are allocated to a particular connection each of the descriptors not shown associated with the allocated send buffer s is linked to the connection or more specifically to the data structures used to represent the connection on the host . In one embodiment of the invention once data in particular send buffer s is no longer needed by the TOE e.g. the data has been sent and acknowledged the send buffer s is freed such that it may be used again.

In one embodiment of the invention an upper bound is fixed on the amount of send buffer s in use by one particular connection. When this upper bound is about to be exceeded for a connection the application executing on the host may register with the TOE for a notification when one or more send buffer s are not in use anymore on this connection and then sleep until such a notification is received. In one embodiment of the invention return of unused send buffers to the operating system can be delayed. This favors efficient reuse of the same buffers. The aforementioned process of freeing idle buffer s may be implemented when the amount of available send buffer s is below a minimum threshold level. Flow control using the aforementioned data structures is discussed below.

In one embodiment of the invention the receive buffer s correspond to fixed length anonymous buffer s . The receive buffer s are not initially associated with any particular connection. Once data is placed in the receive buffer s then the receive buffer s are associated with a particular connection i.e. the connection which received the data . However the receive buffer s is only temporarily associated with the connection once the data in the receive buffer s has been consumed by the application executing on the host the receive buffer s are freed i.e. the receive buffer s are no longer associated with the connection . At this stage the receive buffer s may be reused by any connection. In one embodiment of the invention the send buffer s and the receive buffer s are implemented using a ring buffering mechanism.

In one embodiment of the invention the mirror of TOE connection registers correspond to memory locations in the host memory which are used to store data corresponding to the following registers a state register a bytes sent count register and an incoming connection count register. The content of the aforementioned registers is updated by the TOE from the corresponding TOE connection registers which are located in TOE memory . More specifically the TOE connection registers in the TOE memory include a state register a bytes sent count register and an incoming connection count register.

In one embodiment of the invention the state register stores the current state e.g. waiting to receive data no connection etc. of the connection. In one embodiment of the invention the host uses contents of the state register which is mirrored in the host memory to determine whether to proceed with an operation e.g. sending data to a receiver etc. or to abort the operation and return an error code to the host or a process executing on the host an application executing on the host which requested the operation .

In one embodiment of the invention the bytes sent count register stores the number of bytes that have been sent on the particular connection. In one embodiment of the invention the bytes sent count register is used by the host to verify that data stored in the send buffer s has been sent. In one embodiment of the invention the incoming connection count register stores the number of incoming connections. In one embodiment of the invention the incoming connection count register is used by the host to determine whether a new connection is ready. More specifically the host can compare the current value of the incoming connection count register by checking the corresponding incoming connection count register in the mirror of TOE connection registers with the previous value of the incoming connection count register.

In one embodiment of the invention the contents of the TOE connection registers is mirrored into host memory by the TOE using a direct memory access mechanism DMA . Generally DMA mechanisms enable data to be transferred from one memory location or region to another memory location or region without using a central processing unit e.g. the host CPU the TOE CPU etc. . In one embodiment of the invention the contents of the bytes sent count register in the mirror of TOE connection registers in the host memory is not necessarily updated every time the corresponding TOE register value changes. Those skilled in the art will appreciate that the rate at which the bytes sent count register in the mirror of TOE connection registers in the host memory is updated may depend on the implementation. In one embodiment of the invention the remaining registers in the mirror of TOE connection registers are updated when there is a state change. As an alternative to the above updating scheme all registers in the mirror of TOE connection registers in the host memory may be updated in response to the occurrence of particular events in the system e.g. receiving data sending data establishing a connection etc. .

As discussed above the host memory includes per processor data structures . In one embodiment of the invention the per processor data structures include a request queue and an event queue . In one embodiment of the invention the request queue and the event queue are located in the host memory but are accessible to the TOE. In one embodiment of the invention the request queue and the event queue are implemented as ring buffer s . In one embodiment of the invention the host uses the request queue to communicate various types of requests to the TOE . For example the host may use the request queue to notify the TOE that the host has data to send to the network. Further the host may use the request queue to register notifications with the TOE. For example the host may post a notification request on the request queue which requests the TOE to notify usually using an event discussed below the host when a particular event occurs e.g. data is received .

Additionally the host may use the request queue to control the TOE. For example if the host or an application executing on the host is not consuming incoming data i.e. data received from the network at a fast enough rate then the host may post a request on the request queue for the TOE to reply on flow control to prevent the sender from sending any more date. At some later point the host may post a subsequent request on the request queue for the TOE to start receiving data. Those skilled in the art will appreciate that when the host requests the TOE to stop receiving incoming data the TOE may not stop receiving data immediately rather the TOE may continue to buffer s incoming data for a certain period of time e.g. the length of the TCP advertise window prior to ceasing to receive data. In one embodiment of the invention the TOE may buffer the incoming data in the incoming packet buffer .

In one embodiment of the invention the TOE uses the event queue to communicate events to the host in an asynchronous manner. In one embodiment of the invention two classes of events may exist 1 events that are triggered following a request discussed above from the host and 2 events that are spontaneously generated by the TOE i.e. events which are not triggered in response to a request from the host . For example in one embodiment of the invention every time a receive buffer s is filled by the TOE the TOE posts an event on the event queue.

In one embodiment of the invention the number of entries in the request queue and the event queue is set at a specific number though not necessarily the same number . Because the host and the TOE include mechanisms which implement flow control on the request queue and the event queue there is a limit on the number of requests that may be posted to the request queue and a limit on the number of events that may be posted to the event queue . In one embodiment of the invention the number of entries in the request queue and the event queue is set at a specific number though not necessarily the same number .

The following describes embodiments of flow control that may be implemented on the request queue . In one embodiment of the invention each time the host posts a new request on the request queue the host notifies the TOE by writing typically using a programmed I O PIO write to a last descriptor posted register in the TOE memory . Similarly after a request is consumed i.e. serviced by the TOE the TOE updates a last descriptor consumed register in the TOE memory . As the host posts requests on the request queue the host keeps track of the number of requests posted to the request queue . Prior to placing requests on the request queue the host determines whether any available entries exist in request queue . The host may determine whether any available entries exist in the request queue by reading typically using a PIO read the last descriptor consumed register . Because the host is aware of the last request that was posted the last request that was serviced and the number of requests posted the host may determine whether an available entry exists in the request queue . Note that the host does not need to read the last descriptor consumed register for every request it posts. Rather the host can cache the value of the last descriptor consumed register and effectively read it only when the cached value leads it to find no free entry in the request queue .

As an alternative in one embodiment of the invention the host may determine whether any available entries exist on the request queue by comparing the number of incoming events with the number of requests posted to the request queue because each event on the event queue is associated with a request on the request queue . Regardless of which flow control embodiment is used if the host determines that the request queue may overflow then the host may temporarily stop posting requests to the request queue .

The following describes an embodiment of flow control that may be implemented on the event queue . As discussed above two classes of events may exist 1 events that are triggered following a request discussed above from the host and 2 events that are spontaneously generated by the TOE. In one embodiment of the invention the number of events generated by the TOE is bounded. For example the number of events that are not triggered in response to a request from the host is bounded by the number of receive buffer s associated with the particular host CPU e.g. host CPU . Because the host can control the number of requests posted on the event queue which trigger events and because the host is aware of the maximum number of bounded events i.e. events that are generated by the TOE the host can determine when at worst an overflow of the event queue may occur. Based on when at worst an overflow of the event queue may occur the host may stop posting requests on the request queue that would trigger an event. At some later time the host may begin to post requests that would trigger an event.

As an alternative the host may notify the TOE that an event has been de queued i.e. removed from the event queue by the host from the event queue . The host may notify the TOE every time an event is de queued by writing to register in the TOE memory . The TOE may use the aforementioned register to determine when at worst the event queue may overflow. If the TOE determines that the event queue is likely to overflow the TOE may temporarily stop receiving data from the network aggregate multiple events into a single event or take other actions to prevent the event queue from overflowing.

The TOE subsequently obtains the descriptor and in some cases additional information associated with the send request from the request queue ST . In one embodiment of the invention the descriptor and in some cases additional information associated with the send request is obtained from the request queue using a DMA read. Once the TOE has obtained the descriptor and in some cases additional information associated with the send request the TOE proceeds to program a NI operatively connected to the TOE ST . In one embodiment of the invention the NI is programmed by the TOE using the descriptor and in some cases additional information associated with the send request . The TOE programs the NI such that data is obtained directly from the send buffer s typically using a DMA read . Once the NI has been programmed the NI proceeds to obtain and send the data to the receiver ST .

At this stage the TOE waits for notification e.g. TCP acknowledgements from the receiver that the data has been received. Once the notification of receipt has been received the TOE may proceed to notify the host ST . Finally the TOE updates the bytes sent count register in the TOE memory ST . In one embodiment of the invention each send request includes a special flag which requests the TOE to update the mirror of the TOE connection registers with the values of the corresponding registers located in the TOE. In one embodiment of the invention the mirror of the TOE connection registers is updated by the TOE using a DMA write.

At some point after ST has been performed the TOE receives data from the network via the NI ST . In one embodiment of the invention the data received from the network is initially stored in the incoming packets buffer within the TOE memory.

Once the TOE has received the data from the network the TOE proceeds to place the data in one or more anonymous receive buffer s ST . In one embodiment of the invention the TOE places the data in the anonymous receive buffer s using a DMA write operation. After the data has been placed in the anonymous receive buffer s the TOE proceeds to post an event on the event queue ST . In one embodiment of the invention the event includes a reference to the connection on which the data was received a reference to the anonymous receive buffer s typically a descriptor associated with the anonymous receive buffer s and a field indicating the amount of data stored in the corresponding anonymous receive buffer s . In one embodiment of the invention the event is posted on the event queue using a DMA write operation.

In one embodiment of the invention once the TOE posts the event on the event queue the TOE issues an interrupt to the appropriate CPU i.e. the CPU with which the connection is associated ST . The TOE then transfers control of the anonymous receive buffer s back to the host. The interrupt subsequently notifies the appropriate host CPU ST . The data in the anonymous receive buffer s is subsequently consumed ST . In one embodiment of the invention an application consumes the data.

An embodiment of the invention may be implemented on virtually any type of computer regardless of the platform being used. For example as shown in a networked computer system includes a processor associated memory a storage device and numerous other elements and functionalities typical of today s computers not shown . The networked computer may also include input means such as a keyboard and a mouse and output means such as a monitor . The networked computer system is connected to a local area network LAN or a wide area network via a network interface connection not shown . Those skilled in the art will appreciate that these input and output means may take other forms. Further those skilled in the art will appreciate that one or more elements of the aforementioned computer may be located at a remote location and connected to the other elements over a network. Further software instructions to perform embodiments of the invention may be stored on a computer readable medium such as a compact disc CD a diskette a tape a file or any other computer readable storage device.

While the invention has been described with respect to a limited number of embodiments those skilled in the art having benefit of this disclosure will appreciate that other embodiments can be devised which do not depart from the scope of the invention as disclosed herein. Accordingly the scope of the invention should be limited only by the attached claims.

