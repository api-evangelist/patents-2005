---

title: Common concurrency runtime
abstract: The common concurrency runtime (CCR) provides a simple and self-consistent set of concurrency primitives that developers can use to more readily split their computation into more discrete chunks that can scale better with additional processors. This set of primitives provides for very scalable applications that are well suited for the coming world of ubiquitous communication and very large scale out for the number of local processors. The CCR may be implemented as a single library in C# that implements channels with input and asynchronous output capabilities, along with an atomic test-and-input primitive. On top of this, richer derived operators (e.g., choice, join, replication, reader-writers, scatter-gather, etc.) may be encoded. Thus, existing C# may be built upon to provide the capability to concurrently issue I/O requests to remote systems while simultaneously performing other functions locally to increase the efficiency of the distributed system. Because it is based on C#, the CCR allows users to take advantage of a well-known programming model to address unique issues in a new programming environment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07774750&OS=07774750&RS=07774750
owner: Microsoft Corporation
number: 07774750
owner_city: Redmond
owner_country: US
publication_date: 20050719
---
A known problem in computer science involves effective programmability around concurrency. Existing programming paradigms involve such things as threads mutexes semaphores critical sections and other primitives. Though these have been around for decades they are hard to use properly without inducing hard to find timing bugs or inadvertently serializing access through code sections via improper locking hierarchies.

Though this has been an issue in the past developers typically did not worry about it because they could count on their purely sequential code automatically running faster due to ever increasing CPU clock rates. This is changing however now that clock rates have hit the thermal barrier and CPU vendors are moving toward multi core designs. This means that for a software developer to make their code run faster they will need to split the work into multiple concurrent sections that can run across 2 4 32 64 or even hundreds of local processors in a single machine. This is difficult to do with threads.

Additionally there are latencies in distributed operating systems. For example if a user issues a read request from their local disk using traditional local architectures they are assured to get data back within a few milliseconds. But if the user does the same thing over a network to a remote machine it may take seconds or minutes or forever to return the data. A new language called C Omega has been developed and provides an alternative to threading for dealing with local concurrent programming. C Omega however requires an entirely new language infrastructure and toolchain debuggers etc. . Also C omega takes a declarative approach where any programming primitives are fixed at compile time and cannot be modified or adjusted based on runtime information. It would be desirable therefore if a mechanism were available to address the problem of concurrency and coordination in a concurrent and potentially distributed operating system without the need for a new language.

The common concurrency runtime CCR provides a simple and self consistent set of concurrency primitives that developers can use to more readily split their computation into more discrete chunks and coordinate results in a concise manner so that the applications can scale better with additional processors and the complexity of software can be kept small relative to programming with threads. This set of primitives provides for very scalable applications that are well suited for the coming world of ubiquitous communication and very large scale out for the number of local processors.

The CCR may be implemented as a single coordination concurrency library in C that implements channels with input and asynchronous output capabilities along with an atomic test and input primitive. On top of this richer derived operators e.g. choice join replication reader writers scatter gather etc. may be encoded. Thus the invention may build upon the existing C to provide the capability to concurrently issue I O requests to concurrent processes while simultaneously performing other functions locally to increase the efficiency of the system. Because it is based on C it allows users to take advantage of a well known programming model to address unique issues in a new programming environment.

The invention provides an implementation of a type safe queue and collection of queues that may be referred to as channels or ports. These ports have First In First Out semantics and are similar to the channels described in many process algebras. They have input and asynchronous output capabilities and atomic test and input primitives. The port architecture allows a lock free flexible implementation of message coordination primitives like choice join replication and other higher level operators. Higher levels of abstraction can be built using a small number of primitives. Any number of other concurrent languages including C Omega Join Java etc. could compile down to the ports and the basic coordination primitives. The scheduling model in particular allows an efficient and simple way to interface with legacy systems where threading and thread affinity has to be taken into consideration.

All interaction is through the core interaction primitive i.e. a port. Receivers may be attached i.e. waiting on a port and messages may be posted. When a message arrives code executes with the message as a parameter. In a preferred embodiment everything is asynchronous. The CCR may include a hierarchical arbitration procedure and arbiters may be nested. Coordination primitives can be nested and a coordination primitive may itself be executed as another work unit. Thus the invention provides an extensible messaging system with defined coordination primitives. A scheduler or dispatcher may be provided to take the union of messages with receivers and activate them. The dispatcher may be decoupled from the port and arbiter implementations.

According to an aspect of the invention everything may be treated as a task. Because one can determine how long it takes to run a task one can throttle down the CPU time used according to a desired formula. Such throttling may be based on simple feedback control to effectively control runtime CPU usage.

Thus the invention provides an extensible coordination and concurrency library that can be embedded in existing languages and be targeted by new languages.

Although not required the invention can be implemented via an application programming interface API for use by a developer or tester and or included within the network browsing software which will be described in the general context of computer executable instructions such as program modules being executed by one or more computers e.g. client workstations servers or other devices . Generally program modules include routines programs objects components data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the program modules may be combined or distributed as desired in various embodiments. Moreover those skilled in the art will appreciate that the invention may be practiced with other computer system configurations. Other well known computing systems environments and or configurations that may be suitable for use with the invention include but are not limited to personal computers PCs automated teller machines server computers hand held or laptop devices multi processor systems microprocessor based systems programmable consumer electronics network PCs minicomputers mainframe computers and the like. An embodiment of the invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network or other data transmission medium. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

With reference to an example system for implementing the invention includes a general purpose computing device in the form of a computer . Components of computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus .

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to random access memory RAM read only memory ROM Electrically Erasable Programmable Read Only Memory EEPROM flash memory or other memory technology compact disc read only memory CDROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic radio frequency RF infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as ROM and RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data . RAM may contain other data and or program modules.

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the example operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies. A user may enter commands and information into the computer through input devices such as a keyboard and pointing device commonly referred to as a mouse trackball or touch pad. Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB .

A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

One of ordinary skill in the art can appreciate that a computer or other client devices can be deployed as part of a computer network. In this regard the present invention pertains to any computer system having any number of memory or storage units and any number of applications and processes occurring across any number of storage units or volumes. An embodiment of the present invention may apply to an environment with server computers and client computers deployed in a network environment having remote or local storage. The present invention may also apply to a standalone computing device having programming language functionality interpretation and execution capabilities.

Common Concurrency Runtime CCR may be implemented as a C library for concurrency. At its core are pi like channels with input and asynchronous output primitives along with an atomic test and input primitive. On top of this richer derived operators e.g. choice join replication readers writers may be implemented.

An application is created by running it from the command line. It creates dispatchers which in turn create their scheduler threads. Each scheduler thread proceeds picking up a task and executing it. This execution may create new ports or add messages to ports thus triggering the execution of arbiters or create arbiters or tasks in the current dispatcher.

Messages are asynchronous but order respecting. The CCR is asynchronous in the sense that when a task posts a message it continues immediately rather than waiting for the message to be received. Patterns that may be used include state as message and scatter gather. In state as message the state of a process is represented as an asynchronous message to an internal port. In scatter gather a dynamically determined number of work units are created to do some tasks in parallel. Then a join select command waits until they have all finished or one has generated an error. The dynamic variable nature of the choice and join primitive is a feature of this invention not found in C omega join java or other similar efforts.

A work unit is a linear sequence of commands. The commands are to 1 create a new port 2 create a new task delegate and append it to the dispatcher s list of tasks 3 post a message to a port 4 create a new arbiter and register it on the ports it wants to watch.

Rather than registering it immediately on its ports a new task may be created which will do the registering later e.g. when it comes to be executed. An activate may be used for creating this task and appending it to the dispatcher s list. To append a task to the dispatcher s task list the dispatcher may manage its own port and an append task message may be posted to the dispatcher. It should be understood that a language may be developed in which a work unit consisted solely of message postings to create a new port it posts a message to the new port creating service that is expected to be present and so on.

The New command allocates a fresh port. The port is indexed by the types it can carry. Within C programming of the CCR it is a compile time error to post an invalid type or to activate a handler which waits for an invalid type. An example New command is 

The Post command posts an asynchronous message. Messages are ordered in CCR. It is a compile time error to post a message that has the wrong type for the port. An example Post command is 

Arbiter Simple Input. Following are two syntaxes for input. The first uses a delegate so as to write the continuation inline. The second uses a static method or can use an instance method.

The activate command creates a task and p.with creates an arbiter. When the task is scheduled it registers the arbiter on port p. When a message eventually arrives it executes the arbiter thus unregistering the arbiter . The arbiter is a simple one that merely creates a task for subsequent scheduling. When this next task is scheduled it attempts an atomic test and remove operation on the port. If successful it executes the delegate first case or static method second case . If unsuccessful the arbiter is re registered to watch for subsequent messages.

A two phase form of message reception may be employed. In the first phase the arbiter watches enough messages arrive until it thinks its continuation may be able to run. In the second phase an attempt is made to get the necessary message atomically before running the continuation. This two phase technique becomes more useful with more complicated arbiters particularly for join and choice.

The following synchronous version of choice waits until the handler has finished executing before it continues 

Arbiter Join. In CCR join is a multi way version of the pi calculus input command. An example join is 

Note that there are many arbiters looking on the same channels. So when pand phave arrived they may schedule continuations in many different dispatchers. Every one of these continuations will attempt to test and remove in alphabetical order. Exactly one of the continuations will succeed and the others will restart their waiting. Therefore this encoding of join is divergent locally but not globally. Compare this to the encoding of join into pi which is divergent . The different in expressivity is due to CCR having test and remove and alphabetical orderings on ports. Pi calculus for example cannot express these.

The CCR s two phase implementation of joins first waking up arbiters then doing test and remove is more practical than the finite state automata technique used in Join Calculus. That is because it allows extra joins to be added at runtime without recompiling the machine and the size of the join automata increase non linearly with number of inputs.

This dynamic join can be implemented in a conventional process calculus that has no dynamic joins. For example one could write a multi way join service that does a 1 way join if the list had one element a 2 way join if it had two elements and so on up to some bound. Such a multi way join service should be adequate for modeling dynamic joins in many situations.

As the number of messages that are to be sent is unknown at compile time joinvariable single port many messages and joinarray many ports one message each provide a concise way to gather any number of messages known only at runtime.

The way CCR makes use of parallelism in the hardware is that it uses one or more scheduler threads per CPU for executing work units. More concurrency can be provided with an advanced primitive known as an interleave. An example interleave may be 

At runtime all APIs create Tasks which can be scheduled by a dispatcher. A resource manager allocates dispatchers which can all be ion the same resource group. Multiple front end queues may share TaskExecutionWorker threads which load balance tasks. Queues are just ports. The ability to associate a dispatcher per task arbiter leads to easy interoperability for STA threaded legacy code and provides flexibility to the programmer.

 Iterators allow people to write sequential non blocking code which may greatly increase readability. The iterator pattern from C 2.0 may be used according to the invention in a novel way. Loops for example now become easy again even if they involve asynchronous messaging. A point of novelty is that since the CCR runtime operates on tasks user code may be turned into iterators over tasks. That is they can yield to a choice arbiter join etc. because it runs a task to be scheduled. That will eventually make progress and allow the scheduler to iterate over to the next logical block in the user s code.

An example programming model for concurrency and distribution is depicted in . The base layer contains the existing programming constructs from the CLR or from any other acceptable base . This layer supports the ordinary sequential programming model used in systems today. The concurrency layer adds new local constructs for concurrency replacing the existing Win32 CLR constructs with new more modern constructs that are easier to understand easier to use easier to check and altogether easier to get right. The concurrency layer may be separated from the base layer to keep from making ordinary sequential programming as hard as concurrent programming. Where the CCR is implemented in a distributed environment a distributed layer may provide new constructs for dealing with programming problems unique to distributed systems. It may incorporate a failure model a security model etc. The distributed layer may be separated from the concurrency layer to keep from making local programming as hard as distributed programming.

The CCR is a concurrency layer atop the CLR base layer. The Decentralized System Services DSS architecture may build upon the CCR to support a distributed layer. It should be understood that the CCR does not itself address problems unique to distributed systems e.g. marshalling versioning failure security etc. In particular a program using the CCR may run on a single machine. It cannot fail in part. That is if any part of a program fails the entire program fails. Also it runs in a single security domain.

A CCR program may execute in a number of asynchronous actions. Actions are similar to threads or tasks or work items in other systems. In its current implementation the CCR sometimes uses Win32 CLR threads internally and CCR programs must not bypass the CCR to use Win32 CLR threads or perform synchronization either directly or indirectly through a library. The programmer must be careful to avoid such cases which have no meaning.

A CCR program may have an initial action and any action can start new actions. Individual actions can terminate and the entire program terminates once all of its actions have terminated. One way to start a new action is by calling spawn specifying a method for the new action to call and a list of arguments. For example a first action might call spawn as follows 

to start a new action that calls LogMethod with the string argument Program initializing and then terminates when the method returns. LogMethod may for example print the message to the console 

so this call to spawn will log the message asynchronously while other actions proceed with program initialization. An alternative is for the programmer to use an anonymous delegate to specify the method inline 

While spawn is one way to start new actions another way to start new threads is by sending messages to ports. As described below the program can register certain types of actions to start when certain messages arrive at certain ports.

The CCR provides a message passing programming model. Programs can send asynchronous messages to ports and these messages can start new asynchronous actions. The simplest kind of port receives messages of some specified type. For example 

constructs a new port log for messages of type string. A program calls with and activate to bind a method to a port and to activate the binding 

Once the method is bound posting a message to log using log.post will start a new action that calls the method with the message as its argument.

In this example posting multiple messages to log will start multiple actions that can run concurrently. In this program 

The CCR features described above can be used to start any number of new actions. Each action may be independent of all others. A join is a new kind of binding that enables actions to synchronize and communicate. It specifies a method for a new action when messages arrive at more than one port.

Consider the example of a cashbox that holds n dollars. We can deposit more dollars when we want and we can request i.e. ask to know the current balance. First we define new ports deposit balanceRequest and balance. 

Portdeposit new Port The deposit port takes an int from the client and its action increments the balance by the specified amount.

PortbalanceRequest new Port The balanceRequest port takes a continuation port which is called a continuation port by analogy to continuations in programming languages that takes an int and its action returns the current balance to that port.

Portbalance new Port The balance port takes an int as an argument i.e. as a message representing the current balance. The client cannot post directly to balance it is internal to the implementation.

These two messages to deposit start two new actions each of which increments the balance by the specified amount. These two new actions cannot execute concurrently with each other because starting each action requires a message on balance as well as a message on deposit. As soon as one action starts the next message on deposit must wait for another message on balance. The same message cannot start more than one action. This eliminates a race condition typical in other programming models. Similarly there is at most one int at balance at any moment since the old argument is consumed before the new one is posted and since the client cannot post directly to balance. 

Consider a second example of an extended cashbox that allows withdrawals as well as deposits but does not allow for withdrawing more money than is available. An attempted withdrawal with insufficient funds will wait until more money is deposited. New ports deposit withdraw balance withdrawalWaiting and noWithdrawalWaiting may be defined.

The new internal withdrawalWaiting port has a Withdrawal waiting when a withdrawal is in progress. At any time at most one Withdrawal can be waiting on withdrawalWaiting. The client cannot post directly to withdrawalWaiting it is internal to the implementation.

If the deposit is for less than the waiting withdrawal amount then the full amount is deposited. Otherwise the withdrawal can be completed posting a message to noWithdrawalWaiting. The current balance can be recomputed and a MsgSuccess posted to the withdrawal s continuation.

This activate defines the operation of withdraw when no withdrawal is waiting i.e. when an int is at balance a Withdrawal is at withdraw and a MsgSuccess is at noWithdrawalWaiting. The action sends these three messages as arguments to the delegate. If the withdrawal amount does not exceed the current balance then it may be subtracted from the remaining balance and the client notified. There is still no withdrawal waiting. Otherwise the Withdrawal is sent to withdrawalWaiting. If a Withdrawal message arrives at withdraw when another withdrawal is already waiting i.e. when withdrawalWaiting has a message so that noWithdrawalWaiting does not there is no defined action and the Withdrawal message waits at withdraw until its turn.

Here a new port afterWithdrawal is created which takes a MsgSuccess. It may be bound to a delegate that logs the withdrawal and then does some further work. A new Withdrawal for 200 dollars may be sent to it that includes afterWithdrawal. This starts the withdrawal. The delegate runs after the withdrawal is complete which in this case requires 50 more dollars to be deposited.

As described above ports can take messages of a particular type. The CCR however also lets ports take messages of multiple types with separate bindings for the separate message types. Consider the first cashbox example described above. Instead of three ports balance deposit and balanceRequest one port cashbox. may be used that takes messages of type Balance Deposit or BalanceRequest. 

When methods are bound to cashbox the different types of the messages determine the different uses of the port.

Another example program which simulates a disk scheduler will now be described. A number of concurrent actions generate streams of disk requests. The requests in each stream must be processed in order while requests in different streams may be interleaved.

The port headAt may be created to record the current head position initially 0 . The client cannot post directly to headAt. It is internal to the implementation.

 Distance computes the distance from the current head position to head position n in terms of an elevator algorithm. An elevator policy performs its requested operations in order of non decreasing block address for efficiency then repeats.

A list of pending requests may be maintained sorted by the results of Sooner. InsertionSort inserts a request into the sorted list.

The pending port may be created to hold one or more pending requests the nonePending port if there are none. The client cannot post directly to pending and nonePending. They are part of the implementation.

We create a service port and send it a MsgSuccess when requests are pending but no request is in progress. Its action removes the first pending request from the list seeks the head performs the operation and sends another message to service to handle the next request. The client cannot post directly to the service port. It is part of the implementation.

Calling DoBlocks simulates a sequential computation repeatedly reading random blocks. Its argument is a BlocksRequest. 

The disk head is initially at position 0. There are no pending requests yet but requests may now be serviced.

Thus there have been described systems and methods for effective programmability around concurrency that address the problem of latency in a distributed operating system. Though the invention has been described in connection with certain preferred embodiments depicted in the various figures it should be understood that other similar embodiments may be used and that modifications or additions may be made to the described embodiments for practicing the invention without deviating therefrom. The invention therefore should not be limited to any single embodiment but rather should be construed in breadth and scope in accordance with the following claims.

