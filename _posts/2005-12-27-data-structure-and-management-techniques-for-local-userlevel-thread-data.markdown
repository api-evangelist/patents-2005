---

title: Data structure and management techniques for local user-level thread data
abstract: Data structure creation, organization and management techniques for data local to user-level threads are provided. In one embodiment, a method includes generating, for a user-level thread (“shred”) to run on a thread unit that is not managed by an operating system (“OS”), a storage area for local data and maintaining state in the storage area across a context switch from the thread unit that is not managed by the OS to a second thread unit that is managed by the OS. Other embodiments are also described and claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08079035&OS=08079035&RS=08079035
owner: Intel Corporation
number: 08079035
owner_city: Santa Clara
owner_country: US
publication_date: 20051227
---
The present disclosure relates generally to information processing systems and more specifically to managing data local to user level threads in a multi sequencer multi threading system.

In order to increase performance of information processing systems such as those that include microprocessors both hardware and software multi threading techniques have been employed. Increasingly multi threading is supported in hardware. For instance in one approach processors in a multi processor system such as a chip multiprocessor CMP system may each act on one of the multiple software threads concurrently. In another approach referred to as simultaneous multi threading SMT a single physical processor is made to appear as multiple logical processors to operating systems and user programs. For SMT multiple software threads can be active and execute simultaneously on a single processor without switching. That is each logical processor maintains a complete set of the architecture state but many other resources of the physical processor such as caches execution units branch predictors control logic and buses are shared. For SMT the instructions from multiple software threads thus execute concurrently on each logical processor.

For a system that supports concurrent execution of software threads such as SMT and or CMP systems an operating system application may control scheduling and execution of the software threads on thread execution resource s . However other thread execution resources that are not controlled by the operating system may also be available to a programmer and may be controlled by user level code. Common operating systems do not provide for data structures to maintain local data for user level threads that may execute on such resources.

The following discussion describes selected embodiments of methods systems data structures apparatuses and mechanisms to manage local data for user level threads that are neither created nor managed by the operating system but instead are created and managed by user level code.

Such user level threads sometimes referred to herein as shreds are sequences of instructions that a programmer may cause to be executed based on instructions in user level code. Multiple shreds associated with the same thread may execute concurrently. For at least one embodiment the shreds are scheduled to run on available hardware resources such as e.g. by a scheduler in a software library or otherwise residing in user space without intervention of an operating system. The embodiments described herein may be utilized with single core or multi core multi threading systems.

As used herein a thread unit also interchangeably referred to herein as a hardware thread context or sequencer may be any physical or logical unit capable of executing a thread or shred.

In the following description numerous specific details such as processor types multi threading environments system configurations data structures and specific operating system processing have been set forth to provide a more thorough understanding of embodiments of the present invention. It will be appreciated however by one skilled in the art that the invention may be practiced without such specific details. Additionally some well known structures circuits and the like have not been shown in detail to avoid unnecessarily obscuring the present invention.

In the single core multi threading environment a single physical processor is made to appear as multiple logical processors not shown referred to herein as LPthrough LP to operating systems and user programs. Each logical processor LPthrough LPmaintains a complete set of the architecture state AS AS respectively. The architecture state may include for at least one embodiment data registers segment registers control registers debug registers and most of the model specific registers.

The logical processors LP LPshare most other resources of the physical processor such as caches execution units branch predictors control logic and buses. Although such features may be shared each thread context in the multi threading environment can independently generate the next instruction address and perform for instance a fetch from an instruction cache an execution instruction cache or trace cache .

Thus the processor includes logically independent next instruction pointer and fetch logic for each thread context even though the multiple logical sequencers may be implemented in a single physical fetch decode unit . The next instruction pointer and fetch logic is to determine the next instruction to be executed for the given thread or shred.

For a single core embodiment a sequencer thus may be a logical thread unit. In such case the term sequencer encompasses at least the next instruction pointer and fetch logic for a thread context along with at least some of the associated architecture state AS for that thread context. It should be noted that the sequencers of a single core multi threading system need not be symmetric. For example two logical sequencers for the same physical core may differ in the amount of architectural state information that they each maintain.

A single core multithreading system can implement any of various multithreading schemes including simultaneous multithreading SMT switch on event multithreading SoeMT and or time multiplexing multithreading TMUX . When instructions from more than one hardware thread contexts or logical processor run in the processor concurrently at any particular point in time it is referred to as SMT. Otherwise a single core multithreading system may implement SoeMT where the processor pipeline is multiplexed between multiple hardware thread contexts but at any given time only instructions from one hardware thread context may execute in the pipeline. For SoeMT if the thread switch event is time based then it is TMUX.

Thus for at least one embodiment the multi sequencer system is a single core processor that supports concurrent multi threading. For such embodiment each sequencer is a logical processor having its own next instruction pointer and fetch logic and its own architectural state information although the same execution resources of the single processor core may be shared among concurrently executing threads so that the same core executes all instructions for the concurrent threads.

However for at least one embodiment each processor supports multiple thread contexts that is each processor may be a multi threaded single core processor such as that shown in embodiment . For such embodiment the fetch decode unit of each core in the system implements distinct next instruction pointer and fetch logic for each supported thread context and each thread context maintains a separate copy of the architecture state AS . The optional nature of additional next instruction pointer and fetch logic and additional copies of the architecture state in a multiprocessor environment are denoted by dotted lines in .

For at least one embodiment of the multi core system illustrated in each of the sequencers may be a processor core with the multiple cores residing in a single chip package . As is described immediately above each core may be either a single threaded or multi threaded processor core. The chip package is denoted with a broken line in to indicate that the illustrated single chip embodiment of a multi core system is illustrative only. For other embodiments processor cores of a multi core system may reside on separate chips. That is the multi core system may be a multi socket symmetric multiprocessing system.

For ease of discussion the following discussion focuses on embodiments of the multi core system . However this focus should not be taken to be limiting in that the mechanisms described below may be performed in either a multi core or single core multi sequencer environment.

The operating system OS is commonly responsible for managing the user defined tasks for a process. While each process has at least one task see e.g. process and process bearing reference numerals and respectively others may have more than one e.g. Process bearing reference numeral . The number of processes illustrated in as well as the number of user defined tasks for each process should not be taken to be limiting. Such illustration is for explanatory purposes only.

The OS is commonly responsible for scheduling these threads . . . for execution on the execution resources. The threads associated with the same process have the same virtual memory address space.

Because the OS is responsible for creating mapping and scheduling threads the threads . . . are visible to the OS . In addition embodiments of the present invention comprehend additional user level threads that are not visible to the OS . That is the OS does not create manage or otherwise control these additional user level threads .

These additional threads which are neither created nor controlled by the OS and may be scheduled to execute concurrently with each other are sometimes referred to herein as shreds in order to distinguish them from OS visible threads and to further distinguish them from other user level threads that may not be executed concurrently with each other for the same OS visible thread. That is multiple shreds that are associated with the same OS visible thread may execute concurrently with each other.

The shreds are created and managed by user level programs referred to as shredded programs and may be scheduled to run on sequencers that are sequestered from the operating system. For example the OS managed thread illustrated in may execute on one sequencer not shown that is visible to the OS while each of the active shreds may execute on other OS sequestered sequencers see e.g. seq seq respectively of . For sequencers that are sequestered from the OS streams of instructions are scheduled for execution by a user level scheduler. An OS sequestered sequencer is thus managed by user level applications rather than the OS and is therefore referred to herein as an application managed sequencer or AMS .

The representation of two threads and four shreds for Process and of one thread and two shreds for Process is illustrative only and should not be taken to be limiting. Embodiments of the present invention do not necessarily impose an upper or lower bound on the number of threads or shreds associated with a process. Regarding a lower bound for threads illustrates that every process running at a given time is associated with at least one thread.

However the threads need not necessarily be associated with any shreds at all. Thus no lower bound is imposed for shreds. For example Process illustrated in is shown to run with one thread but without any shreds at the particular time illustrated in .

Regarding an upper bound the number of OS visible threads associated with a process may be limited by the OS program. However the upper bound for the cumulative number of shreds associated with a process is limited for at least one embodiment only by the number of shred execution resources e.g. number of sequencers available at a particular time during execution.

The common logical view of memory that is associated with all threads for a program or process may be referred to herein as an application image. For embodiments of the present invention this application program image is also shared by shreds associated with a process.

Accordingly illustrates that a system for at least one embodiment of the present invention may support a 1 to many relationship between an OS visible thread such as thread and the shreds which are not visible to the OS associated with the thread. The shreds are not visible to the OS see in the sense that a programmer not the OS may employ user level techniques to create synchronize and otherwise manage and control operation of the shreds. While the OS is aware of and manages one or more threads . . . the OS is not aware of and does not manage or control shreds. As used herein the terms thread and shred include at least the concept of a set of instructions to be executed concurrently with other threads and or shreds of a process. As used herein a distinguishing factor between a thread which is OS controlled and a shred which is not visible to the operating system and is instead user controlled which are both instruction streams lies in the difference of how scheduling and execution of the respective thread and shred instruction streams are managed. A thread is generated in response to a system call to the OS. The OS generates that thread and allocates resources to run the thread. Such resources allocated for a thread may include data structures that the operating system uses to control and schedule the threads.

In contrast at least one embodiment of a shred is generated via a user level software primitive that invokes an OS independent mechanism for generating and scheduling a shred that the OS is not aware of. A shred may thus be generated in response to a user level software call. For at least one embodiment the user level software primitives may involve user level ring instructions that invoke hardware or firmware to create a shred. The shred thus created may be scheduled by hardware and or firmware and or user level software. The OS independent mechanism may be software code that sits in user space such as a software library. Thus instead of relying on the operating system to manage the mapping between thread unit hardware and shreds scheduler logic in user space may manage the mapping. A further discussion of user level shredding instructions may be found in copending patent application U.S. patent Ser. No. 11 173 326 entitled A Mechanism For Instructions Set Based Thread Execution on a Plurality of Instruction Sequencers. 

It should be noted that the sequencers of a system capable of performing embodiments of techniques disclosed herein need not be symmetric. Sequencers may differ in any manner including those aspects that affect quality of computation. For example the sequencers may differ in terms of power consumption speed of computational performance functional features or the like.

In the following sections background information regarding common thread processing is presented in order to provide context for selected embodiments of the present invention. These sections provide background regarding thread environment blocks thread local storage descriptor tables and structured exception handling for OS managed threads. Thereafter selected embodiments of the structure and management of shred environment blocks in accordance with the present invention are presented.

Thread Environment Blocks for OS Managed Threads. is a block diagram illustrating a prior art data management mechanism for OS managed threads that run on OS managed sequencers. OS managed sequencers are referred to herein as OMS .

For at least some common systems critical operating system services such as thread local storage and structured exception handling depend on the presence of a TEB data structure for each thread.

Thread local Storage. A main function of the TEB in many common systems is to maintain and support the private also referred to as local thread data.

In order to maintain the local thread data the operating system may reserve a dedicated instance of the TEB upon creation of each thread. The private data for an OS managed thread may be maintained in a thread local storage TLS area for the thread. The TLS area may be maintained in main memory within the TEB segment or it may be referenced by an address pointer contained within the TEB segment . That is the TEB segment may contain a pointer to the TLS area rather than including the actual storage area reserved for the TLS area .

The value of each thread local variable is stored within the TLS area of the TEB data structure . The value of each thread local variable may be accessed through a per variable key. For example the compiler may store thread local variable foo at an offset of 3 within the TLS area . Each thread accessing foo reads from index inside of the respective TLS areas . Because the TLS area is included within the TEB structure each access to a thread local data variable is converted into direct accesses of the TLS area in memory.

For at least one embodiment a register may be utilized to indicate the base address of the current TEB . For a first embodiment the register may hold a direct memory pointer to the TEB structure . For example for an embodiment based on IA 64 architecture a direct memory pointer to the TEB structure for the correct thread may be maintained in the register . For such embodiment the register may be a general purpose register such as R rather than a dedicated segment register.

Managing thread local data over contexts switches for an embodiment utilizing direct memory pointers may be performed as follows. Upon a context switch the operating system see e.g. of may update the register with a direct memory pointer that points to the new thread s TEB

However providing access to the TEB structure on other platforms such as e.g. IA 32 platforms is more complex. For at least one alternative embodiment the register may be implemented as a segment register. A sequencer s segment register may contain a value that indicates the currently executing thread s TEB segment . Upon a context switch the operating system see e.g. of may update the value in the segment register to indicate the second thread s TEB segment via an index into a global descriptor table described immediately below .

Descriptor Tables. For embodiments that use the segment register approach the value in the segment register may be an index into a descriptor table rather than a direct memory pointer. For such embodiments the TEB structure is maintained in a segment of memory defined by the operating system see e.g. of . A segment descriptor describes this segment of memory. A segment is a block of memory that starts at a fixed base address and has a set length. Each memory reference includes both a segment value and an offset value. The offset value is the location relative to the base address of the segment.

The index into the global descriptor table that accesses the descriptor for the TEB segment for a current thread is kept in one of the sequencer s registers . For an embodiment based on IA 32 architecture the register may be a segment register for example the FS segment register is used by the Windows OS and the GS segment register is used by the Linux OS on the IA 32 architecture .

For example illustrates that a descriptor for the segment for a TEB for Thread may be maintained in the global descriptor table and that an index into the GBT for the descriptor may be stored in the segment register while Thread is active on the OMS .

To directly access the TEB data structure therefore a programmer may specify an index into the TEB segment . For example for a WINDOWS operating system running on an IA 32 platform mov eax FS 0 may load into the EAX register the first word from the TEB segment for the current thread.

For at least some systems there are at least two main tables in memory that store segment descriptors. A third type of descriptor table related to interrupts is outside the scope of this discussion . The first type of descriptor table is the GDT described above. This descriptor table may be equally accessible from all processes. The global descriptor table contains descriptors that are normally available to all tasks in the system. Generally these are tasks used by the operating system.

In addition a descriptor table may be created for a task of a given process. These tables are referred to as local descriptor tables LDT and are discussed in further detail below in connection with .

In addition to supporting thread local data the TEB may also support structured exception handling SEH . SEH is an OS provided service complemented with support from the compiler for handling exceptional events that occur during program execution. Such exceptional events may include for example divide by zero errors privilege violations and the like.

Commonly to invoke structured exception handling an application programmer may include instructions in the code that registers a callback function with the OS . In turn the OS performs the callback when it encounters an exception in user mode during execution of the code thus allowing the user mode application itself to handle the exception that happened when the application was executing.

The compiler may use try except syntax to insert or register exception handlers into the linked list pointed to by the TEB . This registration process specified by the try except syntax is transformed into direct reads from and writes to the TEB . When an exception occurs the SEH service of the OS searches the linked list of user supplied exception handlers to determine the next course of action.

The structured exception handling mechanism illustrated in is thus a dynamic OS supported mechanism that permits registration of a callback handler routine that is to be invoked to transfer control from the operating system to the user level exception handler to handle certain specified user mode exceptions. However because the OS does not provide a TEB for shreds that run on OS sequestered sequencers SEH is not supported for shreds by the mechanism illustrated in .

That is illustrates a TEB that has been allocated for the main thread T by the operating system . However further illustrates that no TEB has been allocated by the operating system for either of the two shreds S and S running on sequestered sequencers Seq and Seq respectively because they are not visible to the OS .

Shred Environment Blocks. Reference is now made to which is a block diagram illustrating at least one embodiment of a data management mechanism for shred local data in a multi shredding system . The embodiment of the multi shredding system illustrated in includes one or more OMS sequencers that are visible to and controlled by operating system .

The system may thus include multiple sequencers that may be managed by an application developer s code outside of operating system control. The developer may utilize in his code instructions that can cause concurrent execution of streams of instructions or shreds on the sequestered sequencers . Shreds running on the OS sequestered sequencers do not commonly have access to a valid TEB instance because the operating system typically reserves an instance of the TEB only upon creation of native threads but not for shreds.

For at least one embodiment the sequencers illustrated in may differ in terms of functionality. An example of functional asymmetry illustrated in shows that at least one sequencer may be visible to the OS and may therefore be capable of performing ring operations such as performing system calls servicing a page fault and the like.

On the other hand one or more other sequencers may be sequestered from the OS and therefore be incapable of performing ring operations. However this is just one example of functional asymmetry. The sequencers of a multi sequencer system may also differ in any other manner such as dimensions word and or data path size topology memory power consumption number of functional units communication architectures multi drop bus vs. point to point interconnect or any other metric related to functionality performance footprint or the like.

For the sample embodiment illustrated in each of the sequestered sequencers have been initialized to be able to execute shreds e.g. S and S associated with a thread. The example illustrated in shows two shreds S and S that have been generated by a thread T which is running on an OS visible sequencer . The shreds S and S have been scheduled to execute on sequestered sequencers Seq and Seq respectively.

Because the system illustrated in allows a programmer to manage execution of shreds S and S on AMS sequencers Seq Seq outside control of the operating system the system includes a mechanism to create and manage data structures to maintain shred local data for the shreds S S. Such data structures are referred to herein as shred environment blocks SEB .

There are many options for the structure organization and management of a shred environment block . For the embodiments described herein the structure organization and management were designed with some specific goals in mind.

The first goal is to provide support for shred local data in a manner that supports ease of programming and the relatively seamless porting of applications that were written for a traditional threading environment into a multi shredding environment. Thus one of the motivations for the SEB structure is to seamlessly facilitate code that contains explicit and or implicit access to the TEB data structure. By using the proposed SEB structure code with explicit and or implicit TEB accesses may execute on both an OS managed sequencer OMS and an application managed sequencer AMS without source code transformations or even recompilation of an existing binary. For example code that implicitly accesses the TLS area through a compiler directive for thread local data can be executed on both an OMS and an AMS without modification in the absence of an SEB structure the code would require modification to be able to execute on an AMS.

The second goal is accomplished by mechanisms as described below to synchronize the SEB and TEB state when migrating code between an OMS and an AMS. In addition the mechanism also provides for synchronizing state when switching contexts from one shred to another on an AMS.

Regarding the first goal the structure of embodiments of the SEB described herein has been designed in order to emulate the design of a commonly known TEB structure. In this manner a programmer familiar with a TEB may understand the structure of the SEB without significant additional effort. Thus the location of each field in the SEB emulates the organization of such fields in the TEB .

The organization of data within the SEB structure may be based for example on the structure of the thread environment block . For example the organization of data of the SEB may be based on a well known implementation for running 32 bit applications for a WINDOWS operating system called Win32. For example the third word in the TEB structure of the Win32 environment holds the starting address of the thread s stack. Accordingly the third word of the SEB structure holds the starting address of the shred s stack. In other words the stack address is maintained at the same offset in the SEB that the programmer is used to for a TEB .

In addition to the organization of the data within the SEB it is desirable to provide for accessing the data of the SEB in a manner that is familiar to the user.

For an embodiment that provides a direct memory pointer in register providing access to the SEB structure is straightforward a system need merely maintain a pointer to the SEB in the Thread Pointer register on the AMS . For example on the IA 64 architecture the software convention is to use register R to contain a pointer to the TEB.

On IA 32 based platforms the SEB in order to be consistent with common TEB schemes is contained within a segment of memory referenced by the FS GS Win32 Linux32 segment register . This segment of memory is referred to herein as a shred local segment .

At least one embodiment of the mechanism illustrated in provides for defining a segment for SEB structures . illustrates a single SEB segment for AMS . As is discussed below multiple SEB segments may be defined for use each AMS . For the time being however is discussed with reference to a single SEB per AMS embodiment.

A descriptor for the segment is created in the Local Descriptor Table LDT . Such segment and its corresponding descriptor may be created for at least one embodiment through an operating system provided service. The operating system service may be invoked by a system call in the shredded code.

The LDT index for the descriptor of the shred local segment is stored in the segment register of the AMS . The SEB is then maintained within the local segment of memory . In this way accessing an offset into the shred local segment on an AMS reads data from the SEB whereas accessing the same offset into the thread local segment on an OMS accesses the native thread s TEB . Because the same fields offsets are maintained in the TEB and SEB structures and respectively code that accesses the TEB will now work with the SEB without requiring code modification.

In summary for the embodiments illustrated in both one can see that the TEB is referenced via a register of the OMS while the SEB is referenced via a register of the AMS . For both embodiments shred local data is stored in or referenced by a shred local storage SLS area . The SLS may be analogous to a thread local storage TLS area that is provided in some existing systems.

For the embodiment the SLS area in the SEB is maintained at the same segment offset as the TLS area . Maintaining the SLS area at the same relative offset as the TLS area allows implicit accesses to the TLS area such as through the compiler directives for TLS storage to work properly regardless of whether the code is running on an AMS or an OMS . Such approach may be utilized for embodiments based on the Win32 based platform but one of skill in the art will recognize that the embodiments discussed herein are not so limited. Various embodiments of the mechanisms illustrated in may be utilized for example for LINUX operating systems running on IA 32 platforms as well as LINUX and or WINDOWS operating systems running on IA 64 platforms.

SEB and Structured Exception Handling. illustrate that the SEB structure may include a field to support structured exception handling in a manner similar to that discussed above in connection with . The SEB structure may include in a field having the same offset as that in a TEB structure a pointer to one or more exception registration records . The compiler provides the try except syntax to the application to insert or register exception handlers into the records pointed to by the pointer in the SEB .

This registration process specified by the try accept syntax is transformed into direct reads from and writes to the SEB . When an exception occurs the SEH service of the OS searches the linked list of user supplied exception handlers to determine the next course of action. In this manner structured exception handling may be supported for shreds.

SEB and Context Switches. A second goal of the design of an SEB management mechanism is facilitate correct code operation across context switches. Context switches may be of several different origins a few of which are discussed below.

For example one type of user level context switch may occur when an OMS sequencer switches from an OS visible task to a user level task. Such context switch may occur for example when an OMS is available to execute a shred. In such case a user level context switch may occur such that the OMS begins to run a user level shred. Such context switch may be viewed as the migration of a shred onto an OMS . For at least one such embodiment the operating system does not schedule the OMS to run the shredded application. Instead a user level scheduler may redirect the OMS to pick up a shred.

If the operating system obtains and schedules work on the OMS before the OMS has completed execution of the shred then another context switch occurs to migrate the shred off of the OMS . Accordingly at least some embodiments discussed herein provide for management of the SEB to synchronize the SEB and TEB state when migrating code between an OMS and an AMS see discussion below of .

In addition to shred migration involving an OMS embodiments of an SEB management mechanism may also provide for synchronizing state when switching contexts from one shred to another on a single AMS or from one AMS to another AMS . A single AMS context switch may occur for example on a system that provides for a round robin or other sharing approach for scheduling of shreds on AMSs . Such scheduling approach may be utilized for instance in a system that supports more shreds M than the number of available AMSs N .

That is much like the manner in which modern operating systems can manage more threads than the number of available sequencers a system that supports user level shreds may manage more shreds M than available AMS s N through an M N user space scheduler. At least one embodiment of such a scheduler is described for example in a co pending patent application bearing Ser. No. 11 027 445 entitled Mechanism To Schedule Threads On OS Sequestered Sequencers Without Operating System Intervention. 

Such a scheduling approach results in a user level context switch each time a new shred is switched in to an AMS and the currently executing shred is switched out. Accordingly at least some embodiments discussed herein provide for management of multiple SEB s to synchronize state when switching contexts from one shred to another on an AMS see discussion below of .

The paragraphs above illustrate several specific examples of the general proposition that a multi shredding system should manage SEB s in a manner that provides correct operation across context switches involving shreds. To achieve this goal there are multiple approaches for creating and maintaining the SEB s .

For an alternative embodiment that assumes a scheme along the lines of that shown in the register illustrated in is not a segment register but instead is designed to store a direct memory pointer to the current SEB structure . The remaining discussion of focuses on an embodiment wherein is a segment register. However one of skill in the art will understand that a similar SEB maintenance scheme may be employed for an embodiment such as that illustrated in that is designed to store a direct memory pointer in register .

At block it is determined whether a context switch on the AMS is desired. In order to discuss the example shown in it is assumed for purposes of discussion that a context switch is desired at block to switch out Shred from the AMS and to switch in Shred n. Accordingly processing proceeds from block to block . If no context switch is desired processing would end at block .

For a context switch the contents of the local segment register for the current shred are saved to a storage location at block . For the example shown in the initial value of the segment register is an index associated with the descriptor for the shred local segment associated with Shred . The contents of register may be saved at block in a manner that associates it with the proper task. Processing then proceeds to block .

At block the segment register is loaded with an index identifying the descriptor for the segment for the new shred that is being switched in. For the example shown in the segment register is loaded at block with an index associated with the descriptor for the shred local segment associated with Shred n.

A later context switch to switch Shred onto an AMS triggers operations and again such that the stored value for Shred is returned to the segment register .

Processing for the method then ends at block . As is illustrated by the discussion above the creation of an SEB for each shred has the result that only the contents of the register are saved restored for context switches but the contents of the shred specific SEB need not be saved and restored for context switches. This is because each shred has its own SEB instance 

An alternative approach is to define only one SEB for each AMS . Such approach is generally illustrated in for an embodiment that uses direct memory pointers in the register . Similarly such approach is generally illustrated in for an embodiment that utilizes register as a segment register. For such embodiments a shred context switch involves saving and restoring the contents of each shred s SEB structure each time a shred context switch occurs.

At block the current contents of the SEB structure for the shred being switched out may be saved in a storage area such as a backing store. Processing then proceeds to block . At block the contents of the SEB structure for the incoming shred may be loaded from a storage area into the SEB structure .

A later context switch to switch the original shred onto an AMS triggers operations and again such that the stored state for the Shred shred environment block may be returned to the SEB structure . Processing then ends at block .

If instead of a context switch that involves only AMS resources the context switch is an AMS to OMS or OMS to AMS migration situation then the management of the contents of the SEB structure may include additional considerations. We turn to for further discussion of such considerations.

For the first context switch data of the TEB structure for the current thread is copied out to a backing store for example . Then at data migration a subset of the SEB state is included with the thread state and is written to the TEB . At least some of this subset of the SEB state may be written to the thread local storage area for the TEB .

For the second context switch a subset of the current TEB state is included with the shred state and is written into the SEB . At least a portion of the subset of the current TEB state may be written to the shred local storage area . The original data for the TEB structure may then be restored from the backing store.

The data migrations illustrated in are applicable to systems that utilize a direct memory pointer in registers and and are equally applicable to systems that utilize the registers as segment registers.

Memory system is intended as a generalized representation of memory and may include a variety of forms of memory such as a hard drive CD ROM random access memory RAM dynamic random access memory DRAM static random access memory SRAM flash memory and related circuitry. Memory system may store instructions and or data represented by data signals that may be executed by processor . The instructions and or data may include code and or data for performing any or all of the techniques discussed herein.

Instructions may include main thread code . Main thread code may include instructions to initialize one or more OS invisible shreds. The initialization instructions of main thread code may when executed by a sequencer cause an OS invisible sequencer to execute a shred instruction stream while sharing the logical execution environment of the main thread.

For at least one embodiment instructions may also include a scheduler routine to schedule shreds for execution on sequencers. The scheduler routine may include instructions to perform at least one of the methods illustrated in respectively for context switches. The scheduler routine may also include logic to perform the data migrations illustrated in .

The processors need not be symmetrical but each may include a front end that supplies instruction information to an execution core . Fetched instruction information may be buffered in a cache to await execution by the execution core . The front end may supply the instruction information to the execution core in program order. For at least one embodiment the front end includes a fetch decode unit that determines the next instruction to be executed. For at least one embodiment of the system the fetch decode unit may include a single next instruction pointer and fetch logic . However in an embodiment where each processor supports multiple thread contexts the fetch decode unit implements distinct next instruction pointer and fetch logic for each supported thread context. The optional nature of additional next instruction pointer and fetch logic in a multiprocessor environment is denoted by dotted lines in .

Embodiments of the methods described herein may be implemented in hardware hardware emulation software or other software firmware or a combination of such implementation approaches. Embodiments of the invention may be implemented for a programmable system comprising at least one processor a data storage system including volatile and non volatile memory and or storage elements at least one input device and at least one output device. For purposes of this application a processing system includes any system that has a processor such as for example a digital signal processor DSP a microcontroller an application specific integrated circuit ASIC or a microprocessor.

A program may be stored on a storage media or device e.g. hard disk drive floppy disk drive read only memory ROM CD ROM device flash memory device digital versatile disk DVD or other storage device readable by a general or special purpose programmable processing system. The instructions accessible to a processor in a processing system provide for configuring and operating the processing system when the storage media or device is read by the processing system to perform the procedures described herein. Embodiments of the invention may also be considered to be implemented as a tangible machine readable storage medium configured for use with a processing system where the storage medium so configured causes the processing system to operate in a specific and predefined manner to perform the functions described herein.

Sample system is representative of processing systems based on the Pentium Pentium Pro Pentium II Pentium III Pentium 4 and Itanium and Itanium 2 microprocessors available from Intel Corporation although other systems including personal computers PCs having other microprocessors engineering workstations personal digital assistants and other hand held devices set top boxes and the like may also be used. For one embodiment sample system may execute a version of the Windows operating system available from Microsoft Corporation although other operating systems and graphical user interfaces for example may also be used.

While particular embodiments of the present invention have been shown and described it will be obvious to those skilled in the art that changes and modifications can be made without departing from the scope of the appended claims. For example although registers and have been discussed above as the means for storing pointers or indices related to shred environment blocks its should be understood by one of skill in the art that any storage means including a latch memory location or other storage mechanism may be utilized instead of a register.

Also for example various application programming interfaces and platforms have been mentioned above including 32 bit and 64 bit WINDOWS platforms as well as 32 bit and 64 bit LINUX platforms However one of skill in the art will recognize that features of the embodiments described herein may be applied to other environments without departing from the scope of the claims appended below.

Accordingly one of skill in the art will recognize that changes and modifications can be made without departing from the present invention in its broader aspects. The appended claims are to encompass within their scope all such changes and modifications that fall within the true scope of the present invention.

