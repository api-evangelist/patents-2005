---

title: Word recognition using ontologies
abstract: Systems, and associated apparatus, methods, or computer program products, may use ontologies to provide improved word recognition. The ontologies may be applied in word recognition processes to resolve ambiguities in language elements (e.g., words) where the values of some of the characters in the language elements are uncertain. Implementations of the method may use an ontology to resolve ambiguities in an input string of characters, for example. In some implementations, the input string may be received from a language conversion source such as, for example, an optical character recognition (OCR) device that generates a string of characters in electronic form from visible character images, or a voice recognition (VR) device that generates a string of characters in electronic form from speech input. Some implementations may process the generated character strings by using an ontology in combination with syntactic and/or grammatical analysis engines to further improve word recognition accuracy.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07587308&OS=07587308&RS=07587308
owner: Hewlett-Packard Development Company, L.P.
number: 07587308
owner_city: Houston
owner_country: US
publication_date: 20051121
---
This disclosure relates generally to computational linguistics and in particular implementations to word recognition.

Languages enable efficient high quality communication. People for example use language to communicate ideas. Even computers use language to interpret information in the form of program instructions. Languages are typically based on a vocabulary and a grammar. Many modern languages are capable of verbal i.e. oral expression whereby a human voice may carry speech. Examples of verbal expressions of language are evident for example in radio broadcasts audio books and everyday dialogue. Many languages are also capable of written expression whereby characters are tangibly embodied in a medium. Examples of written expressions of language include books newspapers and legal documents.

Whether expressed in verbal or written form languages typically include a vocabulary to represent ideas or concepts. A vocabulary may include all the words of a language or all words that may be used by or understood by a particular group.

Although there may be many exceptions languages typically employ a set of rules or a grammar to structure the vocabulary into organized patterns. Grammar may provide structural relationships in a language including a system of rules for generating all sentences possible in a language.

In addition to vocabulary and grammar expressions of language may be understood from the perspective of semantics. In general semantics may relate to the meaning that is conveyed by language which may be shaped in large measure by the context in which an idea is expressed. For example row may be understood as a verb in the context of a small boat but be understood as a noun with a very different meaning in the context of a table in a spreadsheet. Whether communicated in verbal or written form semantic analysis may be important to understanding the intended meaning of the word row. For example verbal expressions of homonyms like way and weigh may be indistinguishable without reference to the context in which they are used.

Systems and associated apparatus methods or computer program products may use ontologies to provide improved word recognition. The ontoLogies may be applied in word recognition processes to resolve ambiguities in language elements e.g. words where the values of some of the characters in the language elements are uncertain. Implementations of the method may use an ontology to resolve ambiguities in an input string of characters for example. In some implementations the input string may be received from a language conversion source such as for example an optical character recognition OCR device that generates a string of characters in electronic form from visible character images or a voice recognition VR device that generates a string of characters in electronic form from speech input. Some implementations may process the generated character strings by using an ontology in combination with syntactic and or grammatical analysis engines to further improve word recognition accuracy.

In an illustrative example a most likely value for a character e.g. alphanumeric character in a converted word is selected so that the converted word makes sense in the context of other words in the same phrase sentence paragraph or discourse. The most likely value may be selected by performing an analysis according a predetermined ontology. An ontology may include a linked collection of words concepts or issues that relate to the environment or domain that pertains to the phrase sentence paragraph or discourse. The predetermined ontology may define relationships among a number of language elements that are likely to be used for example in a particular context and or are related to particular subject matter. In this illustrative example a VR or OCR device may output a string of ASCII characters representing a sentence to be converted into recognized words.

Some implementations may provide one or more advantages. For example the system may provide for improved accuracy in resolving ambiguous character strings to text. Some implementations may reduce the number of errors associated with VR or OCR devices thereby promoting their broad adoption. Such implementations may be used to provide more reliable products or services for disabled individuals such as those who have visual or auditory impairments and who may benefit from more accurate word recognition in VR and or OCR technologies. Improved word recognition may promote broader use of text searchable documents by reducing the occurrence of word recognition errors.

The details of one or more implementations of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

The system receives an input text which may contain ambiguous language elements. The input text may undergo a syntactic analysis followed by a semantic analysis to generate an output text that may have reduced ambiguity. In this example the syntactic analysis is performed by a syntax analyzer and a grammar analyzer that is coupled to access a part of speech POS rules database . The syntax analyzer compiles a word list with POS which the grammar analyzer processes into an improved word list . The improved word list undergoes semantic analysis processing by a context analyzer in cooperation with an ontology selector that is coupled to a context ontology database .

In this example the system further includes a capability for manually automatically and or adaptively defining ontology relationships. This relationship defining capability may be performed by a user feedback module and or a machine learning module processing information contained in the output text for example. The modules feed relationship defining information back to the context ontology database . In various implementations this adaptation may be performed on line and or off line.

In some implementations the system may operate as follows. The input text may contain information in electronic form and or signals that represent a number of characters i.e. a string of characters . The character string may correspond to a sequence of words. In some cases the sequence may include one or more ambiguous words in which the values of some characters are uncertain and or have less than a threshold level of certainty for example. The system may perform an analysis on the words in the received character string according to an ontology that defines relationships among a number of words or other language elements. By identifying matches between words in the ontology and words in the received character string the context analyzer may be able to identify the most likely value for one or more of the ambiguous words in the input text . The context analyzer may then assign the identified most likely values to the respective uncertain words to generate the output text .

In the exemplary system the input text may be derived from speech information and or character image information that have been processed by a voice recognition module or an optical character recognition module respectively. The speech information or character image information may be in a physical form e.g. sound pressure vibrations a visual display or in an electronic form such as an analog i.e. continuous time recording or in a digitally sampled form. In some examples the information may be derived from a radio television or satellite broadcasts or from a telephone conversation microphone video image capture device e.g. video camera or the like. The information may be received in real time or stored in an electronic storage media i.e. data storage device such as volatile and or non volatile memory for example. In some implementations the speech information may be formatted in an audio format such as for example Wave Form Audio format WAV or Moving Pictures Experts Group Audio Layer 3 format MP 3 . In some implementations the character image information may be formatted using for example Joint Photographic Experts Group format JPEG or the Tagged Image File Format TIFF .

The process of converting written or verbal expressions to electronic strings of characters can involve some uncertainty in the resulting character string. For example the speech information and character image information may be inputs that are not perfectly recognizable. Accordingly the recognition modules may employ various algorithms resolution levels and processing times to optimize the recognition quality against the processing time to perform the conversion. However in some cases some of the input speech or written language may not be recognized with a high degree of certainty. The accuracy of the conversion from these language inputs to electronically stored characters depends on the quality of both the input language information and the conversion processes in the recognition module. To the degree that the conversion performed by the recognition module is imperfect the value assigned to the electronic characters can be uncertain. If one or more characters in a converted character string have uncertain values then some words in the text may be ambiguous.

To quantify the degree of uncertainty associated with the converted characters or other language elements the recognition modules may in some implementations associate confidence levels with some or all language elements in the input text . The confidence level associated with a particular language element may indicate the degree of uncertainty in the converted value of the electronic language element which may be a character i.e. letter number group of characters e.g. sh ch th or other language component e.g. phrase in the text . As such the confidence levels may indicate to what degree the conversion is likely to correctly represent the expression of the language input information . In one implementation confidence levels may be expressed for example as a percentage i.e. 0 100 . In other examples language elements e.g. characters of the text may be associated with other quantitative indicia that represent the degree of confidence in the values of the characters determined by either or both of the recognition modules . Recognition modules may also use multiple probability indicia to represent the likelihood of the accuracy of a word or use symbolic labels such as Low Medium and High to accomplish the same. Confidence ranges may also be represented via functions that are dependent on other domain parameters for example Confidence 0.0.5 Frequency 2 for a telephone conversation and Confidence 0.08 Frequency 2 for a CD recording where the confidence is influenced by a combination of high pitch and the medium phone v. CD .

For purposes of clearly describing various aspects and operations the system will be described with reference to the following exemplary character string which may be included in an exemplary input text The at ate he at.

The question marks indicate Language elements which may include one or more characters that are associated with a relatively low confidence level. In this illustrative example the character string is only partially recognized. Specifically the second fourth and fifth words include values that are sufficiently uncertain that those words may be considered to be ambiguous. Furthermore the VR module is subject to incorrect interpretations of words that have substantially identical sounds i.e. homonyms but are otherwise unambiguous. For example the third word ate has a homonym eight that has a substantially identical phonetic form but conveys a substantially different meaning.

According to the example the syntax analyzer module may receive the exemplary input text . In general the input text may be symbolically represented by the following C w C . . . w C .

Where T represents the input text Wis the ktoken of a word vector e.g. sentence in the text wis the hpossible alternate of a word e.g. cat sat bat fat and eat for at and Cis the confidence level associated with the halternate. In some implementations the syntax analyzer module may operate on one word vector at a time for example to generate a set of possible word sequences that represent potentially grammatically sound sentences. In the exemplary sentence there are multiple points of imprecision and or ambiguity. Accordingly the syntax analyzer may identify a list of alternate words or word lists for each of the ambiguous words in the exemplary sentence as indicated in Table 1.

The syntax analyzer module may identify for each alternate word any possible part of speech POS and a corresponding probability value to indicate the degree of certainty or uncertainty for each identified POS.

Based on the input text the corresponding confidence levels and the alternate words in the word lists the syntax analyzer may assign probabilities that a particular word in one of the word lists is of a particular part of speech such as a noun a verb or an adjective. The syntax analyzer module may generate the word list with POS which may be symbolically represented by the following . . . .

Where L is the word list with POS Srepresents a sentence wis a word in position j and Qcontains probabilities that the word in position j is of a particular part of speech. For example in the word list for at 1occurrence shown below in Table 2 the language element cat may have a 90 probability of being a noun and a 10 probability of being a verb. Table 2 also shows the confidence levels which may have been provided by the recognition module as described above .

In the system of the syntactic analysis of the syntax analyzer is followed by the grammatical analysis of the grammar analyzer . Some additional details of an implementation of the grammar analyzer are described below with reference to . In other implementations the grammatical analysis need not follow the analysis performed by the syntax analyzer .

The grammar analyzer may receive the word list with POS and apply the part of speech rules in connection with a predetermined or a selected grammar to verify the syntactic soundness of the word list with POS . In some implementations the part of speech rules may access information contained in an electronic dictionary and or an electronic thesaurus regarding possible part of speech interpretations of each language element in the word list.

The grammar analyzer may be configured to use a context free grammar such as the Cocke Younger Kasami CYK or Earley s algorithms which may include rules such as in Table 3.

As will be described in detail with reference to the grammar analyzer may determine that the exemplary sentence has the following form Determiner Noun Verb Determiner Noun.

Because the sentence has an acceptable form under the grammar rules of Table 3 it is included in the improved word list . In embodiments if the input is not grammatically recognized then it may be discarded. In some examples words or language elements from Table 1 that would not form grammatically acceptable sentences may be removed from further consideration. In some examples the score associated with such words or sequences of words may be adjusted based on this determination. In the illustrative example as indicated in Table 2 the low probability of eat being a noun eight being a verb and she being a determiner these language elements may be effectively eliminated from further consideration. Accordingly the grammar analyzer may output the improved word list as shown in Table 4.

Semantic analysis may be performed on the improved word list by the context analyzer . Based on the semantic analysis the context analyzer may score the words in the improved word list for example by assigning a score to each word. The score may be used as a basis for inferring a most likely meaning from a context ontology selected from the context ontology database .

In some implementations the context ontology database may store one or more ontologies. For example the context ontology database may contain or otherwise have access to predetermined ontologies such as an automotive ontology a sales and marketing ontology and or a zoological ontology. in various embodiments ontologies have nodes connected by links that are meaningful in a selected domain. Domains in which a context ontology may be defined include for example the context of credit card applications in the banking industry or quality control assessments in a particular manufacturing e.g. automotive or service e.g. travel context.

In one implementation the improved word list may first be examined to identify keywords that may be useful for selecting a ontology. The ontology selector may use the identified keywords to select at least one candidate ontology from the context ontology database .

Using a selected ontology the context analyzer may score the candidate words individually i.e. each word in the improved word list and or in groups. For example the context analyzer may determine a composite score for a sequence of words e.g. a sentence that contains more than one ambiguous word.

In various implementations the score may be determined by applying functions of one or more criteria and the criteria may be combined according to linear and or non linear relationships e.g. additive multiplicative exponential logarithmic case based logic . Each score may be a function of criteria including for example the number and or degree e.g. proximity of matches between nodes in the selected ontology and the words in the improved word list . Another factor that may be accounted for in the scoring is the relatedness or proximity in the input text of the ambiguous word to other words in the sentence that match nodes in the ontology. In some examples a particular match that may be identified between two words in a sentence and corresponding matching nodes in an ontology may be highly relevant such as if the two words in the sentence are closely coupled in meaning. In other examples however two unrelated words in a sentence may happen to match nodes defined in an ontology but not in a meaningful way. As such factors such as proximity between words in the input text and coupling between such words may be applied to adjust a score.

Scoring functions may depend on other factors such as the confidence level information associated with characters or other language elements in the text for example. In some implementations the scores may be functions of weights that are associated with links between nodes of interest in an ontology. The probabilities associated with possible parts of speech as discussed with reference to Table 2 for example may influence scoring. As mentioned elsewhere herein the results of the grammatical analysis may be applied to adjust the scores. In some implementations the score may be a function of weights that may be assigned to grammar rules such as those defined in Table 3. Important grammar rules are assigned relatively heavy weights less important grammar rules are assigned relatively light weights.

The context analyzer may select the highest scoring word or sequence of words to replace an ambiguous word in the text with a candidate word or sequence of words that is identified as having the highest score. After substituting the ambiguous words in the text with the high scoring words the context analyzer may output the output text . In implementations the output text may be stored in a memory location saved in an electronic document and or sent for display on a display device.

In implementations the scoring may be performed on candidate words that are generated by processes that differ from those described above to generate the improved word list . For example a List of candidate words may not have been analyzed by the grammar analyzer .

In various implementations the ontology selector may use one or more techniques such as neural networks case based reasoning or hash tables to select appropriate context ontoLogies. If more than one ontology is determined to be appropriate then in one example each candidate ontology may be applied in turn to score the candidate words in the improved word list . In one implementation scores resulting from each candidate ontology may be compared on an individual or an aggregate basis to identify which candidate ontology is the best match to the improved word list . The ontology with the best match may be selected as the ontology from which the highest scoring words or sequence of words may be selected to generate the output text .

In some implementations the context analyzer may use other e.g. non textual information to generate scores. Non textual speech information may include for example pitch speed and or amplitude. Non textual image information may include for example font type character size boldface italics color and or underlining. Such non textual information may be used to score and or to select candidate language elements. Context analyzer may use functions heuristics or other techniques to compute the score.

For example the pitch and speed of the speech information may be considered by the ontology selector to select appropriate ontologies. Specifically a high pitch and fast speed may indicate an excited or rushed context e.g. an emergency or high tension context whereas a low pitch and normal speed may indicate a more relaxed discourse.

Such non textual information may be defined to be associated with links or nodes in an ontology. The context analyzer may then use such non textual information in scoring candidate words. By accounting for non textual information the accuracy of the word recognition process may be further improved.

The system of further includes the user feedback module and or the machine Learning module which provide features to define i.e. build update or edit an ontology during on line and or off line operations. In addition these feedback modules may also provide information that can be used by the ontology selector to more accurately select an ontology for a given input text . For example in response to the output text a user may provide information to update one or more ontologies via the user feedback module . In some implementations the machine learning module may provide information to improve ontology selection based on historical i.e. past performance data. The machine learning module may use any appropriate machine learning method based on user feedback. For example case based reasoning neural nets or rules heuristics may be implemented to direct the user feedback into a representation that improves the performance of the context ontologies .

The user feedback module may be coupled to a user interface to send information to a user or a database maintainer e.g. a context expert to indicate the current status of a particular ontology such as the ontology that is currently or most recently active. The user feedback module may also be operably coupled to a user interface configured to receive user input e.g. via keyboard or voice command . The user input may define new information for use in an ontology such as a new link and or a new node. The user input may include corrective feedback information for example that is responsive to a scoring result generated by the context analyzer .

In some implementations the corrective feedback may include information to adapt weights associated with specific links in an ontology. For example confirming user feedback information may provoke the machine learning module to increase weights associated with links that were significant in the scoring of a correct value for the output text . As another example corrective user feedback information may provoke the machine learning module to decrease weights associated with links that were significant in the scoring of an incorrect value for the output text .

The context analyzer may process improved word lists according to an exemplary ontology which is schematically represented in . In this example the ontology pertains to zoological context which is appropriate for the exemplary sentence discussed above. The ontology includes concepts represented as nodes and relationships represented as links between the nodes. In this example the ontology includes a cat node that is related to a rat node by a link . The link represents the idea that cats sometimes eat rats.

The context analyzer may for example process the improved word list according to the ontology . In some implementations the candidate words in the improved word list may be scored based on the degree of relatedness between ambiguous elements in the input text and nodes in the ontology . In one implementation candidate words that do not appear in the ontology may be eliminated from further consideration or scores may be adjusted to reflect the degree of apparent relatedness. For example the candidate language elements sat and fat do not exist in the zoological ontology selected. Therefore sat and fat from Table 4 may be eliminated from further consideration which results in the list shown in Table 5.

In the ontology the bat node does not have a link to the rat node . Therefore it is unlikely that bat is the correct candidate word. As such the context analyzer would assign a low score to the candidate word bat which indicates that it is not a likely candidate. However the cat node does have a direct link to the rat node . Therefore the context analyzer would assign a high score to the candidate language element cat indicating that it is a likely candidate. The context analyzer may output in the output text the first sentence in Table 6 because it includes the most likely set of words according to the selected ontology .

As described above with reference to Table 3 the grammar analyzer may be configured to use a context free grammar that includes a number of grammatical rules. Operations performed by the grammar analyzer are next described in additional detail with reference to an exemplary parse tree as shown in .

The grammar analyzer may output one or more possible grammatically acceptable sentences in the improved word list . Associated with each of the sentences may be a corresponding parse tree.

The parse tree may be used to describe the analysis of a possible sentence from the word list with POS . The grammar analyzer may parse the words of the sentence according to their possible part of speech which are summarized for one example in Table 2. If the words in the possible sentence are ordered so that their corresponding part of speech can be grouped in permissible ways according to the grammar rules in Table 3 then the possible sentence is considered to be a grammatically acceptable sentence.

The following example illustrates one implementation of the analysis that may be performed by the grammar analyzer to generate the improved word list . In this example the parse tree represents a corresponding parse tree for one of the possible sentences that may be derived from the word list with POS Table 1 The cat ate the rat. This possible sentence and an associated set of possible part of speech see Table 2 are shown in Table 7.

In this example the parse tree has at its root node a sentence that takes the value of the possible sentence i.e. The cat ate the rat. According to the first rule in Table 3 the sentence contains a noun phrase and a verb phrase which comports with the first rule in Table 3. The noun phrase is divided into a determiner and a noun which complies with the third rule in Table 3. The verb phrase is divided into a verb and another noun phrase which is in accord with the fifth rule in Table 3. The noun phrase is also divided into a determiner and a noun which complies with the third rule in Table 3. The candidate words The cat ate the and rat and their corresponding part of speech comply with the grammar rules defined in Table 3. Accordingly for this set of part of speech this possible sentence is grammatically acceptable.

Grammatically acceptable sentences or other sequences of language elements may be included in the improved word list . Some other possible sentences or other sequence of language elements may not qualify under the grammar rules defined in Table 3. For example possible sentences from Table 1 that contain the candidate words eat eight and she do not comply with any of the grammar rules in Table 3. In some implementations these possible sentences that are not grammatically acceptable are not included in the improved word list . In other implementations a penalty may be applied to the scores determined by the context analyzer for sentences that are not grammatically acceptable.

One implementation of the method will be described with reference to the system of . This is to more clearly describe the method. As such other implementations of the method may be performed by apparatus other than the system and the system may perform methods other than the method .

Beginning at the system receives a plurality of characters such as a character string in the input text . In some implementations other information such as non textual information may be associated with the character string. The character string is parsed at step into language elements LEs which may be words for example. At step confidence levels may be associated with each parsed LE for example by multiplying or otherwise combining the confidence levels associated with each character in the LE. Ambiguous LEs are identified at step for example by comparing the confidence level of each LE or each character in the received string to various possible thresholds.

The syntax of the character string is analyzed beginning at step by the syntax analyzer . The syntax analyzer may generate a list of alternate LEs for each identified ambiguous LE at step . For each generated alternate word the syntax analyzer may also identify one or more possible part of speech and an associated probability for each possible part of speech at step to generate the word list with POS .

Some implementations may include the grammar analyzer to perform a grammatical analysis step that generates the improved word list . In general the grammar analyzer may employ techniques to filter out possible combinations of language elements that do not comply with a predetermined grammar or set of grammar rules such the part of speech rules . This grammatical analysis step may remove from further consideration possible sentences that would not be grammatically acceptable.

The context analyzer receives a list of words at step the list being generated either by the syntax analyzer or by the grammar analyzer .

At step candidate ontologies are identified. For example the context analyzer may provide keywords to the ontology selector . The ontology selector may use the keywords to determine one or more candidate ontologies.

At step the ontology selector selects a candidate ontology and at step the context analyzer selects one of the ambiguous language elements that was identified at step .

At step the context analyzer identifies nodes in the selected ontology that correspond to the selected ambiguous language element.

At step the context analyzer identifies relationships or links in the selected ontology between the identified nodes and other nodes in the ontology that correspond to other language elements in the character string. For example the cat node which corresponds to the ambiguous element at 1occurrence has the relationship with the rat node which corresponds to the at second occurrence . The link represents the relationship based on the fact that cats sometimes eat rats.

At step the context analyzer identifies candidate language elements that may replace the selected ambiguous language element. In this process for example the context analyzer may identify nodes in the selected ontology that might match based on the known i.e. not uncertain characters in the selected ambiguous word.

At step the context analyzer scores the identified candidate language elements. The score may be a function of one or more criteria such as the number arrangement and or strength of the links identified at step . For example the context analyzer may assign a high score to the language element cat because the cat node has a direct relationship i.e. separated by a single link with another identified node namely the rat node . The bat node does not have a direct relationship with any of the identified nodes. Therefore the context analyzer may assign a low score to the language element bat. Accordingly the bat language element is a less likely candidate than the cat as a substitute for the at language element in the text .

In some implementations the score of a candidate language element may be determined by an equation. For example the score may be determined by the directness of a candidate language element s relationship to another language element in the improved word list . A more direct relationship i.e. one link of separation between nodes may result in a higher score which indicates a more likely candidate. In another example each relationship is given a weight where stronger relationships have more weight than weaker relationships. The score of the individual relationship may be multiplied for example by its weight to determine an overall score for the candidate. The candidate s score may be a function of a combination of the weights of several series connected links that connect the candidate language element to the second language element. Subsequent relationships between the two language elements may reduce the score of the candidate. Multiple chains of links from a node to multiple related nodes may be aggregated to yield a higher score. In some implementations certain additional links in a series of links connecting to a third language element included in the text may increase the score of the candidate. The score of the candidate may also be a function of the confidence levels provided by the recognition modules . For example the score determined by the candidate s relationships to other language elements in the text may be multiplied by its confidence level as determined by one of the recognition modules .

At step if more ambiguous language elements remain then step is repeated after selecting another ambiguous language element. Otherwise if all identified ambiguous language elements have been evaluated then at step the context analyzer checks whether other identified candidate ontologies must still be evaluated. If other identified candidate ontologies must still be evaluated then step is repeated to select the next candidate ontology.

If all identified candidate ontologies have been evaluated then at step the context analyzer selects the candidate language elements with the highest scores. Finally at step the context analyzer prepares to generate the output text by replacing each ambiguous LE with the highest scoring candidate.

Relationships in an ontology such as one stored in the context ontology database may be defined by performing a set of operations according to an exemplary method as illustrated in a flowchart in . For example the method may be performed by the user feedback module and the machine learning module to define nodes and links in the context ontology database . Operations to define relationships in an ontology may include for example creating modifying updating editing or removing nodes and or links. In some implementations instructions tangibly embodied in an information carrier may be executed on a processor to perform portions of the method .

One implementation of the method will be described with reference to the system of . This is to more clearly describe the method. As such other implementations of the method may be performed by apparatus other than the system and the system may perform methods other than the method .

In this example the operations begin at step with identifying language elements used together in a particular context. For example a user may input the language elements or the language elements may be derived from another source such as an on line and or electronic dictionary or thesaurus.

At step a link is defined between two language elements in an ontology and at step the link is stored in an information repository. For example the user may input the link and it may be stored in the context ontology database .

At step if additional links are to be defined step is repeated. Otherwise at step links and language elements may be updated in response to user input. For example upon receiving the output text the user may provide corrective feedback information to the user feedback module and or to the machine learning module . In some implementations the machine learning module may update the context ontology database according to the user feedback and or according to other input signals or conditions as specified in a set of stored program control instructions.

In some implementations the updates may occur during on line operation as the user input is received. In other implementations the updates may occur during off line operations such as during development installation or maintenance.

At step the user feedback module checks whether to continue or to terminate the method . If no termination signal is received then the user feedback module may repeat step and continue to update context ontologies. If a termination signal is received such as by a termination command from a user then the method ends.

In addition to the above described examples word recognition systems may be implemented using systems methods or computer program products other than the examples described above.

For example each language element may include one more characters e.g. letters numbers or symbols e.g. punctuation e.g. . numbers letters and similar written expressions. The language elements may represent groups of characters such as phonetic groupings e.g. sh th or qu words phrases sentences paragraphs discourses documents conversations and the like. In some implementations the language elements may be expressed in any recognizable communication language e.g. English Spanish Urdu or combination of languages. Written language elements may be expressed in a variety of combinations of capitalizations font sizes styles colors and emphases e.g. normal bold underline italic . Verbal language elements may be expressed using a variety of variable characteristics such as pitch speed amplitude and tone quality.

As another example scores may be bounded within a range e.g. 0 . . . 1.0 or 10 to 10 or unbounded and may be positive and or negative. Similarly confidence levels that may be used to score a number of candidate language elements may include both positive and negative values. While positive confidence levels may indicate the likelihood that a language element should be converted to a particular value negative confidence levels may indicate the likelihood that a language element should not be converted to a particular value. A scoring algorithm which may be part of the context analyzer may take into account both positive and negative type confidence levels for example by assigning positive and negative weights e.g. coefficients respectively to the corresponding values. In this manner a recognition module for example may further contribute to word recognition accuracy by providing information about both what values the language elements are likely to be and what the language elements are likely not to be.

In some implementations one or both of the recognition modules may generate more than one possible value for each character in the text and each value may be assigned a confidence level. In such implementations the system may resolve ambiguities by analyzing one or more of the values generated by the recognition modules . In one implementation for example the system may analyze all values that are assigned a confidence value that is at or above a predetermined threshold. In another implementation the system may analyze a certain number of the highest confidence threshold values and may or may not apply a threshold.

In addition to using ontologies having links between nodes as described above some embodiments may include reflective links which are described in published U.S. patent application Ser. No. 10 455 780 Pub. No. 2004 0249829 filed on Jun. 5 2003 the entire contents of which are incorporated herein by reference.

The word recognition system may be implemented as a computer system that can be used with implementations of the invention.

Various implementations of the invention may be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. Apparatus can be implemented in a computer program product tangibly embodied in an information carrier e.g. in a machine readable storage device or in a propagated signal for execution by a programmable processor and methods can be performed by a programmable processor executing a program of instructions to perform functions of the invention by operating on input data and generating output. The invention can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. A computer program is a set of instructions that can be used directly or indirectly in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment.

Suitable processors for the execution of a program of instructions include by way of example both general and special purpose microprocessors and the sole processor or one of multiple processors of any kind of computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally a computer will also include or be operatively coupled to communicate with one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in ASICs application specific integrated circuits .

To provide for interaction with a user the invention can be implemented on a computer having a display device such as a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.

The invention can be implemented in a computer system that includes a back end component such as a data server or that includes a middleware component such as an application server or an Internet server or that includes a front end component such as a client computer having a graphical user interface or an Internet browser or any combination of them. The components of the system can be connected by any form or medium of analog or digital data communication including packet based messages on a communication network. Examples of communication networks include e.g. a LAN a WAN wireless and or optical networks and the computers and networks forming the Internet.

The computer system may be implemented as a distributed computing system and can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

A number of implementations of the invention have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the invention. For example advantageous results may be achieved if the steps of the disclosed techniques were performed in a different sequence if components in the disclosed systems were combined in a different manner or if the components were replaced or supplemented by other components. The functions and processes including algorithms may be performed in hardware software or a combination thereof and some implementations may be performed on modules or hardware not identical to those described. Accordingly other implementations are within the scope of the following claims.

