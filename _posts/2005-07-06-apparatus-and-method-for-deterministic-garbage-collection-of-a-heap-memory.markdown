---

title: Apparatus and method for deterministic garbage collection of a heap memory
abstract: A method includes executing an application in an execution environment. The application is allocated a plurality of memory blocks in a memory during execution. The method also includes executing a deterministic garbage collection process. The garbage collection process is capable of reclaiming at least one of the memory blocks in the memory from the application so that the at least one reclaimed memory block can be reallocated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07962707&OS=07962707&RS=07962707
owner: Honeywell International Inc.
number: 07962707
owner_city: Morristown
owner_country: US
publication_date: 20050706
---
A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

This application is related to U.S. patent application Ser. No. 11 175 848 entitled DETERMINISTIC RUNTIME EXECUTION ENVIRONMENT AND METHOD filed on Jul. 6 2005 which is hereby incorporated by reference.

This disclosure relates generally to memory management and more specifically to an apparatus and method for deterministic garbage collection of a heap memory.

Several attempts have been made to create execution environments in which certain types of computer programs are executed. In general a conventional execution environment provides support for basic features that many programs assume are available for use. For example conventional execution environments typically include support for performing various mathematical functions such as sine and cosine operations input output functions such as reading and writing files and communication functions such as network and database access . Some conventional execution environments provide additional functionality such as just in time compilation of code machine independence and portability remote operation and enhanced internetworking. Just in time compilation refers to the compilation of code that is performed when execution of the code is requested for the first time. Conventional execution environments that support these additional functions are generally referred to as virtual machines. The Common Language Infrastructure CLI by MICROSOFT CORPORATION and JAVA by SUN MICROSYSTEMS are examples of execution environments.

Conventional execution environments often support the management of memory used during execution of computer programs. Memory is typically a finite resource needing careful management so that programs needing memory can obtain it when necessary. There are often several types or classes of memory in an execution environment including a heap memory or heap . The heap typically represents memory that is highly dynamic in its use meaning that heap memory is frequently allocated to programs that use it for a short time and then return it for reuse by other programs. A heap manager often controls when and how the heap memory is used by the computer programs.

In some execution environments programs that request the use of heap memory may not explicitly return it when their use of the heap memory is complete. In these execution environments the heap manager often automatically determines when heap memory is no longer being used by a program that requested it. The heap manager then reclaims the identified heap memory allowing the heap manager to allocate the identified memory to another program. This function is typically referred to as garbage collection. 

A problem with conventional execution environments is that they are not deterministic in nature. The term deterministic generally refers to the ability to predict or specify the behavior of a program or environment. Conventional execution environments are not deterministic because they typically suffer from occasional and unpredictable delays including delays associated with the functions performed by the heap manager. These delays represent non deterministic behavior in the conventional execution environments.

This typically makes these execution environments unsuitable for use with real time applications. Real time applications may represent programs that interact with an outside environment or otherwise operate in a way that is carefully timed. The non deterministic behavior of conventional execution environments often translates into or causes non deterministic behavior in the execution of real time applications. As a result the real time applications often cannot maintain their exact time schedules when executed in conventional execution environments which causes the real time applications to fail.

This disclosure provides an apparatus and method for deterministic garbage collection of a heap memory.

In one embodiment a method includes executing an application in an execution environment. The application is allocated a plurality of memory blocks in a memory during execution. The method also includes executing a deterministic garbage collection process. The garbage collection process is capable of reclaiming at least one of the memory blocks in the memory from the application so that the at least one reclaimed memory block can be reallocated.

In another embodiment an apparatus includes a memory comprising a plurality of memory blocks capable of being allocated to an application executed in an execution environment. The apparatus also includes a deterministic memory manager capable of reclaiming at least one of the memory blocks in the memory from the application so that the at least one reclaimed memory block can be reallocated.

In a third embodiment a computer program is embodied on a computer readable medium and is operable to be executed by a processor. The computer program includes computer readable program code for executing an application in a plurality of first time slots in an execution environment. The application is allocated a plurality of memory blocks in a memory during execution. The computer program also includes computer readable program code for reclaiming at least one of the memory blocks in the memory from the application so that the at least one reclaimed memory block can be reallocated. The reclaiming is performed incrementally in a plurality of second time slots. The second time slots interleaved with the first time slots.

Other technical features may be readily apparent to one skilled in the art from the following figures descriptions and claims.

In this example embodiment the process control system includes one or more process elements . The process elements represent components in a process or production system that may perform any of a wide variety of functions. For example the process elements could represent motors catalytic crackers valves and other industrial equipment in a production environment. The process elements could represent any other or additional components in any suitable process or production system. Each of the process elements includes any hardware software firmware or combination thereof for performing one or more functions in a process or production system.

Two controllers are coupled to the process elements . In this document the term couple and its derivatives refer to any direct or indirect communication between two or more elements whether or not those elements are in physical contact with one another. The controllers control the operation of the process elements . For example the controllers could be capable of providing control signals to the process elements periodically. As a particular example if a process element represents a motor one of the controllers could provide control information to the motor once every millisecond. Each of the controllers includes any hardware software firmware or combination thereof for controlling one or more of the process elements . The controllers could for example represent C controllers.

Two servers are coupled to the controllers . The servers perform various functions to support the operation and control of the controllers and the process elements . For example the servers could log information collected or generated by the controllers such as status information related to the operation of the process elements . The servers could also execute applications that control the operation of the controllers thereby controlling the operation of the process elements . In addition the servers could provide secure access to the controllers . Each of the servers includes any hardware software firmware or combination thereof for providing access to or control of the controllers . The servers could for example represent personal computers such as desktop computers executing WINDOWS 2000 from MICROSOFT CORPORATION. In this document the term application refers to one or more computer programs sets of instructions procedures functions objects classes instances or related data adapted for implementation in a suitable computer language.

One or more operator stations are coupled to the servers . The operator stations represent computing or communication devices providing user access to the servers which could then provide user access to the controllers and the process elements . For example the operator stations could allow users to review the operational history of the process elements using information collected by the controllers and servers . The operator stations could also allow the users to adjust the operation of the process elements controllers or servers . Each of the operator stations includes any hardware software firmware or combination thereof for supporting user access and control of the system . The operator stations could for example represent personal computers executing WINDOWS 95 WINDOWS 2000 or WINDOWS NT from MICROSOFT CORPORATION.

In this example at least one of the operator stations is a remote station. The remote station is coupled to the servers through a network . The network facilitates communication between various components in the system . For example the network may communicate Internet Protocol IP packets frame relay frames Asynchronous Transfer Mode ATM cells or other suitable information between network addresses. The network may include one or more local area networks LANs metropolitan area networks MANs wide area networks WANs all or a portion of a global network such as the Internet or any other communication system or systems at one or more locations.

In this example the system includes two additional servers . The servers execute various applications to control the overall operation of the system . For example the system could be used in a processing or production plant or other facility and the servers could execute applications used to control the plant or other facility. As particular examples the servers could execute applications such as enterprise resource planning ERP manufacturing execution system MES or any other or additional plant or process control applications. Each of the servers includes any hardware software firmware or combination thereof for controlling the overall operation of the system .

As shown in the system includes various redundant networks and single networks that support communication between components in the system . Each of these networks represents any suitable network or combination of networks facilitating communication between components in the system . The networks could for example represent Ethernet networks.

In one aspect of operation one or more of the controllers servers or other components in the system execute one or more applications such as real time applications to control the process elements . For example the controllers could periodically generate control signals or other signals needed by the process elements to operate correctly.

At least one of the components in the system also executes supports or otherwise provides access to an execution environment. The execution environment provides support for various features that managed applications may use during execution. For example the execution environment could provide support for mathematical functions input output functions and communication functions used by the managed applications. The execution environment could also support compilation of assembly code management of a heap memory and any other or additional functions. The phrase managed application refers to an application executed in the execution environment where the execution of the application is managed by the execution environment. Managed applications could include the real time applications used to control the process elements in the system . Managed applications may occasionally be referred to as managed code user code or user applications. 

The execution environment used in the system to execute the managed applications is deterministic. A deterministic execution environment is an execution environment whose behavior is predictable or that can be precisely specified. Because the execution environment is deterministic in nature there is a reduced likelihood that real time managed applications will be unable to maintain their exact time schedules when executed. This also reduces the likelihood that the managed applications will fail. One example embodiment of a deterministic execution environment is shown in which is described below.

In particular embodiments the execution environment could be supported in each of the controllers and the servers of the system . Also these components of the system could use processors of the POWERPC processor family running the GREEN HILLS INTEGRITY operating system or processors of the X86 processor family running a MICROSOFT WINDOWS operating system. In addition the execution environment could be implemented in any suitable manner such as by using .Net programming based on the CLI specification as ratified by ECMA 335 and support both the Kernel and Compact profiles.

The deterministic execution environment used in the system includes various features. For example the execution environment supports a deterministic memory manager which supports the use of a heap memory. The heap memory could represent a memory in one of the controllers or servers such as a random access memory or other type of memory. The memory manager in the execution environment is deterministic and predictably and automatically manages the heap memory on behalf of the managed applications being executed. As examples the memory manager could support bounded memory allocation times and a non intrusive background garbage collection process that executes incrementally using only time slices explicitly given to the garbage collection process. The garbage collection process identifies heap memory that may be reclaimed from a managed application and reused. The memory manager may also support a defragmentation process for the heap memory in which the memory manager moves blocks of used memory within the heap to combine smaller blocks of unused memory into larger blocks. An example heap memory and example operation of a memory manager are shown in which are described below.

The deterministic execution environment also supports load time compilation of assembly code which is also known as ahead of time compilation. Just in time compilation of code is performed when execution of the code is requested for the first time. This has an inherent variable execution time since it may take longer to perform the compilation in response to the first request to execute a program as opposed to subsequent requests . Load time compilation of assembly code helps to avoid this non deterministic behavior by pre compiling assembly code when the code is loaded rather than waiting for the first request to execute the code. An assembly code load process and an assembly code unload process are used in the execution environment to load assembly code for compilation and to unload compiled code in the system . Example loading compilation unloading and management of assembly code are shown in which are described below.

Beyond that a scheduler in the execution environment ensures that processing resources are shared between the managed applications being executed and housekeeping tasks. The phrase housekeeping tasks refers generally to the various processes performed in the execution environment that are needed to ensure proper execution of the managed applications or proper operation of the execution environment. Housekeeping tasks may include heap memory management such as garbage collection and defragmentation loading and unloading of assembly code and compilation of assembly code. In some embodiments the managed applications and the housekeeping tasks are executed as schedulable threads in various time slices. The housekeeping task threads may be preempted when needed in order to ensure that the managed application threads satisfy their time schedules. Moreover the housekeeping tasks are designed to operate incrementally allowing the housekeeping tasks to perform meaningful units of work in the time slices allocated to the housekeeping tasks. In other words the housekeeping tasks can perform useful operations before being preempted by the managed applications at which point the managed applications could overwrite data used by or otherwise interfere with the operation of the housekeeping tasks. An example operation of a scheduler is shown in which is described below.

In addition class libraries that may be used by the managed applications are examined or audited. Functions or libraries are examined to determine if the functions or libraries are bounded in terms of resource utilization and execution time. Managed applications are then prevented from using functions or libraries that are unbounded in terms of resource utilization or execution time since these functions or libraries are non deterministic. As an example a function may support communication with an external component over a network. If the external component is a non deterministic component or the network is unreliable the execution time of the function may be unknown. The function is therefore non deterministic and managed applications are blocked from using that function in the execution environment. The audit could be performed manually or automatically based on any suitable criteria.

By providing these various features and limits in the execution environment the execution environment is more deterministic and therefore more suitable for use with real time and other managed applications. The housekeeping tasks needed to maintain the execution environment do not interfere with the execution of the managed applications which may allow the managed applications to meet any necessary time schedules or otherwise operate effectively.

Although illustrates one example of a process control system various changes may be made to . For example a control system could include any number of process elements controllers servers and operator stations. Also illustrates one operational environment in which the execution environment described above could be used. The execution environment could be used in any other suitable device or system.

In this example embodiment the execution environment includes a global assembly cache GAC . The global assembly cache represents a memory capable of storing different assembly code programs to be executed in the execution environment . The assembly code programs could represent the managed applications to be executed in the execution environment . As an example the global assembly cache could store an assembly code program capable of controlling one or more of the process elements of . The global assembly cache could store multiple assembly code programs and or different versions of the same assembly code program. The global assembly cache represents any suitable storage and retrieval device or devices.

An assembly loader loads assembly code into the execution environment for execution. For example the assembly loader may retrieve new assembly code downloaded by a user into the global assembly cache . The assembly loader may then load the identified assembly code into a compiler for compilation and use in the execution environment . The assembly loader includes any hardware software firmware or combination thereof for loading assembly code for compilation. The assembly loader could for example represent a software thread executed in the background of the execution environment .

An ahead of time AOT compiler compiles the assembly code loaded by the assembly loader . The AOT compiler represents a load time compiler that compiles assembly code when the assembly code is loaded. For example the AOT compiler may convert assembly code from an intermediate language to native executable code capable of being executed in the execution environment . Also the AOT compiler could insert instructions into the native executable code to ensure proper execution of the code in the execution environment . The AOT compiler includes any hardware software firmware or combination thereof for compiling assembly code. The AOT compiler could for example represent a software thread executed in the background of the execution environment .

The AOT compiler produces native executable code such as native executable codes . The native executable codes represent executable code capable of being executed in the execution environment . The native executable codes could provide any suitable functionality in the execution environment such as providing control of one or more process elements of . The native executable codes could provide any other or additional functionality in the execution environment .

One or more application domains represent the domains in which one or more managed applications such as the applications implemented by the native executable codes are executed in the execution domain . Each application domain represents any suitable domain for executing one or more managed applications. While shown as a single application domain in multiple application domains could be used.

The assembly codes and native executable codes in the execution environment are managed by a code manager . For example the code manager may control the loading and unloading of assembly code in the execution environment . As a particular example the code manager could receive a command from a user or managed application instructing the execution environment to load an assembly code program. The code manager could then cause the assembly loader to load the assembly code into the AOT compiler and the AOT compiler generates native executable code that is loaded into the application domain . The code manager could also receive a command from a user or managed application instructing the execution environment to unload an assembly code program. The code manager could then unload the native executable code associated with the identified assembly code from the application domain . The process of unloading an assembly code may include reclaiming the memory associated with that assembly code and ensuring that associations between the assembly code being unloaded and the execution environment or other programs are removed. The code manager includes any hardware software firmware or combination thereof for managing assembly code and or compiled code used in the execution environment . The code manager could for example represent a software thread executed in the background of the execution environment .

The execution environment also includes a memory manager . The memory manager represents a deterministic memory manager that manages the use of a heap memory. For example the memory manager could allocate blocks of heap memory to managed applications being executed in the application domain . The memory manager could also use garbage collection information to release blocks of heap memory that are no longer being used by the managed applications. The garbage collection information could for example be generated by a garbage collection process provided by the memory manager and executed in the background of the execution environment . In addition the memory manager could support a defragmentation process for the heap memory. The defragmentation process could be used to combine unused blocks of heap memory into larger blocks. The memory manager includes any hardware software firmware or combination thereof for managing a heap memory. The memory manager could for example represent a software thread executed in the background of the execution environment .

In addition the execution environment includes an exception table which stores exception information . The exception information identifies various problems experienced in the execution environment . Example problems could include attempting to load assembly code that does not exist in an explicitly specified location or in the global assembly cache an error during compilation of loaded assembly code or attempting to unload assembly code not previously loaded. An application or process being executed in the execution environment could generate an exception identifying a detected problem. The exception is identified by the exception information which is stored in the exception table for later use such as during debugging or for use by the application or process for automatic recovery at runtime.

A scheduler is used to schedule execution of the managed applications. The scheduler may also be used to schedule execution of the housekeeping tasks in the execution environment . The housekeeping tasks include among other things heap memory management assembly loading and unloading assembly compilation management of asynchronous and event driven methods such as timers and callbacks checkpointing and serialization of data used to store data in a persistent memory such as a battery backup RAM to support the use of redundant controllers and warm restarts and miscellaneous tasks such as metrics collection. For example the scheduler could support time slicing to allow multiple threads to be executed where the threads represent the housekeeping tasks and the managed applications. The scheduler includes any hardware software firmware or combination thereof for scheduling the execution of applications and other tasks.

In some embodiments the scheduler and the execution environment cooperate and collaborate to ensure that the managed applications and the housekeeping tasks are executed properly. For example the scheduler may control when and for how long the housekeeping tasks may be executed in the execution environment . As a particular example the scheduler could preempt all threads executing the managed applications and then call the execution environment to execute one or more housekeeping tasks. The scheduler informs the execution environment of the amount of time available to perform the housekeeping tasks. The execution environment guarantees that control is returned to the scheduler on or before the expiration of that amount of time. While the execution environment is performing a housekeeping task managed applications that read or write data to a heap memory may not interrupt the housekeeping task. Other threads that do not access a heap memory such as an interrupt service routine or ISR could be allowed to interrupt a housekeeping task. Averaged over time the scheduler may provide the execution environment with enough time to perform the housekeeping tasks needed for the managed applications to execute properly. As an example the managed applications may use up to approximately 80 of the time slices available while the remaining 20 are used by the housekeeping tasks.

This type of scheduling may impose certain requirements on the managed applications. For example the managed applications should over time allow adequate processing resources to be provided to and used by the housekeeping tasks. Also a managed application should either come to a clean point or use read and write barriers before transferring control to the housekeeping tasks. A clean point generally represents a point where a sequence of related instructions being executed for the managed application has been completed rather than a point that occurs during execution of the sequence of related instructions. As an example a managed application should complete accessing data in a data structure or file when the transfer of control occurs rather than being in the middle of reading data or writing data. A read or write barrier is used when the managed application is not at a clean point when the transfer of control occurs. The read or write barrier generally represents a marker or flag used to inform the housekeeping tasks that particular data is currently being used by a managed application. This may prevent the housekeeping tasks from moving the data during defragmentation or discarding the data during garbage collection.

In some embodiments the various components shown in operate over a platform operating system abstraction layer. The platform operating system abstraction layer logically separates the execution environment from the underlying hardware platform or operating system. In this way the execution environment may be used with different hardware platforms and operating systems without requiring the execution environment to be specifically designed for a particular hardware platform or operating system.

Although illustrates one example of an execution environment various changes may be made to . For example the functional division shown in is for illustration only. Various components in could be combined or omitted and additional components could be added according to particular needs.

As noted above the memory manager may support various functions to facilitate the use of a heap memory. For example memory manager could support bounded allocation of heap memory within a predeterminable maximum time. Also the memory manager could support an incremental and interoperable garbage collection process to reclaim allocated blocks of heap memory. In addition the memory manager could support an incremental defragmentation process to consolidate smaller unused blocks of heap memory into larger blocks.

As shown in the heap memory includes a management area and a heap area . The management area stores information used to manage the heap memory . For example the management area may include information identifying used and unused blocks of memory in the heap area . The heap area stores information used by managed applications and housekeeping tasks executed in the execution environment . In some embodiments the management area and the heap area are located adjacent to one another. In particular embodiments both of the areas are provided in response to a single request when a heap is created.

The heap area typically includes used blocks of memory and unused or free blocks of memory. Used blocks of memory are allocated to one or more managed applications for use during execution. Free blocks of memory are not allocated to any applications and are available to be allocated by the memory manager . In some embodiments each block of memory in the heap area includes a value identifying the size of the block at the beginning and the end of the block. For example a free block may have its size denoted with a positive value in both the first and last positions of the block. A used block may have its size denoted with a negative value in both the first and last positions of the block.

In some embodiments a used block of memory in the heap area has one of two forms. A relocatable used block represents a block of memory that could be moved during defragmentation of the heap area . A non relocatable used block represents a block of memory that cannot be moved during defragmentation and that is fixed in the heap area . Relocatable blocks are addressed indirectly and non relocatable blocks are addressed directly. Relocatable and non relocatable blocks could be allocated using different function calls such as GetMemoryFloating and GetMemoryFixed respectively .

In some embodiments the memory manager could provide simultaneous and independent management of multiple heap memories . For example each new instance of a heap memory may be created using a function call such as InitializeHeap . Each independent heap memory may have its own associated garbage collection and defragmentation processes and each may operate independently from the others. Also one instance of a heap memory could be used by one or multiple threads in the execution environment . A heap memory could be designated as requiring or not requiring multi threading support. As an example multi threading support may not be needed if a heap memory is used by a single thread or if simultaneous heap requests from multiple threads do not occur. Otherwise if multi threading support is needed it could be provided for example using mutually exclusive MUTEX structures provided by an underlying operating system.

Each instance of a heap memory may have a unique instance identifier which may be provided when the heap memory is created. The instance identifier for a heap memory may be used in later function calls affecting that heap memory . For example application programming interfaces APIS may be provided to allow managed applications to create access and use a heap memory such as APIs used to allocate memory blocks from a heap memory . In particular embodiments the APIs may or may not require an instance identifier although providing the instance identified for a heap memory may provide enhanced performance. If a heap identifier is not provided in an API function call but a heap identifier is needed to perform a requested function the memory manager could use a memory address in the API function call to identify a heap instance or the memory manager could use the heap identifier of the most recently created heap instance. Appendix A contains a description of example APIs that may be supported by the memory manager .

As shown in an indirect addressing scheme may be used with the heap memory block . In this indirect addressing scheme a connector is used to link a pointer with an actual block in the heap memory . The pointer may represent a pointer used by a managed application. From the perspective of the managed application the pointer allows the managed application to access the heap block . However the pointer itself points to the connector and the connector then directs the pointer to a specific block in the heap memory . In a direct addressing scheme the pointer would point directly to the heap block without any intervening connector .

In some embodiments multiple connectors are used to facilitate relocation of multiple heap blocks during defragmentation. For example when a managed application invokes a GetMemoryFloating API function call the application receives a pointer to a connector which points to the actual allocated heap block . If the heap block is later moved during defragmentation the connector pointing to that heap block is adjusted to point to the new location of the heap block . The managed application itself may contain any number of references to the connector but only the connector itself needs to be adjusted when the heap block is moved during defragmentation.

The heap block shown in may represent any suitable amount of space in a heap memory . Also the heap block may include any suitable contents including size values placed at the beginning and end of the block . In addition the connector may represent any suitable pointer or other mechanism to identify a heap block .

As shown in the heap memory includes both indirect used blocks and direct used blocks . The indirect used blocks represent blocks indirectly addressed in the execution environment as with block in . Direct used blocks represent blocks directly addressed in the execution environment . For example the indirect used blocks could be identified by the connectors shown in and direct used blocks could be identified without the use of connectors. In some embodiments the indirect used blocks may be relocated during defragmentation while the direct used blocks are not. The indirect used blocks and the direct used blocks could be allocated using the GetMemoryFloating and GetMemoryFixed API function calls respectively.

The connectors such as connectors used with the indirect used blocks are stored within connector groups . In this example each of the groups contains space for 32 connectors and each of the connectors may or may not be in use pointing to an allocated heap block . Each of the groups also includes a map such as a long word bitmap identifying which connectors in that group are in use. In addition each of the groups includes one or more pointers PTRS which may be used to point to prior and subsequent groups if a prior or subsequent group exists . The groups are anchored by a group list head which points to the first group . In some embodiments the group list head is located in the management area of a heap memory .

The connector groups may be allocated from the heap area of the heap memory as needed in order to provide indirect used blocks for one or more managed applications. The groups are bidirectionally chained together to facilitate tracking and deallocation. In particular embodiments each of the groups should have at least one connector in use or a group is deallocated if all of its connectors are unused. Also the groups need not be visible to the managed applications.

As shown in the direct used blocks and the connectors in the groups are referenced by a root set . The root set includes registers stacks static memories or other memory structures capable of storing pointers such as the pointer of to blocks in the heap memory . Moreover one indirect used block could point to another indirect used block through a connector. As an example the indirect used block points to the indirect used block through the second connector in the group

In some embodiments the groups may be allocated from the heap memory as normal used blocks. Also the groups are not relocated during defragmentation. This may be indicated in any suitable manner such as by providing a flag such as a HEAP MANAGER bit in a header of each block storing one or more of the groups . In addition to help hide this unique memory referencing scheme from the managed applications the compilation performed by the AOT compiler may implement this referencing scheme automatically and invisibly.

In particular embodiments a new connector group may be allocated from the general heap memory when a connector is needed and space in the existing groups is unavailable. These connector groups may have a long or even permanent lifetime which might impact the defragmentation of the heap memory . A specified number of special connector groups may be pre allocated during initialization of the heap memory . These pre allocated connector groups may be allocated contiguously from the bottom of the new heap memory and they may not be freed to the general pool of free space even when they contain no used connectors. During operation if a connector is needed and no connectors in the pre allocated groups are free a connector group may be allocated from the general heap area and later freed if it becomes possible to do so. By pre allocating a reasonable number of special connector groups during heap initialization many or all of the connector groups can be kept out of the middle of the heap memory . This may help to improving the overall defragmentation of the heap memory .

While illustrates the use of indirect used heap blocks and direct used heap blocks additional kinds of blocks could be used or supported in the heap memory . For example transient used blocks could be created and used during the garbage collection process. Among other things these blocks could store a list of recursively discovered references to the heap memory whose memory locations have not yet been examined. At the end of the garbage collection process these blocks may be deallocated. The garbage collection process is described in more detail below.

As shown in the heap memory includes various free blocks that are arranged into multiple chains . Each of the free blocks could represent any suitable amount of contiguous unused space in the heap memory . Each of the chains could include any number of free blocks including zero free blocks . The chains are anchored by multiple anchors . The anchors represent pointers that point to the first free block and the last free block in the chains . In some embodiments the anchors are stored in the management area of the heap memory and the free blocks reside in the heap area of the heap memory .

In some embodiments the anchors are associated with size indicators respectively. Also the anchors may be sorted based on the size indicators . Each of the size indicators identifies the smallest free block in one of the chains . In other words the free blocks in a chain are all at least as big as the size indicator for that chain. Moreover the free blocks in one chain are all smaller than the size indicator for the next higher chain. In addition the free blocks within a chain could be sorted or semi sorted in order of increasing block size.

To allocate a free block to a managed application a scan is performed to identify an anchor having a size indicator that equals or is greater than the amount of memory needed by the application. In some embodiments a rapid and efficient binary search may be used so that the search time is deterministic. When a suitable anchor is identified a check is performed to determine if the anchor is associated with a chain having any free blocks. If the chain associated with the identified anchor is empty the search continues for another suitable anchor. Otherwise the chain associated with the identified anchor is not empty and a free block in that chain is allocated to the managed application. In particular embodiments two modes of allocation are supported one for absolute determinism and another for best effort which is almost deterministic and may be a better choice for some applications . Example code used to support these modes of allocation is shown in Appendix B.

Turning to as noted above the memory manager is responsible for managing the heap memory and providing access to the heap memory . Also the memory manager could support one or more APIs to allow external elements such as the managed applications to invoke functions of the memory manager such as an allocation of memory . The following represents several examples of functions performed by the memory manager . The memory manager could support any other or additional functionality and APIs or other interfaces without departing from the scope of this disclosure.

The memory manager initializes a heap memory at step . For example the memory manager may first initialize a management area . After that a malloc command is executed to obtain an amount of memory requested by a managed application. The obtained memory represents the heap area of the new heap. In some embodiments the heap area is somewhat larger than the amount of memory requested by the managed application. In these embodiments additional memory is requested to provide buffering both before and after the actual heap. Information about where the heap area is located may be stored in the management area . If the managed application requests that one or more connector groups be pre allocated the memory manager may create the connector groups in the new heap area .

The memory manager allocates one or more blocks in the new heap memory at step . To allocate blocks within an initialized heap to an application the anchors and associated chains are scanned to identify the first chain containing a free block that could satisfy the managed application s requirements such as the requested block size . Optionally additional free blocks in the same chain could be examined to determine if they more accurately meet the application s requirements such as more closely matching the requested block size . This could provide statistically better performance while retaining the deterministic character of the memory manager . For example three additional free blocks could be examined to determine if any of those free blocks more closely match the block size needed by the application. One of the located free blocks is then allocated for use by the application. In particular embodiments a balanced binary tree is used to enable a fast search of the anchors 

When allocating a free block to a managed application extra memory beyond the application s needs could be allocated. For example the free block anchors may be scanned to locate a free block as described above. The selected free block could match the application s needs perfectly the requested size matches the actual block size could be slightly too large or could be much too large. If the free block is a perfect fit the entire free block is allocated to the application. If the free block is much too large the free block is split into one used block that is allocated to the application and one free block that is placed in the appropriate chain such as one of the chains .

If the free block is slightly too large a determination is made as to whether the free block is split or completely allocated to the application. For example a determination could be made as to whether the free block exceeds the application s requested size by more than a specified amount such as ten long words . If so the free block is split as described above. Otherwise the entire free block is allocated to the application and the actual size of the block and the amount of over allocation are stored in the block . By storing the amount of over allocation in the block the extra allocated space may be reclaimed later during defragmentation.

Eventually the blocks allocated to a managed application may be deallocated. This could occur in response to an express deallocation request from the application in response to the garbage collection process or in response to any other suitable event. The memory manager deallocates one or more blocks in the heap memory at step . To deallocate used blocks within an initialized heap the memory manager receives information identifying a used block. If the memory manager maintains statistics about usage of the heap memory the memory manager could update the statistics. The memory manager also releases any connector pointing to the used block being deallocated. In addition the memory manager coalesces the used block being deallocated with any adjacent free block s . In general coalescing is a process where adjacent free blocks are merged into a single larger free block. The memory manager could examine the blocks adjacent to the used block being deallocated. If all neighboring blocks are used the block being deallocated is freed and placed into the appropriate chain one of the chains . If one or more neighboring blocks are free each free neighboring block is removed from its chain the blocks are combined into a single block and the combined block is placed in the appropriate chain.

The garbage collection process implemented by the memory manager is responsible for locating allocated blocks of heap memory that are no longer being used by the applications that requested them. In some embodiments the memory manager uses a mark sweep garbage collection process. In this process the memory manager unmarks or clears some or all of the blocks in a heap memory at step . Each block in the heap memory may include a header where various bits or flags may be set. The memory manager could set the appropriate bit or flag in each block to an unmarked value. Some blocks such as connector groups are automatically marked as being in use and these block need not be unmarked by the memory manager .

The memory manager then scans the root set for references to the heap memory at step . The root set could include registers stacks and static memory used by applications in the execution environment . The root set is examined to locate any pointers that may point to an in use heap block. Those pointers may point directly to heap blocks direct used heap blocks or indirectly to heap blocks indirect used heap blocks .

The memory manager follows the pointers and marks the direct and indirect used heap blocks at step . For example the memory manager may set the bit or flag in each direct and indirect used heap block to indicate that the block is currently in use. As shown in locating indirect used heap blocks may involve following a pointer to one connector which points to one indirect used heap block. That indirect used heap block may itself point to another connector which points to another indirect used heap block.

The memory manager then sweeps through the heap memory to free all allocated and unmarked blocks at step . The allocated and unmarked blocks represent used blocks that have been allocated but the blocks no longer appear to be used by the applications that requested them. These blocks are then freed using for example the deallocation technique described above with respect to step in . Again special blocks such as the connector groups may be automatically marked as being in use and are not swept. In this document each full execution of the mark sweep algorithm may be referred to as a garbage collection cycle. 

To make the garbage collection process deterministic the garbage collection process is implemented as an incremental interruptible and interoperable process. For example the garbage collection process may be executed in incremental steps allowing the garbage collection process to operate in time slices not used by the managed applications being executed in the execution environment . Also the garbage collection process is interoperable with the managed applications meaning that the garbage collection process and the managed applications may be executed in interleaved time slices.

One aspect of interoperability concerns a used block of heap memory being marked and swept when a managed application is in the process of reading data from or writing data to that block. This may occur when the managed application s time slice ends as a read or write operation is being performed and the garbage collection process time slice begins. In some embodiments read and write barriers are used to help make the garbage collection process interoperable with the managed applications. The read or write barrier informs the garbage collection process that a particular block of heap memory is currently being used preventing the garbage collection process from reclaiming the block of memory.

Instead of or in addition to using read and write barriers the mark sweep process could be modified into a mark mark again sweep process. In this process a block that is unmarked after a pass through the heap memory is not immediately swept. Rather the block is swept only if it remains unmarked after two consecutive passes through of the heap memory

The various phases of the garbage collection process scan the root set mark blocks sweep blocks could each occur incrementally in multiple time slices. However scanning the root set may not need to be implemented incrementally since the root set is bounded in size number of registers size of static memory and worst case size of a fully allocated heap . Also in particular embodiments the sweep phase could be incremental and interoperable only if object finalizers are not executed when objects are deallocated. A finalizer represents a method used by an object to free resources and perform other cleanup operations before the memory occupied by the object is reclaimed.

During the garbage collection process the memory manager may encounter internal pointers used by applications being executed in the execution environment . Internal pointers are pointers that address locations within a block of heap memory other than the beginning of the block. The internal pointers create an issue with garbage collection because they can drive a severe design constraint. While a pointer to the beginning of a used block of heap memory is easier to identify a pointer to an arbitrary place in the middle of an unknown size structure can be difficult to characterize. In some conventional garbage collection processes only pointers to the beginning of a heap block protected that heap block from reclamation. Other conventional garbage collection processes grouped blocks of allocatable memory into size clusters frequently pages of memory so that it was possible to compute the starting address of the block.

In some embodiments of this disclosure each block in the heap memory includes a size at the beginning and end of the block. This makes the heap memory walkable or easily traversable from any known beginning of a block. In these embodiments the start of a block for an arbitrary internal pointer can be located by starting at one end of the heap memory and walking through the heap memory until the block that encompasses the internal pointer is located.

In other embodiments a portion of the management area contains pointers into the heap area . For example 40 000 bytes in the management area could contain 10 000 pointers into the heap area . These pointers referred to as management pointers are approximately evenly spaced point to the beginnings of various blocks in the heap area and are continuously maintained as new heap operations allocate and deallocate heap blocks. When it is time to locate the beginning of a block that contains an address referenced by an internal pointer the following formulas may be used StartingIndex AddressToFind HeapLowAddress TenThousanthHeapSize 1 StartingAddress HeapBlockAddresses StartingIndex 2 where HeapBlockAddresses represents an array of 10 000 management pointers into the heap area StartingIndex represents an index into the array of management pointers AddressToFind represents the address referenced by the internal pointer HeapLowAddress represents the lowest memory address of the heap area TenThousanthHeapSize represents 1 10 000th the size of the heap area and StartingAddress represents the beginning of a block that either contains the address referenced by the internal pointer or is near the address referenced by the internal pointer. From StartingAddress it is possible to use the sizes contained in the heap memory blocks to walk in either direction in the heap memory to locate the beginning of the block that contains the address referenced by the internal pointer.

As mentioned above scanning the root set may not be implemented in an incremental fashion in the execution environment . However it may need to be performed incrementally such as when many threads are running in the execution environment since each thread has a stack that is scanned . In some embodiments scanning the root set is initiated by a TellHmRootSet API function call executed immediately after a heap memory is initialized. TellHmRootSet passes a function vector identifying multiple functions to the memory manager which records the vector s contents for later. When a garbage collection cycle begins the root set is obtained by executing from within the memory manager each of the functions in the saved vector.

In particular embodiments six functions are passed through the vector corresponding to six dynamic link libraries DLLs built as part of the XIMIAN NOVELL MONO CLI. Each of the DLLs has a function named GetRootSet and addresses of these functions are passed via the vector. When the root set is needed at the beginning of a garbage collection cycle these six functions or however many functions are identified in the vector are executed. Each DLL is the linked combination of many files and the GetRootSet function in turn calls a GetRootSet function for each file included in the DLL. Each of these GetRootSet functions makes zero or more calls to an AddReferenceExternal function of the memory manager passing in the value of a pointer that exists within that code and that refers to a block that is reachable via that pointer. If every file tells the memory manager about all pointer variables that it currently holds the memory manager is informed about all blocks that are directly reachable and which therefore constitute the root set. 

In addition to the GetRootSet functions an additional function called StacksAndRegisters is used to inform the memory manager about the stack and register contents of all threads. Since threads may come and go this function may operate using a dynamically maintained thread table. After the root set is fully identified the marking phase begins as described above.

While the execution environment has been described up until now as using a single type of heap memory multiple types of heap memories could be used in the execution environment . For example the memory manager could segregate a single heap memory into a short term heap and a long term heap or the memory manager could support the creation of separate short term and long term heaps. The short term heap could provide support for shorter duration and higher rate usage of memory. The long term heap could provide support for longer duration or permanent usage of memory. A third type of heap could represent a conventional heap which is accessed by unmanaged applications in a conventional manner such as malloc and free commands .

To support the use of short term and long term heaps the memory manager could support a redirection mechanism to direct requests for heap memory to either the short term heap or the long term heap. For example the GetMemoryFloating and GetMemoryFixed API function calls could be used to request a block of heap memory. The redirection mechanism determines whether the block of memory is provided from the short term heap or the long term heap. In some embodiments a PushHeapSelection STH LTH function is used to place either an STH or LTH indicator onto a special selection stack and a PopHeapSelection function is used to remove the indicator from the special selection stack. If the STH indicator such as a value of 0 is at the top of the special selection stack when the GetMemoryFloating or GetMemoryFixed API function call is made a block of memory is allocated from the short term heap. If the LTH indicator such as a value of 1 is at the top of the special selection stack when the GetMemoryFloating or GetMemoryFixed API function call is made a block of memory is allocated from the long term heap. Alternatively one of these two values such as LTH could be used as a default and only the other value such as STH needs to be pushed and popped from the special selection stack. In this way a managed application can control which heap is used for a particular memory allocation request. Also the short term heap may be quickly and repeatedly abandoned and reinitialized when it is no longer in use without affecting the long term heap.

In some embodiments the short term heap and the long term heap are managed in different ways. For example different garbage collection processes could be used with the short term heap and the long term heap. As a particular example the long term heap could be managed as shown in and the short term heap could be managed as shown in .

Conventional garbage collection processes typically operate continuously over a longer period of time. This would interfere with the execution of managed applications in the execution environment and represents a non deterministic way to perform garbage collection. As shown in the garbage collection process in the execution environment is broken up into various portions which are invoked and executed periodically rather than all at once. illustrates that the portions of the garbage collection process are executed at a regular interval. illustrates that the portions of the garbage collection process may or may not be executed but if executed the portions are invoked at a regular interval. In a portion of the garbage collection process may not be invoked at the regular interval for example if the free space in the heap memory exceeds a specified percentage such as 30 .

Finalizers may or may not be allowed in the execution environment . If allowed the solid time slice is used to allow objects being reclaimed to invoke any finalizers. Also rules may be established to ensure that users create finalizers capable of being executed within a specified amount of time such as 0.5 ms . This helps to ensure that the execution of the finalizers is deterministic. In other embodiments the use of finalizers in the execution environment is not allowed.

In addition the square hatched time slices are used by the garbage collection process to implement the sweep phase where allocated and unmarked heap blocks are reclaimed. While not shown in one or more time slices preceding time slice could be used to unmark the blocks of the heap memory or the unmarking could occur after the time slice and before the first time slice .

By implementing the garbage collection process in this manner the garbage collection process may be executed successfully in the background of the execution environment . Also the garbage collection process may be executed without interfering with the managed applications in the execution environment .

In some embodiments the garbage collection process requires the use of some space in the heap memory being cleaned. For example the garbage collection process may need to store the addresses of heap blocks such as when the garbage collection process recursively examines all accessible heap blocks for references to other accessible heap blocks. As a result the garbage collection process may be unable to execute if all blocks in the heap memory are allocated to other applications or processes. To avoid this problem the garbage collection process could be invoked when a specified amount of the heap memory is allocated such as 70 of the heap memory . This may help to avoid attempting to perform the garbage collection process when too little heap memory is free for use by the garbage collection process.

In particular embodiments the reclamation of heap memory blocks is hidden from the managed applications in the execution environment . However a managed application might wish to be made aware when the reclamation of particular memory blocks occurs. This may be supported with a callback mechanism in which a callback routine is associated with an individual block of heap memory. When that block of memory is freed either implicitly or explicitly the callback routine is called with the address of the memory block being freed. Information from the callback routine may be used to update data structures used by the managed application. The API function call SetCallback may be used to establish a callback routine for a particular block of heap memory whether the block is indirectly or directly addressed.

As shown in execution in the execution environment is divided into multiple cycles each of which includes multiple time slices. The time slices in each cycle are used by different functional blocks the managed applications except for one idle period where housekeeping tasks such as heap management are performed.

In the example shown in abandonment and reinitialization of the short term heap occur at the end of each cycle . The circles shown in illustrate when the abandonment and reinitialization of the short term heap occur using this technique.

As shown in the short term heap could also be abandoned and reinitialized at the end of the execution of each functional block. The circles shown in illustrate when the abandonment and reinitialization of the short term heap occur using this technique.

In addition as shown in the short term heap for a stack frame could be abandoned and reinitialized at the end of the execution of the method for which the stack frame was created. The circles shown in illustrate when the abandonment and reinitialization of the short term heap occur using this technique. In the short term heap could be abandoned and reinitialized once at the end of the execution of a functional block or multiple times during execution of a functional block.

The various techniques shown in and A through C may be used by the memory manager to manage a short term heap and a long term heap in the execution environment . However the techniques shown in and A through C are for illustration only. The memory manager could use any other or additional techniques to manage a heap memory. As a particular example the memory manager could use the technique shown in to manage both the short term heap and the long term heap.

The defragmentation process performed by the memory manager may or may not require time slicing in the way that the garbage collection process may require. The defragmentation process could involve as little as locating and filling one free block in the heap memory with one or more used blocks. In some embodiments the defragmentation process is invoked and instructed to perform as many defragmentation operations in a given fixed amount of time. During that time the defragmentation process need not inter operate with applications being executed in the execution environment . This may reduce the need to use read and write barriers which protect managed applications from wrongly accessing used blocks of memory that have been relocated during the defragmentation process.

In some embodiments the defragmentation process involves moving relocatable used blocks such as indirect used blocks to fill in small free blocks of heap memory such as free blocks and create larger free blocks. The used blocks that are to be moved during the defragmentation process may be selected by one or more heuristics.

The heuristics may contain a number of parameters including the number of free blocks neighboring a used block. Each used block may have zero one or two free blocks surrounding it. In particular embodiments used blocks with two free neighbors are relocated because doing so allows the space occupied by the used block to be combined with both neighboring free blocks. This leads to the creation of one bigger block formed by coalescing all three blocks. Also used blocks with one free neighbor could be relocated. Used blocks with no free neighbors may not be relocated during the defragmentation process.

Block size is also taken into consideration during the defragmentation process. Ideally a free block in the heap memory is filled with one or more used blocks that fit perfectly into the free block. If that is not possible the free block may be filled with one or more used blocks that almost fit into the free block. In addition the age of the used blocks is considered before relocating the used blocks during the defragmentation process. In general recently allocated blocks tend to have shorter lifetimes than non recently allocated blocks. As a result the memory manager may not relocate recently allocated blocks during the defragmentation process because these memory blocks are new and have a statistical likelihood of being deallocated sooner.

The memory manager selects a free hole a free block in the heap memory to be filled at step . For example the free block may be selected from one of the chains where the chosen chain contains the smallest free blocks . In particular embodiments the free block selected is subject to the limitation that it cannot be more than four times the size of the largest used block capable of being relocated.

The memory manager searches for used blocks such as indirect used blocks that are potential candidates for filling the selected free hole at step . The search may be limited to used blocks that are not larger than the selected free block . The search may also be limited to used blocks with at least one free neighbor. In addition the search may be confined to older allocated blocks. Blocks marked with the HEAP MANAGER bit described above or a DO NOT RELOCATE bit in their headers are not considered during the defragmentation process as are blocks neighboring the selected free block . In particular embodiments the search continues until 15 potential candidates are located satisfying these criteria or until a certain amount of time has elapsed during the search.

The memory manager determines if the search terminated upon an exact match at step . The search performed at step may end immediately if a used block is found that is a perfect fit for the selected free block and the used block has two free neighbors. If an exact match is found the memory manager fills the free hole with the matching used block at step .

If no perfect fit is found during the search the memory manager analyzes the fifteen candidates found during the search to identify statistics of the candidates at step . For each candidate used block the memory manager could identify the location of the used block the age of the used block the number of free neighbors of the used block the total size of any free neighbors of the used block and any over allocation contained in the used block.

The memory manager then scores each individual candidate at step . An example algorithm for scoring a candidate or combination of candidates is provided below. In this algorithm an exact match returns a higher score. Non exact matches are scored based on their age in terms of garbage collection cycles their number of free neighbors and their sizes compared to the size of the selected free block .

These scores are compared to a threshold at step . If any score exceeds the threshold the individual candidate associated with that score is used to fill in the free hole at step .

Otherwise the memory manager begins considering combinations of candidates to fill the free hole. In some embodiments the memory manager could use two three or four candidates to fill the free hole. The memory manager determines at step if any additional combinations of candidates remain to be examined. If so the memory manager scores certain combinations of candidates at step . At that point steps are repeated to determine if any combination of candidates can be used to fill the free hole in the heap memory .

As a particular example of how steps may be implemented if no individual candidate has a score that exceeds the threshold at step the memory manager may score all combinations of two candidates at step . If no combination of two candidates has a score that exceeds the threshold at step the memory manager may score all combinations of three candidates at step . If no combination of three candidates has a score that exceeds the threshold at step the memory manager may score all combinations of four candidates at step .

If no combination of candidates exceeds the threshold the memory manager determines if any non zero score exists for any individual candidate or combination of candidates at step . If so the memory manager picks the individual candidate or combination of candidates with the highest score and that individual candidate or combination of candidates is used to fill the free hole at step . Otherwise all scores equal zero and the memory manager was unable to fill the free hole in the heap memory . At this point the method may end with an exception indicating a failure of the defragmentation process.

As noted earlier some blocks requested by applications may contain an over allocation of space meaning the block contains more space than requested by an application. A similar mechanism could be used during defragmentation. When the defragmentation process is relocating one or more used blocks to fill a free hole the defragmentation process may sometimes find it advantageous to extend the size of a relocated block so that it entirely fills the free hole. This may occur for example when filling the free hole with an unextended used block would create a residual fragment of free space that is too small to tolerate. As with over allocations that occur during allocation of a block to an application over allocations that occur during defragmentation may be recoverable during later defragmenting or if the block is ever deallocated. Also if a block containing over allocated space could be moved during defragmentation but cannot fit into a free hole in its entirety the actual size of the block without the over allocated space could be examined to determine if the actual size fits into the free space. If by eliminating the over allocation the block fits into the free space this provides the dual benefits of filling the free hole and recovering the over allocated space.

The memory manager may use any suitable scoring mechanism to score individual candidates and combination of candidates. The algorithm shown below could be used to score the candidate s . A score of zero is returned for a situation that is not acceptable. Some of the factors considered in the algorithm TotalLength TotalAge and TotalFreeNeighbor Size may be aggregates for between one and four used blocks.

The heuristics used in the defragmentation process use a number of constants to define or control how the defragmentation process occurs. These constants may be defined in software or represent initialization time configuration values. The following describes these various constants and provides example values for the constants.

Various weights may also be used during the scoring of the used block candidates. These weights represent the sums for all individual or combination of candidates under consideration.

In some embodiments the garbage collection process and the defragmentation process are used in an alternating fashion to manage the heap memory . For example small amounts of defragmentation may be alternated with occasional episodes of garbage collection. This allows the defragmentation and garbage collection processes to solve two different problems both of which may contribute to the inability of a managed application to obtain memory when needed. Garbage collection frees used blocks that are no longer actually in use while defragmentation removes small and typically bothersome free blocks and creates larger and more valuable free blocks.

Although illustrate an example heap memory and how a memory manager may manage the heap memory in a deterministic execution environment various changes may be made to . For example while illustrate a particular heap memory and how the heap memory may be used any other heap memory may be used in any suitable manner in the execution environment . Also while illustrate particular mechanisms for managing a heap memory any other or additional mechanisms could be used to manage the heap memory in the execution environment .

New assembly code is downloaded into a file system of an execution environment at step . This may include for example a user downloading the new assembly code into the global assembly cache of the execution environment . The new assembly code could represent a new program or a different version of an existing program already loaded into the execution environment .

A load of the new assembly code is invoked at step . This may include for example the user providing a command to the execution environment requesting that the new assembly code be loaded into the execution environment . The command may be received by the code manager which then causes the assembly loader to attempt to load the new assembly code into the AOT compiler .

A determination is made as to whether the new assembly code associated with the load request is actually stored in the file system at step . This may include for example the assembly loader examining the global assembly cache and determining if the requested assembly code is present in the global assembly cache . If the assembly code is missing an exception is generated at step and the method ends.

Otherwise the assembly code is present in the global assembly cache and appropriate runtime data structures are loaded with the new assembly code at step . This may include for example the assembly loader loading the new assembly code into data structures of the AOT compiler . The data structures could represent any suitable data structures arranged to hold assembly code to be compiled in the execution environment .

The assembly code is then pre compiled at step . This may include for example the AOT compiler compiling the assembly code into native executable code capable of being executed in the execution environment . The AOT compiler may perform the compilation in the background of the execution environment . For example the AOT compiler could compile the assembly code in time slices not used by the managed applications being executed in the execution environment . Once the new assembly code is compiled into native executable code the native executable code may be executed within the application domain of the execution environment . Also a hash table or other structure may be updated to reflect that the compiled assembly code is now available for use in the execution environment . The compiled assembly code could itself be stored in one or more locations in one or more hash tables or other structures.

An unload of assembly code is invoked at step . This may include for example a user providing a command to the execution environment requesting that specific assembly code be unloaded from the execution environment . The command may be received by the code manager which then controls the unloading of the assembly code.

A determination is made as to whether the execution environment currently operating is capable of unloading assembly code during operation at step . This may include for example determining whether the execution environment is currently operating. As a particular example this may include determining whether a Net runtime environment is currently operating. If not an exception is generated at step and the method ends.

If so a determination is made as to whether any other problems are detected at step . This may include for example the code manager determining if the received unload command contained appropriate arguments such as the name of the assembly code to be unloaded. This may also include the code manager determining if the identified assembly code has been loaded into an application domain . This may further include the code manager determining if the identified assembly code is domain neutral meaning it can be invoked in multiple application domains . Beyond that this may include the code manager determining whether the identified assembly code represents a core assembly or assembly code supporting a core or important function in the execution environment . In addition this may include the code manager determining if the identified assembly code has any active instances whether the assembly code is currently being executed .

If the received unload command contains improper arguments the identified assembly code has not been loaded into an application domain or the identified assembly code is domain neutral an exception is generated at step and the method ends. Similarly if the identified assembly code represents a core assembly or if the identified assembly code has at least one active instance an exception is generated at step and the method ends.

Otherwise the assembly code is unloaded during steps . The code manager removes references to the compiled assembly code from the appropriate hash table at step . The code manager closes any file handles identifying the compiled assembly code being unloaded at step . The file handles are used in the execution environment to track different files that are in use at the same time. The file handles closed by the code manager may represent file handles used during execution of the compiled assembly code.

In addition the code manager further cleans runtime data structures at step . The runtime data structures could for example represent data structures used to track which assembly codes have been loaded into working memory. When a compiled assembly code is removed from the hash table at step the runtime data structures may be used to identify where different portions of the assembly code are located. At this point the assembly code has been unloaded and the method ends.

In some embodiments the memory manager determines whether assembly code to be unloaded has any active instances. The memory manager then informs the code manager whether particular assembly code has any active instances allowing the code manager to decide whether the assembly code can be unloaded. In particular embodiments the memory manager lacks object manager functionality and therefore does not manage or track the creation and expiration of objects and instances. In these embodiments the memory manager could use the callback mechanism described above to track instances of assembly code being executed.

As shown in for each assembly code that is loaded the execution environment such as the code manager maintains a structure containing information about the classes supported by that assembly code. A parallel phantom structure is also created and the phantom structure contains a pointer to the original structure . If a series of instances of the assembly code are created each of the instances includes a pointer to the phantom structure .

A pointer in the original structure is initialized to NULL when the assembly code is first loaded. When one or more active instances of the assembly code are in use the pointer in the structure points to the phantom structure . When no active instances of the assembly code are in use the pointer in the structure is again set to NULL. However the pointer in the original structure represents a non mark traversing pointer. This means the pointer does not constitute an active traceable reference to the phantom structure so it is not traversed during the marking phase of the garbage collection process.

A callback routine is defined in the phantom structure and the callback routine is executed each time the phantom structure is deleted by the memory manager . The phantom structure may be deleted for example when the last instance such as instance of the assembly code is deleted. In the callback routine the pointer in the phantom structure is used to access the original structure and set the pointer in the original structure to NULL. In this way the pointer in the original structure may be examined to determine if the pointer points to a phantom structure or has a NULL value. If the pointer in the original structure has a NULL value there are no instances of the assembly code currently in use and the assembly code may be safely unloaded.

In other embodiments the original structure could use a counter that is incremented each time a new instance of the assembly code is created. A callback routine could then be used to decrement the counter each time an instance of the assembly code is deleted. In this way the value of the counter may be examined to determine if there are any active instances of the assembly code. The pointer in the original structure shown in could still be used when the original structure supports the counter in these embodiments.

Although illustrate how assembly code may be managed and used in a deterministic execution environment various changes may be made to . For example while illustrates one possible mechanism for tracking active instances of assembly code other techniques could be used in the execution environment to track active instances of assembly code.

In general both the housekeeping tasks and the managed applications are scheduled for execution in the execution environment . The managed applications being executed could be cyclic in nature or be aperiodic such as event driven or one shot applications . Cyclic applications are categorized by the scheduler into groups based on the amount of execution time needed by the applications. In this example the groups include applications needing 5 20 ms higher priority 100 1000 ms medium priority or more than 10 seconds lower priority . Event driven applications run on their own threads are triggered by external events such as network or user commands are executed at a lower priority and go dormant when execution is complete. One shot applications are similar to event driven applications in behavior and priority but the one shot applications terminate when their execution is complete.

As shown in the scheduler schedules the execution of all categories of applications and housekeeping tasks. In this example the unshaded time slices represent time slices used by the housekeeping tasks in the execution environment . The remaining time slices are used by managed application either cyclic or aperiodic in the execution environment . The time slices are arranged in order of decreasing priority in with the time slices having the highest priorities and the time slices having the lowest priorities.

In some embodiments the amount of time spent executing a housekeeping task in a time slice is configurable such as by being set to 20 of the smallest possible time slice . Also the housekeeping task being executed in a time slice may be guaranteed to reach a clean point on or before the expiration of the time slice . A managed application executing in one of the time slices may or may not reach a clean point before being preempted by the housekeeping task in a time slice . As noted above read and write barriers may be used to prevent a housekeeping task from moving or discarding a block of heap memory being used by a managed application. The establishment of the read and write barriers could be handled automatically in the execution environment such as when the AOT compiler inserts the commands 

The various applications and housekeeping tasks executed in the time slices have associated priorities. Table 1 illustrates how the priorities used in the execution environment EE map to priorities in the WINDOWS and INTEGRITY operating systems.

In some embodiments the Real Time priority in the WINDOWS operating system and priorities in the INTEGRITY operating system are higher than any priorities of the managed applications or housekeeping tasks in the execution environment . These priorities may be used by the operating system for other services and tasks.

When the execution environment is first invoked it is launched on a high priority thread the Highest priority . This thread executes initialization code for the execution environment . Once initialization is complete this thread creates a thread for the scheduler at the same priority level Highest and then goes into a Wait State . This thread remains in the wait state until the scheduler thread exits. At that point the thread terminates which represents a shutdown of the execution environment .

The scheduler thread may create other managed threads at any priority level it desires. Threads with equal priority levels are scheduled in a round robin fashion. The housekeeping tasks are called by the scheduler thread periodically while all other managed threads are preempted. Table 2 gives an example of a threading model that could be supported by the scheduler and used in the execution environment .

As shown here both managed code such as C and Visual Basic and unmanaged code such as C and C may coexist in the execution environment and share a common memory resource like a heap memory. For example some unmanaged code may run at a very high priority level such as ISRs and time critical time based or event based responders . Also unmanaged code may not interact with data structures including managed heap memory that is owned and managed by managed code. This is because managed code can be preempted by unmanaged code at times when the data structures are inconsistent or otherwise not ready for immediate use. In addition both terminating and un terminating code may exist in the execution environment keeping in mind that all code terminates during a shutdown .

Although illustrates one example of the timing of program execution in an execution environment various changes may be made to . For example the scheduler could classify the managed applications into any suitable number of categories. Also the example timing shown in is for illustration and explanation only. The scheduler could schedule threads for execution in any other suitable manner.

In some embodiments the various functions performed within or in conjunction with the execution environment are implemented or supported by a computer program that is formed from computer readable program code and that is embodied in a computer readable medium. The phrase computer readable program code includes any type of computer code including source code object code and executable code. The phrase computer readable medium includes any type of medium capable of being accessed by a computer such as read only memory ROM random access memory RAM a hard disk drive a compact disc CD a digital video disc DVD or any other type of memory.

It may be advantageous to set forth definitions of certain words and phrases used throughout this patent document. The terms include and comprise as well as derivatives thereof mean inclusion without limitation. The term or is inclusive meaning and or. The phrases associated with and associated therewith as well as derivatives thereof may mean to include be included within interconnect with contain be contained within connect to or with couple to or with be communicable with cooperate with interleave juxtapose be proximate to be bound to or with have have a property of or the like. The term controller means any device system or part thereof that controls at least one operation. A controller may be implemented in hardware firmware software or some combination of at least two of the same. The functionality associated with any particular controller may be centralized or distributed whether locally or remotely.

While this disclosure has described certain embodiments and generally associated methods alterations and permutations of these embodiments and methods will be apparent to those skilled in the art. Accordingly the above description of example embodiments does not define or constrain this disclosure. Other changes substitutions and alterations are also possible without departing from the spirit and scope of this disclosure as defined by the following claims.

Create a heap for the memory manager to manage. It is to be created with a size of SizeInLongwords. If present FixedStartingLocation gives the address of a specific location in memory that is to be managed as a heap. If omitted general memory is obtained for the heap via a call to malloc. If memory cannot be allocated the process will terminate with an exit code of one. Other notifications of unavailable memory could be used.

The function returns a HeapIdentifierType which identifies the instance of the heap that was created. The identifier returned is used in many heap manager functions described below in which the heap to be used is specified.

A file called FileNameForDebugInfoOrNULL is opened for debug output. If NULL is provided for this parameter debug output which can be very voluminous depending on user selections is directed to a console screen.

Parameter CoatHeapInitially causes the empty heap to be precoated with a recognizable pattern which can be helpful in debugging. Subsequent coating of memory is governed by DEBUG COAT MEMORY which can be set in function SetDebugInfo. If heap coating is not done during initialization turning it on later may not enable the ValidateHeapStructure module to reliably check for corruption in free blocks which if coating had always been on can be tested for the memory coating pattern .

CountOfConnBlockGroupsToPreconfigure specifies the number of connector groups to preconfigure. These preallocated connector groups may have two special characteristics. First they are allocated contiguously from the bottom of the new heap where they do not create a potential fragmentation problem. Second they are not freed to the general pool of free space unlike later created connector groups even when they contain no used connectors. Instead they are retained for future use.

ThreadSafeExecutionWanted is set to true when thread safe operation is desired and to false when thread safe operation can be omitted. This is done on a heap instance basis. Omitting thread safe operation may give slightly improved performance. An example where this might be beneficial is in a RAM retention restart RRR heap which may have a single user and not require thread safe operation.

AllocatorTrackingWanted is set to true if it is desired that every block of memory allocated by the memory manager be tagged with a thread identifier of the thread that allocates the block. This information may be useful in debugging as it is displayed in the output of DumpHeap. However slightly better performance may result when this option is not enabled.

InitializeHeap may be called once for a heap instance that is to be created before any other functions are executed by memory manager on that heap instance.

Returns the HeapIdentifier that is the current default for calls to the memory manager in which the optional Heap Identifier is not specified.

Obtain an indirect pointer to a block of memory that is at least SizeInBytes long. If memory cannot be provided return NULL.

The compiler function sizeof may be used to obtain the size of the block that an application is trying to obtain.

The pointer returned is an indirect pointer. This supports the memory manager s underlying ability to move blocks that are in use with minimal impact to applications being executed.

Obtain a direct pointer to a block of memory that is at least SizeInBytes long. If memory cannot be provided return NULL.

The compiler function sizeof may be used to obtain the size of the block that an application is trying to obtain.

The pointer returned is a direct pointer like a malloc pointer . Blocks allocated in this way may not be relocated by the memory manager during defragmentation.

Allocate a time slice of execution to a housekeeping task. This time may be used for garbage collection defragmentation assembly load and unload compilation operations or other functions.

Explicitly returns a block of used memory to the heap. The returned block may then be coalesced with any adjacent free block s . Alternatively garbage collection may be used to free blocks of memory. This function is optional and may provide efficiency benefits over relying on the garbage collection process.

The specification of HeapIdentifer is optional. Execution may be slightly faster a search is avoided if the HeapIdentifier is provided.

This function associates a user defined callback routine with a heap block. When the heap block is deleted either implicitly by garbage collection or explicitly by a call to FreeMemory the associated function CallbackFunction is called. This enables the application to track or otherwise monitor invisible memory reclamations.

A distinct function can be specified for individual blocks of memory. A particular user function may be used to monitor multiple blocks of memory.

This function performs a complete validation of the entire heap structure. If any errors are found the errors are written to a debug output see InitializeHeap for how to set the name of the debug output file along with a full heap dump see DumpHeap below . The calling process then terminates with an exit code of one.

ValidateHeapStructure can be very time consuming. It may be used for debugging. Its use may not be desirable during normal operation.

This function generates both a logical and a physical dump of the entire heap on the debug output file see InitializeHeap for how to set the name of the debug output file .

The parameter AllDetails controls whether or not a logical dump is produced. If AllDetails is TRUE a logical dump is produced. If AllDetails is FALSE a logical dump is not produced. A physical dump could always be produced. A logical dump could be very long.

A physical dump shows only the location and size of all free and used blocks. It may not detail the use of these blocks in terms of the memory manager tracking structures.

This function generates a visual representation of the entire heap on the debug output file see InitializeHeap for how to set the name of the debug output file .

The parameter PageWidth specifies how many cells across the page are to be displayed. Typical values may be 80 or 100.

The parameter LongwordsPerCell specifies how many longwords of heap memory are represented by each cell of the output. Selecting a small number of LongwordsPerCell for a very large heap can result in a very long output.

This function allows the user to add additional information to the stream of debug output see InitializeHeap for how to set the name of the debug output file .

The parameter ImmediateFlushWanted slows execution but allows for getting the most output into a file in a system that is ending in a disorderly crash. The Message may be terminated with a n in most cases.

Output that begins a new line in the debug output file may automatically be time stamped for easy reference.

Message may be supplied in Unicode encoding which comes by default in managed code. In C coding it may be specified in various ways.

OverallocatedLongs number of longwords allocated in excess of user requests. These arise from excessively small requests or from the memory manager s efforts to avoid excessively small free blocks.

Two extremes of fragmentation can be identified. The first extreme is when all free memory in the heap is concentrated in one free block. At that point the heap is considered to be 0 fragmented. The second extreme is when the heap is completely full of strictly alternating free and used blocks of the smallest possible size SmallestBlockAllocation . At that point the heap is considered to be 100 fragmented.

At heap initialization the maximum number of free blocks that are theoretically possible for the size of the heap created is calculated. That number is reported as MaximumPossibleCountOfFreeBlocks by GetHeapStatistics. The heap fragmentation percentage is computed as a linear fit between the 0 case 1 free block and the 100 case MaximumPossibleCountOfFreeBlocks blocks using the actual number of free blocks present.

NumberOfBins is the number of bin locations to be used in the GetBlockPopulationReport request. This call returns a suitable block of memory with the field BinCount set to the value of NumberOfBins. All other fields may be returned uninitialized.

This function accepts a histogram like binning specification and returns population counts for each of the bins requested. Populations for both used and free blocks in each category are returned.

Each bin represents all blocks of sizes between that bin s minimum size and one less than the next bin s minimum size to be counted. Minimum sizes may be specified in ascending order.

The MinimumSize for the first bin may be set to 0. If it is set to a number larger than 0 blocks below that size may go unreported. In that case CountOfBlocksNotAggregated may indicate how many blocks were not aggregated in the bins. This number counts both free and used blocks.

The function returns TRUE if the report specification was acceptable in which case data may be returned or FALSE if there is an error in the bin specification.

TimeLastStarted and TimeLastEnded are timestamps indicating an actual time when garbage collection last started and last ended. If TimeLastStarted is greater than TimeLastEnded a garbage collection is in progress it has recorded its start but not its end . If TimeLastStarted is less than TimeLastEnded a garbage collection is not in progress.

GC LongwordsLastFreed reflects the number of longwords that were freed by the most recently completed garbage collection cycle.

GC CurrentState indicates which of the three states of a garbage collection cycle are currently being performed. The states are 

This function is used to tell the memory manager that a block of memory is or is not to be relocatable. By default blocks of memory obtained from GetMemoryFloating may be subject to relocation as part of the defragmentation process. This function may be used to turn on and turn off that relocation.

This call may have no impact on a block that was allocated with GetMemoryFixed. Such a block may not be relocatable.

The function returns TRUE if a valid used block of heap is provided. It returns FALSE if a NULL pointer of otherwise invalid pointer is provided.

This function is used to tell the memory manager that a block of memory is or is not to be reclaimable by the garbage collection process. By default blocks of memory obtained from the heap are subject to reclamation as part of the normal heap management process. This function may be used to turn on and turn off that reclamation.

The function returns TRUE if a valid used block of heap is provided. It returns FALSE if a NULL pointer of otherwise invalid pointer is provided.

This function is used to characterize an address from the perspective of the memory manager. The BlockType gives the first level of information categorizing the pointer as one of the following 

If a valid used block from GetMemoryFixed or GetMemoryFloating is being described then LongwordLengthOfBlock and ByteOffsetIntoBlock further characterize the address.

If the block being described is associated with any valid heap instance at all then HeapIdentifier is the instance indicator for the particular associated heap.

This function causes a garbage collection cycle to be initiated if not already under way during the next call to ManagedEnvironmentBackgroundExecution.

ManagedEnvironmentBackgroundExecution may determine when to perform garbage collection along with defragmentation assembly loading and other background management functions so this may not be used. For testing purposes or if it is determined that heap is low and a garbage collection cycle would be advisable this call maybe used.

This call gets a flag and returns. It does not execute garbage collection code itself. That occurs when ManagedEnvironmentBackgroundExecution is next invoked.

