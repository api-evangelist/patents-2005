---

title: Architecture for rendering graphics on output devices
abstract: A method for accessing graphical information including receiving one or more graphics commands written in an application programming interface (API). The graphics commands are converted into a graphical language that facilitates the execution of the commands for a plurality of output devices of one or more types.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08042094&OS=08042094&RS=08042094
owner: Ellis Amalgamated LLC
number: 08042094
owner_city: Arlington
owner_country: US
publication_date: 20050707
---
This application claims the benefit of the filing date of U.S. provisional patent application Ser. No. 60 586 327 filed Jul. 8 2004 the contents of which are herein incorporated by reference.

The U.S. Government may have certain rights in this invention pursuant to Grant No. 70NANB3H3028 awarded by the National Institute of Standards and Technology NIST .

The present disclosure relates generally to imaging and visualization and more particularly to a generalized application programming interface API for rendering graphics on a variety of output devices. Example output device types include but are not limited to two dimensional displays three dimensional displays such as volumetric multi view and holographic displays and two and three dimensional printers.

Modern graphics environments must solve the problem that the application software generally runs on separate hardware from the rendering algorithms. Since off the shelf personal computers PCs are not yet specialized for spatial three dimensional 3 D rendering the process separation is generally more complicated than sending the data across the peripheral component interface PCI express bus. The Chromium architecture is a prior attempt to solve this problem. Chromium abstracts a graphical execution environment. However the binding between an application rendering resources and display is statically determined by a configuration file. Therefore applications cannot address specific rendering resources.

To ensure applicability to a broad range of users the software environment for the display should cater to at least three classes of software applications. Consider for example the case in which the output device is a volumetric 3 D display such as the Perspecta Spatial 3 D Display Actuality Systems Inc. Bedford Mass. described in U.S. Pat. No. 6 554 430. The first class of software applications includes legacy applications that were written without foreseeing integration with a volumetric display. It is helpful to be able to use legacy applications because a large number of them exist which perform useful and challenging visualization work. A second class of software applications includes ported applications that have had some minor adjustment by an application vendor to accommodate a volumetric display. Since volumetric displays are not already in widespread use application vendors may take a risk when they modify their software for these displays. To get the broadest range of applications it is important to minimize the risk to application vendors by providing easy to use extensions to existing graphics APIs. A third class of software applications includes native applications that are written to take full advantage of the capabilities of volumetric displays.

Continuing this example the software environment for a volumetric display must provide the ability to efficiently handle large datasets and be flexible enough to apply a broad range of rendering effects. Previous low level graphics APIs were not designed with 3 D displays in mind. They do not provide the abstractions needed to efficiently target the variety of two dimensional 2 D and 3 D displays that are now becoming available. The benefit to the visionary application vendors that adapt to new display technologies should be maximized by providing a consistent abstraction to a variety of emerging display types.

A well known graphics API is the Open Graphics Language OpenGL API. In some cases it is possible to adapt an OpenGL driver e.g. the OpenGL Driver from Actuality Systems Incorporated with little effort to yield interoperability with a legacy software application. Many off the shelf applications are fully functional and no additional work is required. However sometimes it is not possible to adapt all of the functions of a complex application or one that uses specialized rendering techniques. Customers are now looking for a higher level of integration with these sophisticated applications. OpenGL does not sufficiently abstract the process of rendering to suit volumetric displays to support this higher level of integration.

One problem with the OpenGL API is that it includes assumptions that are only appropriate for a flat 2 D monitor. For example the depth of the model within the rendering window is not defined. A second problem with OpenGL is that it does not abstract a large enough range of rendering operations. Volume rendering is an important function in many professional applications and it may grow in importance in recreational applications. The parameters to control a volume rendering operation are similar for all display types and there are flexible techniques for implementing the operations on existing graphics hardware. It would be desirable for a display agnostic interface to abstract volume rendering.

Furthermore volumetric displays cannot be completely served by existing graphical user interface GUI infrastructures. Actuality Systems Incorporated has defined a volume manager to organize the new functions required by volumetric applications. One of the first features of this volume manager is to provide an application independent 3 D pointer. See for example U.S. patent application Ser. No. 10 941 452 to Napoli et al. of common assignment herewith and U.S. Patent Application No. 2004 0135974 A1 to Favalora et al. also of common assignment herewith.

In summary current software architectures for 3 D display systems suffer from at least the following limitations. Applications cannot address remote or distributed resources. Such resources are necessary for displays where ready made rendering hardware is not available for PCs. Also applications can only address assets that are stored in a few prescribed formats. Imaging application software cannot access datasets with specialized encodings unless the application software includes a reformatting step. Reformatting reduces the ultimate performance of the application. A further limitation is that legacy ported and native application classes are not handled in a uniform way. Existing 3 D graphics APIs are designed with assumptions that are only valid for flat 2 D monitors. For example the appearance of a scene is only defined from a single given viewpoint. Another limitation is that existing 3 D graphics APIs do not completely abstract rendering algorithms. For example a volume rendering operation cannot be specified. A further limitation is that existing software architectures do not provide support for 3 D user interfaces.

Exemplary embodiments of the present invention are directed to a method for accessing graphical information. The method includes receiving one or more graphics commands written in an application programming interface API . The graphics commands are converted into a graphical language that facilitates the execution of the commands for a plurality of output devices of one or more types.

Other exemplary embodiments are directed to an architecture for accessing graphical information. The architecture includes an API layer for receiving one or more graphics commands written in an API. The architecture also includes a spatial visualization layer for converting the graphics commands into a graphical language that facilitates the execution of the commands for a plurality of output devices of one or more types.

Further exemplary embodiments are directed to a system for accessing graphical information. The system includes an input device and a processor in communication with the input device. The input device receives graphics commands written in an API. The processor includes instructions for converting the graphics commands into a graphical language that facilitates the execution of the commands for a plurality of output devices of one or more types.

Still further exemplary embodiments are directed to a computer program product for accessing graphical information. The computer program processor includes a storage medium readable by a processing circuit and storing instructions for execution by the processing circuit for performing a method. The method includes receiving one or more graphics commands written in an application programming interface API . The graphics commands are converted into a graphical language that facilitates the execution of the commands for a plurality of output devices of one or more types.

Exemplary embodiments of the present invention include a spatial 3 D architecture with a real time compatibility layer to allow legacy applications to support a broad range of 3 D display devices through an application and display agnostic dataflow design. The architecture also has a spatial visualization environment SVE that includes a 3 D rendering API and a display virtualization layer that enables application developers to universally exploit the unique benefits such as true volumetric rendering of 3 D displays. SVE supports the cooperative execution of multiple software applications. As part of the SVE a new API is defined referred to herein as the spatial graphics language SpatialGL to provide an optional display agnostic interface for 3 D rendering. SpatialGL is a graphical language that facilitates access to remote displays and graphical data e.g. rendering modules and assets . The architecture further has core rendering software which includes a collection of high performance rendering algorithms for a variety of 3 D displays. The architecture also includes core rendering electronics including a motherboard that combines a graphics processing unit GPU with a 64 bit processor and double buffered video memory to accelerate 3 D rendering for a variety of high resolution color multiplanar and or multiview displays. Many of today s 3 D software applications use the well known OpenGL API. To provide compatibility with those applications exemplary embodiments of the present invention include an OpenGL driver for the Actuality Systems Incorporated Perspecta Spatial 3 D Display product. Embodiments of the Perspecta Spatial 3 D Display product are described in U.S. Pat. No. 6 554 430 to Dorval et al. of common assignment herewith.

Currently a volume manager is available to manage cooperative access to display resources from one or more simultaneous software applications see for example U.S. Patent Application No. 2004 0135974 A1 to Favalora et al. of common assignment herewith . Current implementations of the volume manager have asset and rendering resources that are not abstracted separately from the display. The display rendering and storage system are considered as a single concept. Therefore the display and rendering system must be designed together. Effectively the display must be designed with the maximum image complexity in mind. Exemplary embodiments of the SVE as described herein remove this restriction by providing separately named asset computation rendering and display resources. Unlike other rendering systems the application has the flexibility to combine these resources by addressing each one independently.

The architecture depicted in includes four layers an application software layer an SVE layer a rendering architecture layer and a display specific rendering module layer . The application software layer includes legacy applications ported applications and native applications . The legacy applications and the ported applications are written to the OpenGL API and converted into the SpatialGL API by the OpenGL compatibility module in the SVE layer . OpenGL and SpatialGL are examples of API types. Exemplary embodiments are not limited to these two types of APIs and may be extended to support any graphics APIs such as the Direct3D API. The native applications are written to the SpatialGL API which is in communication with the volume manager . The rendering architecture layer depicted in includes core rendering software CRS which is a device independent management layer for performing computations renderings based on commands received from the SpatialGL API and data in the volume manager . The display specific rendering module layer includes a Perspecta rendering module for converting data from the CRS for output to a Perspecta Spatial 3 D Display and a multiview rendering module for converting data from the CRS into output to other 3 D and 2 D display devices.

Unlike prior architectures the architecture depicted in transforms commands e.g. graphics commands from several API types into a single graphical language SpatialGL. This permits the architecture to provide consistent access to display and rendering resources to both legacy and native application software. This is contrasted with the currently utilized device specific rendering drivers. Each driver manages rendering hardware visual assets display lists textures vertex buffers etc. and display devices. The architecture depicted in includes a rendering architecture layer that is a device independent management layer that with core rendering software . This rendering architecture layer gives the graphics language SpatialGL access to diverse high level resources such as multiple display geometries rendering clusters and image databases. Each class of resources asset e.g. volume manager computational e.g. core rendering software and display e.g. Perspecta rendering and multiview rendering is enabled by an independent module.

In addition the SVE is a display agnostic and potentially remote rendering architecture. The SVE can communicate with 2 D and very different 3 D displays multiplanar view sequential lenticular stereoscopic holographic . The rendering server does not need to be local to the display s .

The CRS is a collection of rendering strategies. The cost of implementing a rendering engine for a new display geometry breaks down into a system integration effort and an algorithm implementation effort. CRS eliminates the system integration effort by providing a portable communication framework to bridge the client and server domains and by abstracting computation assets. The CRS creates output for a Perspecta rendering module a multiview rendering module and can be tailored to create output for future rendering modules . In addition the architecture depicted in may be utilized to support future graphics display architectures and third party architectures .

The spatial transport protocol STP describes the interaction between the Spatial Visualization Environment and Core Rendering Software. The spatial transport protocol comprises a set of commands. The STP may optionally comprise a physical definition of the bus used to communicate STP formatted information. The STP commands are divided into several groups. One group of commands is for operating the rendering hardware and frame buffer associated with the display. Another group of commands is for synchronizing the STP command stream with events on the host device rendering hardware and frame buffer. Another group of commands is for operating features specific to the display hardware such as changing to a low power mode or reading back diagnostic information.

Different streams of graphics commands from different applications may proceed through the architecture to be merged into a single STP stream. Due to multitasking the STP is able to coherently communicate overlapping streams of graphics commands. STP supports synchronization objects between the applications or any layer below the application and the display hardware. The application level of the system typically generates sequential operations for the display drivers to process. Graphics commands may be communicated with a commutative language. For efficiency the display hardware completes the commands out of order. Occasionally order is important one graphics operation may refer to the output of a previous graphics operation or an application may read information back from the hardware expecting to receive a result from a sequence of graphics operations.

Exemplary embodiments of the SVE include a 3 D rendering API and display virtualization layer that enables application developers to universally exploit the unique benefits such as true volumetric rendering of 3 D displays. It consists of several subsystems SpatialGL OpenGL compatibility module streaming content library and volume manager . Future development may expand SVE to include scene graph rendering engine and application specific plug in subsystems.

Just as OpenGL API implementations are video card specific implementations of the SpatialGL API are display or output device specific. Examples of targets for SpatialGL implementations are 2 D displays volumetric displays view sequential displays and lenticular multi view displays. Exemplary embodiments of the SVE can communicate with a broad range of output devices whose underlying physics are quite different.

As depicted in SpatialGL is input to the client . In exemplary embodiments of the present invention the native API or SpatialGL API provides an object oriented front end to the STP byte code. The SpatialGL API exposes features such as but not limited to define fragment program define vertex program bind geometry source bind texture source swap buffer and synchronize. The client sends SpatialGL commands to the volume manager . The SpatialGL commands may include commands for retrieving persistent objects to be displayed on a graphical display device. The persistent objects include but are not limited to 2 D and 3 D textures and vertex buffers. The persistent objects may be stored on one or more of a database a storage medium and a memory buffer. In addition the SpatialGL commands may include commands for retrieving display nodes to be displayed on a graphical display device. Display nodes refer to an instance of any display that can be individually referenced e.g. a Perspecta display a 2 D display . STP commands from the volume manger are sent to the core rendering client . The core rendering client is the first computation resource available to the STP execution environment. Early data reducing filter stages can also execute here. Stream compression and volume overlay are processes that may be assigned computation resources at this point. The core rendering client formats the remainder of the filter graph to take into account the physical transport layer between the core rendering client and the core rendering server. At the STP interpreter block API calls are converted into STP. Each STP node is a computation resource. STP procedures get bound to STP nodes as the program is processed. The node executes any procedure that has been bound to it by a previous node.

Spatial Transport Protocol may be converted for persistent storage and written to a disk. This can be accomplished by storing the serialized Spatial Transport Protocol byte code to disk along with a global context table. The global context table allows context specific assets to be resolved when the STP file is later read back from disk. The global context table establishes correspondences between local context handles referenced by the STP byte code and persistent forms of the referenced data. For example a STP byte code may reference texture image number . The texture image number is associated with specific data in the original local context of the byte code. When saved to disk texture image number is associated with a particular texture image by the local context table. This can be accomplished by storing in table position a copy of the texture image or by storing a GUID or URL that identifies a persistent source for the texture image.

OpenGL is an industry standard low level 3 D graphics API for scientific and computer aided design CAD applications. OpenGL supplies a language that expresses static information. The application must explicitly break down dynamic scenes into discrete frames and render each one. OpenGL expresses commands such as input a vertex draw a triangle apply a texture engage a lighting model and show the new rendering.

Referring to OpenGL calls are duplicated for both the system library to render on the 2 D monitor and for the GLAS library . By default the first scene is analyzed to determine the depth center of the application s implied coordinate system. Since the depth center is not known until the first swap buffers call it may take until the second scene for the image in Perspecta to render properly.

The first scene is analyzed to determine the depth center of the application s coordinate system. Once the depth center is calculated a fix up transform is calculated. This transform is applied consistently to the projection specified by the application so that the application s further transformations the projection such as scaling and zooming are reflected properly in the spatial rendering. After the depth center is determined the Stub library issues a redraw call to the application to ensure that the first scene is drawn properly in Perspecta.

The two main configurations are ghost mode and extended mode . Ghost mode automatically duplicates OpenGL calls for both the system library and for the GLAS library . In ghost mode depth centering is based on x and y scale and centered to get the majority of the vertices within the display. Ghost mode provides an unextended OpenGL interface and attempts to make a spatial display appear as a 2 D display to the application. Extended mode allows the application to control the call forwarding behavior. Extended mode exposes an extended OpenGL interface. A few commands are added to help the application control a spatial display separately from a 2 D display. Example commands include create a context for a spatial display and draw to a spatial display context. Output from the GLAS library in SpatialGL is sent to the client and then to the volume manager . The volume manager assigns display resources. It filters the STP stream to reformat the data according to the display resource assigned to the given context. The core rendering block which contains the mechanisms for decoding and executing procedures in the STP language receives STP commands.

The configuration is controllable for each application based on a central control repository. Parameters that may configured include but are not limited to context selection strategy allows the controller to change the context selection while the application is running projection fix up strategy that overrides the projection that that application specifies in order to fit the image in the actual display geometry texture processing strategy context STP preamble e.g. resolution hints and scene STP preamble.

Some spatial displays physically realize view dependent lighting effects. In this case lighting is calculated based on the actual view directions rather than the master direction given by the projection matrix.

Specific rasterization constraints and rules can only be specified relative to the unique geometry of each display type. In general only fragments that intersect the projection of an element into the display s native coordinate system may be lit. When rendering polygons elements must not contain holes. When rendering connected polygons where exact vertex positions are shared the rendered figure must not contain holes.

When anti aliasing is used the partial ordering of the color value of the fragments must agree with the partial ordering of the intersection area or length between the fragment and pixels of the display s native coordinate system when normalized for variation in area or volume of the pixels.

The streaming content library permits spatial stream assets. A spatial stream asset is a time varying source of spatial imagery. Optionally the spatial stream may be synchronized with one or more audio streams. A spatial stream may either consist of a real time stream a recorded stream or a dynamically generated stream. An example of a ream time spatial stream is a multi view stream that is fed from an array of cameras. An example of a recorded stream is a spatial movie stored on a removable disk. An example of a dynamically generated stream is a sequence of dynamically rendered 3 D reconstructions from a PACS database.

Each stream is associated with a spatial codec. The intended interpretation of the stream is determined by the associated spatial codec. The spatial codec is comprised of a stream encoding specification and a reconstruction specification. The stream encoding specification determines the mapping from the raw binary stream to a time varying series of pixel arrays. The stream encoding specification may also identify an audio stream synchronized with the pixel arrays. The reconstruction specification determines the intended mapping from pixel arrays to physical light fields. Examples of stream encoding specifications include MPEG coded representations. The reconstruction specification can be defined using the persistent form of Spatial Transport Protocol.

A client of the streaming content library receives the raw binary stream and the spatial codec. The client proceeds to reconstruct an approximation of the intended physical light field by calculating pixel arrays using the stream encoding specification. At each time step the client consumes one or more pixel arrays and interprets an intended light field using the reconstruction specification. The intended light field is rendered into the local display s specific geometry using Core Rendering Software. Finally Core Rendering Software moves the rendered image into the spatial frame buffer causing the display to generate a physical manifestation of a light field.

The streaming content library includes spatial stream asset servers. Each spatial stream asset is published by a spatial asset server. An asset server may publish one or more streams each with a unique URL. A software application using SpatialGL such as a spatial media player can call up a particular spatial stream asset using its associated URL.

Spatial stream assets may be transmitted with unidirectional signaling for example several TV channels may be jointly used to transmit a multi view recording. In this case the spatial codec can be continuously or periodically transmitted. Spatial content may also be broadcast with bidirectional signaling for example a spatial movie may be downloaded from an Internet based asset server and viewed using a spatial media player using SpatialGL. In this case the client could potentially negotiate an optimal spatial codec to match the client s display geometry. Bidirectional signaling can also be used to allow a client to remotely control a dynamically generated stream. For example a client may continuously send updates to a server about the desired view direction and region of interest while the server continuously returns rendered images to the client through the streaming library. Alternately a client may receive notifications from the spatial stream asset server when new data is available. Based on the notifications the client may choose to download and render the new data or else the client may skip the new data. When receiving an notification the client may decide whether to download or skip the new data based on factors such as the currently available buffer space communication bandwidth processing power or desired level of image quality.

In an exemplary embodiment of the present invention the CRS has the character of a slave peripheral and communication to the CRS is limited to proprietary channels. Alternate exemplary embodiments of the CRS have an expanded role as a network device. In addition it can communicate with a host over a network and it supports standard protocols for network configuration. The CRS has both a client part and a server part.

In exemplary embodiments of the present invention a host PC runs an application and is in communication with a single multiplanar 3 D display which contains an embedded core rendering electronics system. The client part is embodied on the host PC while the server part is embodied on the core rendering electronics. CRS is distinct from the SVE because the CRS is meant primarily to provide a rendering engine for specific display types that is compatible with the display independent graphical commands generated in the SVE.

The client side of the CRS interfaces to the SVE using the STP language. STP is used to package and transport SpatialGL API calls. A core rendering client connects the volume manager to the physical transport by acting as an STP interpreter. The core rendering client interpreter exposes procedures with STP linkage that allow an STP program to address specific servers. Exemplary embodiments of the present invention only function when a single server is present. Alternate exemplary embodiments of the core rendering client communicate with servers over a network and are able to list and address the set of available servers.

The client also provides a boot service. This provides the boot image used by the net boot feature of the servers. The boot image is stored in a file that can be updated by Perspecta Software Suite upgrade disks or via web upgrade . The boot service can be enabled and disabled by the SVE. After the boot image file is upgraded the installer must enable the boot service to allow the display to update.

In the current example in which there is one host PC and one Perspecta display all input to the system arrives through the gigabit Ethernet connections. The embedded system acts as a normal Internet Protocol IP device. The embedded system acts as a server while the host PC acts as a client. The server acts as a normal IP device. In exemplary embodiments of the present invention the client and server must be directly connected. In alternate exemplary embodiments of the present invention clients and servers are connected through a gigabit switch. This configuration removes the requirement that the client PC contains two Ethernet controllers and it allows multiple clients to connect to a single server. The server obtains an IP address using dynamic host configuration protocol DHCP unless it has been configured to use a static address . Once an IP address has been obtained the CRS and the client must be made aware of the identity of the server. This is done by a symmetric system where a node client or server broadcasts a datagram when it starts. The node that starts first obtains the identity of the later node. If the server is started first and encounters a client datagram broadcast it opens a connection to the client to communicate the server s identity. A client may simultaneously communicate with multiple servers. Each server may only communicate with a single client at a time.

In alternate exemplary embodiments of the present invention the servers have a user interface and policy for attaching to specific clients when more than one client is available. The CRS provides a simple network management protocol SNMP interface to manage the network settings of the server. The SNMP interface configures the IP address broadcast settings and security settings. The security settings include client allow and deny lists.

In exemplary embodiments of the present invention the host and client support a single gigabit Ethernet connection. In alternate exemplary embodiments the host and client employ an additional protocol to support two gigabit Ethernet connections.

Once a client knows the identity of a server the client may open the server. The client and server communicate through datagrams. The server is single threaded the client may only open a single connection to the server and it is guaranteed exclusive access to the entire display resource. Once the client has opened the server it may begin transacting rendering commands. Rendering commands are moved between the client and server using a command stream and a remote memory protocol.

Since the network graphics service is meant to communicate only over a local network segment a very low level of packet loss is expected. The details of the communication scheme can be arranged to ensure that the system degrades gracefully under packet loss. Device allocation and context creation must be guaranteed to operate correctly under packet loss. The bulk graphics data transfer is not protected except that a frame that is rendered without packet loss must not be degraded by packet loss in previous frames. Persistent texture map data is protected against packet loss by a checksum and a failure retry scheme.

CRS uses the STP language as a form for communicating graphics commands and procedures. STP allows the interfaces between the major components of the Core Rendering Software system to be uniform. In the initial version of Core Rendering Software STP serves as the inter process communication mechanism. STP is used to communicate a sequence of graphics commands from the client to the server. The initial version of STP will include conditional execution and parallel branching prototype features. In later versions of Core Rendering Software modules will be written within the STP language thus flattening the hardware native part of the architecture. Conditional execution and parallel branching features will be optimized in later versions of Core Rendering Software.

An instance of a pipeline operates on a single input stream of homogeneous elements. An exemplary pipeline constructor includes initiate first in first out FIFO length and initialize stage connections. As depicted in fixed length FIFOs constrain the resource usage of the system.

Rendering pipelines are implemented as a series of stages that communicate with tasks. A stage is an algorithmic unit that transforms one stream of tasks into another stream of tasks. Although a stage may be designed to be compatible with a specific active object the binding to the active object is external to the stage . For example a stage may implicitly require a binding with a GPU by making OpenGL calls but it must not own or manipulate an OpenGL context.

Stage objects have an unusually complicated life cycle. They are typically created in one thread but work in a second thread. The lifetime of a stage consists of these distinct transitions construction initialization execution de initialization and destruction. A stage transforms a stream of homogeneous elements. A stage utilizes the resources of a single active object and executes several hundreds of times a second. The biding between a stage and an active object is external to the stage class. Therefore a pipeline may be considered a stage but a pipeline system may not be considered a stage . The remote active object depicted in models a thread of execution that exists outside of the CPU. Input to the active object includes data from the GL context block and the voxel engine context block .

Task objects are not strongly structured outside of their specific implementation domain. In exemplary embodiments of the present invention the pipeline framework includes a Fence class which is utilized to provide a main stream synchronization pattern. A pipeline system operates asynchronously from its enclosing system. The enclosing system can insert a Fence into the command stream of a pipeline . A pipeline passes a fence when all processing due to tasks issued before the fence have completed. The enclosing system can query whether the pipeline has passed the fence or it can block until the fence has been passed.

As described above a key feature of the SVE is display independence or display agnosticism . Implementations of the SpatialGL API can be made for a variety of 2 D and 3 D displays. The SpatialGL API may be utilized with a Perspecta multiplanar volumetric display. In addition the SpatialGL API may be utilized with other types of displays.

Because multi view rendering is very similar to single view rendering the SpatialGL implementation is substantially simpler than the SpatialGL implementation for multiplanar rendering. For example on flat horizontal parallax multi view displays such as the Stereographics 9 view lenticular display or Actualty System s quasi holographic video display a slice volume could be created as part of the rendering process. A slice volume contains a slice for each rendered view direction. Rendered views use sheared versions of standard projection matrices corresponding to the viewing angles. Final views correspond to the views that are physically generated by the display hardware. Final views are sampled from the slice volume for example using texture mapping . The number of final views may be different than the number of rendered views.

Rendering tetrahedra requires special treatment because at the time of writing GPUs lack native volumetric rendering support. In this case SpatialGL wraps an efficient volume rendering implementation such as ray casting.

Depending on the multiview display image formatting can be different. Because Stereographics lenticular display interfaces via digital visual interface DVI it does not require special formatting hardware such as the Voxel Engine in Actuality Systems Incorporated s Core Rendering Electronics . However the distribution of pixels and views is somewhat irregular and requires a reformatting process known as interzigging. Additionally view anti aliasing can occur during this step. On the other hand Actuality Systems holovideo display was designed to use the same Core Rendering Electronics as Perspecta and can share the same implementation.

Because SpatialGL is display agnostic SpatialGL can also be used for non 3D displays. Examples include tiled display walls displays with heterogeneous interfaces e.g. the Sunnybrook HDR LCD foveated resolution displays and displays with unusual geometries e.g. dome sphere or cube shaped displays . Finally an obvious example would be a standard 2 D display such as a desktop cathode ray tube CRT or liquid crystal display LCD . This would allow the use of SpatialGL programs on standard computer hardware without an exotic display configuration. For the most part the rendering of these displays only requires changes in the image reformatting stage and minor changes elsewhere.

Legacy applications invoke a similar command flow. In this case a legacy application renders a scene by issuing function calls via the compatibility module to the GLAS Ghost API similar to OpenGL . The GLAS ghost stub library receives the API calls and reformats the scene in preparation for translation to SpatialGL. For example the stub library may apply a heuristic that inspects the Ghost API calls to estimate the intended depth center of the scene. This additional information is passed to the GLAS translation library along with the API calls generated by the legacy application. Interpreted calls are translated into SpatialGL API calls.

The SpatialGL client library directs the API calls to the volume manager along with association information to identify the software application instance that generated the commands the ported application .

The volume manager modifies the API call stream. For example it may map the application s rendering output to a specific potion of the graphics volume generated by the display device. It may also apply an overlay to the rendering output such as a sprite that marks the location of a 3 D pointer.

After the volume manager the core rendering client library marshals the API calls and transmits them for example using Spatial Transport Protocol to the server execution environment. The core rendering software instantiated in the server execution environment receives and umnarshals API calls.

The core rendering server operates rendering algorithms based on the API calls. The rendering algorithms are defined by a renderer module . In general there is a specialized renderer module for each distinct class of display geometry. The rendering algorithms cause rendered image bitmaps to be moved into the spatial frame buffer using the voxel engine driver .

Exemplary embodiments of the present invention allow applications to have access to a wider variety of assets rendering algorithms and displays. Conversion to new display technologies may be facilitated by providing interfaces to legacy and ported applications.

As described above the embodiments of the invention may be embodied in the form of hardware software firmware or any processes and or apparatuses for practicing the embodiments. Embodiments of the invention may also be embodied in the form of computer program code containing instructions embodied in tangible media such as floppy diskettes CD ROMs hard drives or any other computer readable storage medium wherein when the computer program code is loaded into and executed by a computer the computer becomes an apparatus for practicing the invention. The present invention can also be embodied in the form of computer program code for example whether stored in a storage medium loaded into and or executed by a computer or transmitted over some transmission medium such as over electrical wiring or cabling through fiber optics or via electromagnetic radiation wherein when the computer program code is loaded into and executed by a computer the computer becomes an apparatus for practicing the invention. When implemented on a general purpose microprocessor the computer program code segments configure the microprocessor to create specific logic circuits.

While the invention has been described with reference to exemplary embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted for elements thereof without departing from the scope of the invention. In addition many modifications may be made to adapt a particular situation or material to the teachings of the invention without departing from the essential scope thereof. Therefore it is intended that the invention not be limited to the particular embodiment disclosed as the best mode contemplated for carrying out this invention but that the invention will include all embodiments falling within the scope of the appended claims. Moreover the use of the terms first second etc. do not denote any order or importance but rather the terms first second etc. are used to distinguish one element from another.

