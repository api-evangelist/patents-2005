---

title: Magnified machine vision user interface
abstract: Improved user interface methods facilitate navigation and programming operations for a magnified machine vision inspection system. Large composite images are determined and stored. The composite images include workpiece features that are distributed beyond the limits of a single magnified field of view of the machine vision system. Despite their size, the composite images may be recalled and displayed in a user-friendly manner that approximates a smooth, real-time, zoom effect. The user interface may include controls that allow a user to easily define a set of workpiece features to be inspected using a composite image, and to easily position the machine vision system to view those workpiece features for the purpose of programming inspection operations for them.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07394926&OS=07394926&RS=07394926
owner: Mitutoyo Corporation
number: 07394926
owner_city: Kawasaki-shi
owner_country: JP
publication_date: 20050930
---
The invention relates generally to machine vision inspection systems and more particularly to methods for providing an improved user interface usable to facilitate navigation and inspection operations for such systems.

Precision machine vision inspection systems or vision systems for short can be utilized to obtain precise dimensional measurements of inspected objects and to inspect various other object characteristics. Such systems may include a computer a camera and optical system and a precision stage that is movable in multiple directions so as to allow the camera to scan the features of a workpiece that is being inspected. One exemplary prior art system that is commercially available is the QUICK VISION series of PC based vision systems and QVPAK software available from Mitutoyo America Corporation MAC located in Aurora Ill. The features and operation of the QUICK VISION series of vision systems and the QVPAK software are generally described for example in the QVPAK 3D CNC published January 2003 and the QVPAK 3D CNC published September 1996 each of which is hereby incorporated by reference in their entirety. This product as exemplified by the QV 302 Pro model for example is able to use a microscope type optical system to provide images of a workpiece at various magnifications and a motion control system to move the stage as necessary to traverse the workpiece surface and inspect and measure features that fall beyond the limits of any single video image.

Machine vision inspection systems generally utilize automated video inspection. U.S. Pat. No. 6 542 180 teaches various aspects of such automated video inspection and is incorporated herein by reference in its entirety. As taught in the 180 patent automated video inspection metrology instruments generally have a programming capability that allows an automatic inspection event sequence to be defined by the user for each particular workpiece configuration. This can be implemented by text based programming for example or through a recording mode which progressively learns the inspection event sequence by storing a sequence of machine control instructions corresponding to a sequence of inspection operations performed by a user or through a combination of both methods. Such a recording mode is often referred to as learn mode or training mode. Once the inspection event sequence is defined in learn mode such a sequence can then be used to automatically acquire and additionally analyze or inspect images of a workpiece during run mode. 

The machine control instructions including the specific inspection event sequence i.e. how to acquire each image and how to analyze inspect each acquired image are generally stored as a part program or workpiece program that is specific to the particular workpiece configuration. For example a part program defines how to acquire each image such as how to position the camera relative to the workpiece at what lighting level at what magnification level etc. Further the part program defines how to analyze inspect an acquired image for example by using one or more video tools such as edge boundary detection video tools.

Video tools may be used manually to accomplish manual inspection and or machine control operations. Also their set up parameters and operation can be recorded during learn mode in order to create automatic inspection programs or part programs . Such tools may include for example edge boundary detection tools shape or pattern matching tools dimension measuring tools coordinate establishing tools and the like. For example such tools are routinely used in a variety of commercially available machine vision inspection systems such as the QUICK VISION series of vision systems and the associated QVPAK software discussed above.

General purpose visions systems such as Quick Vision frequently include a lens turret with lenses of various magnifications. It is common to inspect various aspects of a single workpiece or object using the various magnifications. Furthermore in industrial inspection environments very large workpieces are common. A single video image typically encompasses only a portion of the workpiece being observed or inspected given the desired magnification measurement resolution and physical size limitations of such systems. In such cases navigation to various microscopic features to be inspected on a workpiece can be difficult particularly when the field of view is small compared to the size or distribution of the features to be inspected and particularly when a number of confusingly similar or identical features are included on the workpiece. In general purpose visions systems during learn mode and or manual operation it has been conventional to navigate to view particular features on a workpiece either manually by trial and error or based on user knowledge of the workpiece design. However manual navigation may be both slow and prone to errors in distinguishing between similar features.

Alternatively user interfaces for semi automatically navigating based on CAD images or workpiece drawings or the like are known. However in many cases CAD data or workpiece drawings may not exist or may be in a format that is incompatible with such user interfaces. Also the differences between the representation of a feature in a CAD based user interface and the corresponding actual instance of the corresponding feature in a real workpiece image may limit the utility or application of such a CAD interface. For these and other reasons a more convenient realistic and intuitive user interface that enhances the navigation and inspection programming related to relatively small features of relatively large workpieces would desirable.

Currently the users of precision machine vision inspection systems may spend a significant portion of their part programming or manual inspection time navigating to particular features of a workpiece in order to set up appropriate video tools and adjust their parameters. In such systems improvements in the ease of use and GUI features of a user interface that facilitates navigation and or programming are desirable. The present invention is directed to novel and efficient features of a user interface that facilitates navigation and or inspection programming related to workpiece features that are distributed beyond the limits of a single field of view of a workpiece. In various embodiments the user interface includes controls and or menus or windows that allow a set of individual features to be defined or identified in dependence upon a customized set of parameters. The controls and or menus or windows and methods employed in the context of various embodiments of the user interface disclosed herein may allow a more realistic workpiece navigation image and or allow that image to be updated with a discernibly faster and more convenient response time in response to various user inputs in comparison to known user interfaces used for similar purposes. In addition various aspects of the user interface may convey more specific or detailed information regarding features in the workpiece navigation image in comparison to known user interfaces used for similar purposes and may provide a more precise and intuitive method to isolate and identify a desired set of features for purposes of navigation and or inspection programming operations.

The vision measuring machine includes a moveable workpiece stage and an optical imaging system which may include a zoom lens or interchangeable lenses. The zoom lens or interchangeable lenses generally provide various magnifications for the images provided by the optical imaging system . The machine vision inspection system is generally comparable to the QUICK VISION series of vision systems and the QVPAK software discussed above and similar state of the art commercially available precision machine vision inspection systems. The machine vision inspection system is also described in copending and commonly assigned U.S. patent application Ser. No. 10 978 227 which is hereby incorporated by reference in its entirety. Various aspects of vision measuring machines and control systems are also described in more detail in copending and commonly assigned U.S. patent application Ser. No. 10 808 948 filed Mar. 25 2004 and Ser. No. 10 632 823 filed Aug. 4 2003 which are also hereby incorporated by reference in their entirety.

A workpiece that is to be imaged using the machine vision inspection system is placed on the workpiece stage . One or more of the light sources and emits source light or respectively that is usable to illuminate the workpiece . Light emitted by the light sources and or illuminates the workpiece and is reflected or transmitted as workpiece light which passes through the interchangeable objective lens and the turret lens assembly and is gathered by the camera system . The image of the workpiece captured by the camera system is output on a signal line to the control system portion .

The light sources and that are used to illuminate the workpiece can include a stage light a coaxial light and a surface light such as a ring light or a programmable ring light all connected to the control system portion through signal lines or busses and respectively. As a primary optical assembly of the machine vision inspection system the optical assembly portion may include in addition to the previously discussed components other lenses and other optical elements such as apertures beam splitters and the like such as may be needed for providing coaxial illumination or other desirable machine vision inspection system features. When it is included as a secondary optical assembly of the machine vision inspection system the turret lens assembly includes at least a first turret lens position and lens and a second turret lens position and lens . The control system portion rotates the turret lens assembly along axis between at least the first and second turret lens positions through a signal line or bus .

The distance between the workpiece stage and the optical assembly portion can be adjusted to change the focus of the image of the workpiece captured by the camera system . In particular in various exemplary embodiments the optical assembly portion is movable in the vertical Z axis direction relative to the workpiece stage using a controllable motor that drives an actuator a connecting cable or the like to move the optical assembly portion along the Z axis. The term Z axis as used herein refers to the axis that is intended to be used for focusing the image obtained by the optical assembly portion . The controllable motor when used is connected to the input output interface via a signal line .

As shown in in various exemplary embodiments the control system portion includes a controller an input output interface a memory a workpiece program generator and executor a composite image user interface control portion a CAD file feature extractor and a power supply portion . It will be appreciated that each of these components as well as the additional components described below may be interconnected by one or more data control buses and or application programming interfaces or by direct connections between the various elements.

The input output interface includes an imaging control interface a motion control interface a lighting control interface and a lens control interface . The motion control interface includes a position control element and a speed acceleration control element . However it should be appreciated that in various exemplary embodiments such elements may be merged and or indistinguishable. The lighting control interface includes lighting control elements which control for example the selection power on off switch and strobe pulse timing if applicable for the various corresponding light sources of the machine vision inspection system such as the light sources and .

The memory includes an image file memory portion that stores acquired workpiece images and a composite image memory portion that stores workpiece composite images that are determined according to the operations of the composite image user interface control portion as described in greater detail below. In various embodiments the composite image memory portion and the image file memory portion may be merged and or indistinguishable. The memory also includes a workpiece program memory portion that may include one or more part programs or the like and a video tool portion . The video tool portion includes various video tool portions exemplified by the illustrated video tool portions which determine the GUI image processing operation etc. for each of the corresponding video tools. The video tool portion also includes a region of interest generator that supports automatic semi automatic and or manual operations that define various regions of interest that are operable in various video tools included in the video tool portion .

In general the memory portion stores data usable to operate the vision system components portion to capture or acquire an image of the workpiece such that the acquired image of the workpiece has desired image characteristics. The memory portion further stores data usable to operate the machine vision inspection system to perform various inspection and measurement operations on the acquired images either manually or automatically and to output the results through the input output interface . The memory portion also contains data defining a graphical user interface operable through the input output interface .

The signal lines or busses and of the stage light the coaxial light and the surface light respectively are all connected to the input output interface . The signal line from the camera system and the signal line from the controllable motor are connected to the input output interface . In addition to carrying image data the signal line may carry a signal from the controller that initiates image acquisition.

One or more display devices and one or more input devices can also be connected to the input output interface . The display devices and input devices can be used to display a user interface which may include various graphical user interface GUI features that are usable to perform inspection operations and or to create and or modify part programs to view the images captured by the camera system and or to directly control the vision system components portion . In a fully automated system having a predefined part program or workpiece program the display devices and or the input devices may be omitted.

With regard to the CAD file feature extractor information such as a CAD file representing a workpiece is frequently available in industrial applications of machine vision inspection systems. The locations of edges and boundaries in the CAD file representation may be determined manually in a semi automated fashion or fully automatically in such information may be useful for workpiece programming or navigating to a desired workpiece feature.

In various exemplary embodiments when a user utilizes the machine vision inspection system to create a workpiece image acquisition program for the workpiece the user generates workpiece program instructions either by explicitly coding the instructions automatically semi automatically or manually using a workpiece programming language or by generating the instructions by moving the machine vision inspection system through an image acquisition training sequence such that the workpiece program instructions capture the training sequence. This process is repeated for multiple images in a set of images that are to be captured. The workpiece program generator and executor supports generation of the workpiece program instructions and creation and storing of an associated workpiece part program file which may be stored in the workpiece program memory portion and or output. The workpiece program generator and executor also supports execution of program instructions of such workpiece part programs. In many cases such workpiece program instruction generation including workpiece specific training of various video tools is commonly accomplished using learn mode or training mode operations as outlined previously and described in greater detail in various references incorporated herein.

Once a set of workpiece image acquisition instructions are defined the control system under control of the controller and or the workpiece program generator and executor executes the instructions and commands the camera system to capture one or more images of the workpiece according to the instructions. The instructions may include for example instructions that cause the machine vision inspection system to manipulate the workpiece stage and or the camera system at certain speed s such that a particular portion of the workpiece is within the field of view of the camera system and at a desired focus state for each of a set of images to be acquired. In addition to the program instructions that control the relative movement of the camera and the workpiece the workpiece image acquisition program also needs to include program instructions that activate one or more of the light sources to provide a desired illumination of the workpiece during each image acquisition. The control system will then input the captured image s through the input output interface and store the captured or acquired image s in the memory . The controller may also display the captured images on the display device .

The control system portion may further execute instructions to recall captured and stored workpiece inspection images to inspect and analyze workpiece features in such workpiece inspection images and to store and or output the inspection results. These analysis and inspection methods are typically embodied in various video tools included in the video tool portion of the memory . Some of these tools including edge detection tools shape or pattern matching tools dimension measuring tools coordinate matching tools auto focus tools and the like for example are routinely available in a variety of commercially available machine vision inspection systems such as the QUICK VISION series of vision systems and the associated QVPAK software discussed above and in the references incorporated herein.

After the image inspection analysis operations using one or more of these video tools is completed the control system may further execute instructions that output the results of each analysis inspection operation to the input output interface for outputting to various display devices such as a video display printer and the like. The control system may also store the results of each inspection operation in the memory .

As previously outlined it is common to inspect various aspects of a single workpiece or object using various magnifications. Furthermore in industrial environments very large workpieces and high magnifications are common for precision machine vision inspection systems. A single video image typically encompasses only a portion of the workpiece being observed or inspected given the desired magnification measurement resolution and physical size limitations of such systems.

The composite image determination analysis and navigation methods of the present invention provide novel and efficient features that facilitate navigation and or inspection programming related to workpiece features that are distributed beyond the limits of a single field of view or image of a workpiece. The composite image interface and navigation methods may be implemented using a composite image user interface portion as will be described in greater detail below. In one embodiment the composite image user interface portion may be implemented as an integrated window or additional display area of a machine vision user interface such as that shown in . In such a case the composite image user interface portion may be activated using a selection under the Window menu element in the display area .

In various other embodiments the composite image user interface portion may be implemented in a client application for example through an applications programming interface API such as the API of the commercially available QUICK VISION series of vision systems and the associated QVPAK software discussed above. Briefly an API provides a format for a prescribed set of commands event information and or data input output exchange that can be used to facilitate cooperative operation between two programs. In one embodiment the composite image user interface portion may be incorporated in a specialized client application program for example a client application program specifically targeted to enhance the convenience and speed of printed circuit board inspection part programming. In such a case the specialized client application program may be directly activated using a selection under the Client menu element and the composite image user interface portion may be indirectly activated as part of that program. In another embodiment the composite image user interface portion may be implemented as a stand alone client program for example providing an add on program or a retrofit program that enhances the ease of use and utility of an existing machine vision system user interface. In such a case the composite image user interface portion may be directly activated using a selection under the Client menu element. In either case the interaction of the composite image user interface portion in cooperating with the remainder of the machine vision system user interface may implement any or all of the various features described herein.

At a block image acquisition locations are determined for the composite image region. Briefly a set of image acquisition locations may be determined such that all portions of the composite image preparation region are included in at least one image of a set of images acquired at the set of image acquisition locations. Ideally a step size between image acquisition locations may be determined along the X and Y directions such that the resulting images tile the composite image preparation region with a known overlap between adjacent images which may be little or no overlap. The step size may be determined for example based on the vision system magnification the camera imaging area dimensions and or the pixel spacing and count along the pixel rows and columns.

At a block a set of images are acquired at the set of image acquisition locations determined at block . In various embodiments the operations of this block are implemented as an automatically executed step and repeat operation that follows a regular pattern of motion through the set of acquisition locations. In one embodiment these images will have a pixel resolution that is pixels per unit distance on the workpiece that is the highest possible pixel resolution for the camera system and magnification used to acquire the images referred to herein as the acquired image resolution or the acquired image pixel resolution. The method of determining the lighting conditions and camera focus distance for each of the acquired images may depend upon the type of workpiece being imaged and the composite image portion user interface may include control elements that allow the user to indicate the type of workpiece or the preferred method of determining the lighting and camera focus. For example for flat and relatively consistent workpieces e.g. printed circuit boards a manually or automatically determined light setting used to produce a satisfactory image just prior to executing the set of image acquisitions may be reused for each acquired image. For non flat workpieces having inconsistent surfaces an autofocus operation and an automatic light adjustment operation may be performed at each image acquisition location prior to acquiring an image. Various methods for automatically determining light settings are included in U.S. Pat. No. 6 627 863 which is hereby incorporated by reference in its entirety. For the most difficult workpieces the machine vision system may automatically step through the image acquisition locations and prompt the user to set the lighting and focus and manually trigger the image acquisition at each location before automatically moving to the next image acquisition locations. The method of camera focus may even include acquiring a plurality of images at various focus distances for each image acquisition location. A synthetic acquired image having an extended depth of field may then be determined for each respective image acquisition location based on the plurality of images corresponding to the various focus distances. Various methods for determining such extended depth of field images are included in U.S. Pat. Nos. 4 141 032 and 4 584 704 which are hereby incorporated herein by reference in their entirety.

At a block the set of acquired images are used to assemble a high resolution composite image having a relatively high pixel resolution and the image is stored. The set of acquired images may be combined or tiled in the proper positions based on their known acquisition locations and dimensions to form the composite image. In one embodiment the high resolution composite image is formed directly from the acquired images and has a pixel resolution that is the same as the acquired image resolution. However when a composite image is assembled directly from a large number of high magnification images corresponding to a large composite image region the storage requirements and recall and display times for the composite image may become inconvenient. For purposes of navigation a composite image having the acquired image pixel resolution may not be necessary or efficient. Therefore in some embodiments the high resolution composite image may have a pixel resolution as low as one half or one quarter of the acquired image pixel resolution. In various embodiments the pixel resolution may be based at least partly on the number of acquired images used to assemble the high resolution composite image or a maximum number of pixels or a storage limit allowed for the high resolution composite image or a storage limit allowed for a complete set of composite images to be determined for use with the composite image interface portion or a combination of any of these considerations. In various embodiments a high resolution composite image having a pixel resolution that is less than an acquired image pixel resolution may be assembled by sub sampling the pixels of the acquired images at regular intervals or by otherwise scaling or interpolating the acquired images to provide a high resolution composite image with a pixel resolution that is less than the acquired image pixel resolution.

At a block a plurality of respective lower resolution composite images having respective pixel resolutions lower than the high resolution composite image are determined and stored. In one embodiment each of the composite images covers approximately the same portion of the workpiece and each composite image has a different pixel resolution. Therefore each composite image has a different size and or magnification when displayed in the composite image user interface portion. In various embodiments the lowest resolution composite image is determined with a pixel resolution such that its size allows the entire composite image to be displayed at one time in the composite image user interface portion. In one embodiment the remainder of the plurality of lower resolution composite images are determined with respective pixel resolutions that are distributed approximately evenly between the pixel resolution of the high resolution composite image and the pixel resolution of the lowest resolution composite image. In one embodiment the respective lower resolution composite images may be determined by sub sampling or by otherwise scaling or interpolating a previously determined composite image that has a relatively higher pixel resolution. In various embodiments such a plurality of composite images may be used to provide a composite image control feature that responds approximately instantly to user control operations and approximates the effect of using a zoom lens with the composite images as described further below.

At a block the previously determined and stored composite images are recalled displayed and replaced in the display by other composite images according to the setting of a user control provided in the composite image user interface portion. The control is sometimes referred to as a composite image zoom control herein. In various embodiments of the routine the total number of stored composite images including the high resolution composite image and the lower resolution composite images may be as small as three. However although three composite images may be sufficient to facilitate convenient workpiece navigation a disconcerting jumping zoom effect may result. Therefore in various other embodiments at least six composite images may be determined and stored. For an even smoother approximation of a zoom effect especially for composite images that are intended to span a large range of pixel resolutions at least nine composite images may be determined and stored.

The display area also includes an optional grid overlay . In various embodiments the grid overlay may help convey to the user a sense of the size relationship between a composite image displayed in the display area and the real time image displayed in the display area see . For this reason in the embodiment shown in the size of the rectangular cells of the grid overlay correspond to the size of the current field of view FOV of the machine vision inspection system which often approximately corresponds to the size of the display area . Such a grid cell size may be advantageous for helping the user instantly understand the size relationship between the images in the display areas and and intuitively navigate the workpiece using the machine vision inspection system. However in other embodiments the grid cell size may be set to a desired unit of length on the workpiece or any other size that is useful in a particular application or the grid overlay may be omitted or subject to user on off control.

The display area also includes a field of view FOV indicator at a location that corresponds to the current location of the FOV of the machine vision inspection system relative to the composite image . Thus the FOV indicator location may be updated in real time based on the state of the machine vision inspection system and the user may intuitively understand the location relationship between the composite image and the real time image displayed in the display area see . The FOV indicator shown in indicates the size as well as the location of the current FOV of the machine vision inspection system relative to the composite image . Thus if the magnification of the machine vision inspection system is changed or the composite image displayed in the display area is changed that is the pixel resolution of the displayed composite image is changed the size of the FOV indicator in the display area may be updated accordingly. However in various other embodiments a FOV indicator may indicate the location of the FOV without indicating the FOV size and still have substantial utility.

The composite image user interface portion also includes a display area including various controls for setting a composite image coverage region that is the region of the workpiece that is to be processed and included in the composite images for example as previously described with reference to . In the example shown in the display area includes an LUC button an RLC button and a STITCH button . The button is used to set the left upper corner of the coverage region once the user has used the user interface portion see to locate the camera at the desired left upper corner position on the workpiece. After the user has re positioned the camera at the desired right lower corner of the coverage region the button is then similarly used. After these operations the STITCH button may be used to activate an automatic procedure that determines and stores a number of composite images having a size that corresponds to the coverage region for example as previously described with reference to .

The composite image user interface portion also includes a display area that may include a sliding zoom control pointer and a ratio display . As outlined with reference to above previously determined and stored composite images may recalled displayed and replaced in the display area by other composite images according to the setting of the user controlled zoom control pointer . The ratio display shows the dimension ratio between the size of a feature in the currently displayed composite image and the size of that feature in the real time image see .

The composite image user interface portion also includes display areas and and a GET ORIG button all described in greater detail below. Briefly the display area includes various region determining controls used when detecting or determining regions in a composite image. The display area includes a region listing area and related controls for reviewing and manipulating the detected regions. The GET ORIG button recalls a stored composite image into the display area . The composite image user interface portion also includes a display area that includes various controls related to searching a composite image for features resembling a particular template or pattern. A Create template control button initiates operation of a template and or pattern creation user interface portion when activated. A Select template control button activates operation of a template and or pattern recall user interface portion when activated. A Search control button activates operation of a template and or pattern searching user interface portion when activated. All of the foregoing user interface portions related to template and pattern searching may include the same template creation method and interface elements used for conventional images in an existing commercial machine vision system if desired. The only difference is that the associated operations are directed to the currently displayed composite image in this case.

The user may control the X Y motion of the machine vision system using the composite image user interface portion . In one example of a one click method the user may position a cursor at a selected position relative to the composite image in the display area and input a control signal e.g. by pressing a mouse button designated to activate one click motion or the like that initiates a motion of the machine vision system based on the selected position such that a point on the workpiece that corresponds to the selected position is positioned in the field of view of the camera system. In another example the user may drag the FOV indicator to a selected position relative to the composite image in the display area which may automatically initiate a motion of the machine vision system that follows the location of the FOV in the composite image.

As previously outlined the display area includes various region determining controls including region parameter controls usable to detect or determine regions in a composite image and more specifically within an ROI defined by the composite image ROI indicator . In the exemplary embodiment shown in the display area includes an Auto checkbox control an Invert checkbox control a Detect button an intensity lower limit control an intensity upper limit control a minimum region size control and a maximum region size control which each include a slider control and a corresponding displayed value in this example.

When the user activates the Detect button regions will be automatically determined in a manner that depends on the various control settings and the results are indicated in the display . Region determining may generally include any now known or later developed region identification methods and may include a combination of operations such as filtering operations opening and closing operations connectedness analysis and the like. Region determining algorithms are well known in the field of image processing and may be obtained and or applied from various readily available image processing routine libraries.

In one example of operation the displayed values of the lower limit control and the upper limit control may be eight bit grayscale values. The intensity of pixels to be included in the determined regions must fall within the range of values indicated by the settings of these controls when the Auto checkbox control is not checked. Expressed another way the settings of these controls act as threshold values for admitting pixels as candidates for inclusion in a region. The determined regions which may be regarded as comprising connected components must furthermore have a size that falls within the range of values indicated by the settings of the minimum region size control and the maximum region size control which may be expressed as numbers of pixels or other applicable area units e.g. square millimeters .

The Auto checkbox control and the Invert checkbox control may operate as follows. The Invert checkbox control generally inverts creates a negative of the thresholded operations applied to composite image ROI. In one embodiment if the Invert control is unchecked the image pixels in the composite image ROI are admitted as candidates for inclusion in a region if their grayscale values are within the interval specified by the lower limit control and the upper limit control . If the Invert control is checked the image pixels in the composite image ROI are admitted as candidates for inclusion in a region if their grayscale values are outside the interval specified by the lower limit control and the upper limit control .

The Auto checkbox control deactivates the limits indicated by the lower limit control and the upper limit control and enables automatic thresholding operations in the composite image ROI. In one embodiment the automatic thresholding operations compute only a single threshold for example by using Otsu s algorithm or the like. Such automatic thresholding algorithms are well known in the field of image processing and may be obtained and or applied from various readily available image processing routine libraries. In one embodiment when the Auto checkbox control is checked and the Invert control is unchecked the pixels in the composite image ROI are admitted as candidates for inclusion in a region if their grayscale values are higher than the computed threshold. When the Auto checkbox control is checked and the Invert control is also checked the pixels in the composite image ROI are admitted as candidates for inclusion in a region if their grayscale values are lower than the computed threshold. In many cases using the automatic thresholding provides reasonable results if the histogram of grayscales in the selected image area is roughly bimodal for example if the composite image includes features to be inspected that have a relatively uniform image intensity that differs significantly from the background.

In various embodiments the determined regions are indicated in the display area and respond in real time or near real time to alterations in any active slider control settings and or other region determining controls described above. Thus the user may observe the displayed determined regions and simply adjust the various slider controls and or other region determining controls until the determined regions are the ones that the user desires. For example the determined regions may generally correspond to features of the workpiece that the user desires to inspect or measure.

In the exemplary embodiment shown in each determined region and a number of its region characteristics are also automatically listed in a region listing area of the display area which may also include various controls such as the Prev button and the Next button described further below for performing operations related to the listed regions. In various embodiments the region listing area may include a scrolling control not shown such that the region listing area may scroll and display a list having too many members to fit within the available display area . In the example shown in the listed items for each region include from left to right an identification number the x coordinate of the centroid of the region the y coordinate of the centroid of the region the area of the region in pixels an orientation or nominal angle metric and an elongation or conversely roundness metric. In addition a centroid indicator and an orientation indicator may be displayed for each determined region in the binary image representation

In a second example in response to the user selecting the determined region by double clicking on the list member corresponding to the determined region or by double clicking on the determined region itself an exemplary set of user interface operations may include highlighting the information of the list member corresponding to the selected determined region as indicated by the highlight . The exemplary set of operations may further include displaying the display box proximate to the selected determined region in the binary image representation . The exemplary set of operations may further include automatically initiating a motion of the machine vision system such that a point on the workpiece that corresponds to a point in the selected determined region e.g. its centroid is positioned centrally in the FOV of the machine vision inspection system. As a result of that motion a FOV indicator corresponding to the location of FOV of the machine vision system relative to the composite image is displayed surrounding that point in the representative determined region according to FOV operations outlined previously.

Reviewing the selection of the third list member corresponding to the then selected determined region is indicated by the highlight in the region listing area and the FOV indicator is centered about the centroid of the then selected determined region . In contrast in the selection of the fifth list member corresponding to the currently selected determined region is indicated by the highlight in the region listing area and the FOV indicator is centered about the centroid of the currently selected determined region . In various embodiments these differences can arise in response to two different types of user operations performed with the composite image user interface portion initially in the state shown in . In a first example the user may select the determined region by double clicking on the list member corresponding to the determined region or by double clicking on the determined region itself and in response to such double clicking automatic user interface operations will bring about the state shown in in a manner similar to the double click response previously described with reference to .

In a second example the Next button may be used to bring about the state shown in . In general the Next button may be used to select the list member in the region listing area and its corresponding determined region that is next down the list from a currently selected list member. The user interface response to such a selection using the Next button may be the same as the previously described response to selecting a list member by double clicking. Accordingly with the composite image user interface portion initially in the state shown in the user may activate the Next button two times and in response automatic user interface operations will eventually bring about the state shown in . Regarding the Prev button it may operate similarly to the Next button except that it is used to select the list member and its corresponding determined region that is next up the list from a currently selected list member in the region listing area .

It should be appreciated that the previously described region determination operations in combination with the resulting determined region list in the region listing area and the previously described features of the Prev button and Next button can considerably reduce the effort and confusion that may otherwise arise when attempting to locate a large number of similar features on a workpiece and create and or program associated inspection operations for example in a manual inspection mode and or a learn mode of operation of a machine vision inspection system. For example a relatively large composite image ROI containing many similar features to be inspected may be defined in a composite image. Then the region determination parameters may be adjusted with real time visual feedback as outlined above to rapidly isolate and list all of the regions corresponding to all of the similar features included in the composite image ROI. Then the user may simply use the Prev button and Next button to select successive list members in order to automatically locate their associated features in the FOV of the machine vision system and subsequently perform any related inspection or programming operations before selecting the next list member. The chance for overlooking a feature to be inspected or locating the wrong feature in the FOV may thus be virtually eliminated in various applications.

As a further enhancement in various embodiments when the machine vision system completes a programming operation for a workpiece feature corresponding to selected determined region the list member and the corresponding region indicated in the display area may marked or coded as programmed or inspected in some manner. In one example in learn mode when the machine vision system completes training a video tool and determining the related part program instructions for a workpiece feature corresponding to a selected determined region the text color of the list member and the color of the corresponding region indicated in the display area may be changed to a color used only for indicating a programmed or inspected status. As a further enhancement in various embodiments the display region may include a button or other control element not shown that may be operated by a user to manually delete a currently selected list item and its associated region from a set of determined regions.

The example of the composite image user interface portion shown in includes a portion of a composite image of a different workpiece. The display portion composite image is not centered in and does not fill the display area . Accordingly a blank area borders the composite image in the display area . Such an off centered composite image display and the blank area may be associated with desirable features in various embodiments. For example it may be perceptually desirable that when a new composite image having a different pixel resolution replaces a previously displayed composite image in accordance with a changed setting of the zoom control slider that the point on the workpiece that is located at the central point of the display area remains the same in the new composite image as it was in the previous composite image. Accordingly when that central point is near the limits of a relatively high resolution composite image which appears relatively more magnified as increasingly lower resolution composite images which appear relatively less magnified replace the high resolution image in the display area the edges of the coverage region of the composite image appear in the display area and the blank area may appear or become a greater portion of the display area . In various embodiments the FOV indicator not shown may be positioned in the blank area if that is its proper location relative to the displayed composite image. It will be appreciated that the blank area may still spatially correspond to the workpiece but simply falls outside of the composite image coverage region.

In the Show sorting checkbox control included in the display area is checked. In response various sorting parameter controls may be displayed in a sorting parameter display area . In the example shown in some of the sorting parameter controls are checkbox controls corresponding to some of the region parameters listed in the region listing area including the region centroid X coordinate Y coordinate region Area and Orientation . When one of the sorting parameter checkbox controls is checked the determined region list displayed in the region listing area is sorted or re ordered according to an ascending value order or descending value order in some embodiments for the corresponding region characteristic values listed in the region listing area . In the example shown in the Y coordinate checkbox control is checked and the list of determined regions is ordered accordingly.

One other sorting parameter control that may be displayed in the sorting parameter display area is a checkbox control labeled Motion Path . The Motion Path sorting parameter is a computed sorting parameter. When checked the motion path parameter control automatically initiates operations that use the X and Y centroid coordinates of each region corresponding to its nominal position to automatically determine an efficient motion path that connects all of the centroid locations e.g. the shortest path found by a path optimizing algorithm . Any suitable now known or later developed motion path planning algorithm may be used for automatically determining the efficient motion path. Then the determined region list displayed in the region listing area is sorted or re ordered into the same order as the region centroid sequence corresponding to the efficient motion path. The motion path sorting parameter is advantageous for many applications. For example once the region list members have been sorted based on this parameter if the Next button is then used to sequentially select the list members during learn mode operations which causes the machine vision system to move the corresponding region and features into the FOV as previously described and inspection operations are programmed and recorded for the corresponding features in the same order then consequently an efficient motion path will be recorded in the resulting part program.

As previously indicated a composite image user interface portion e.g. the composite image user interface portion may be implemented as an integrated window of a machine vision system user interface as a client application through an applications programming interface API incorporated in a specialized client application program or as a stand alone client program. The composite image user interface portion may also be implemented as a stand alone program which is designed to share data and instructions with other programs or applications. The structure of the embodiment of the composite image user interface control portion shown in and described below is a self contained structure that is readily adaptable to any or all of these implementations.

In one embodiment the composite image determining portion comprises a circuit or routine that is designed to perform operations that may include operations corresponding to the operations of the blocks described with reference to and or operations described in association with the display area and more generally relating to the creation and storing of a set of composite images. In one embodiment the composite image display control portion comprises a circuit or routine that is designed to perform operations that may include operations corresponding to the operations of the block described with reference to and or operations described in association with the display areas and and more generally relating to the display of a set of composite images. In one embodiment the composite image processing and analysis control portion comprises a circuit or routine that is designed to perform operations that may include operations described in association with the display areas and and in association with various methods of indicating determined regions and the like and more generally relating to image processing and or analyzing the composite images and displaying the results.

In one embodiment the composite image interface data and instruction input output portion comprises a circuit or routine that is designed to perform any operations that may be required in order to support any of the operations described above that rely on the exchange of data and or instructions and or event notifications or the like between a composite image user interface portion and a machine vision system control system portion for example data and or instructions and or event notifications associated with the operation of the FOV indicator or selection of a determined region and the resulting motion control current magnification information machine vision system configuration and specification data such as camera pixel specifications etc.

In various embodiments where the composite image user interface portion e.g. the composite image user interface portion is implemented as an integrated window of a machine vision system user interface the composite image interface data and instruction input output portion may be merged with or indistinguishable from the controller shown in . However in various embodiments where the composite image user interface portion is implemented as a client application e.g. through an API the composite image interface data and instruction input output portion may be included entirely in the client application along with the other elements of the composite image user interface control portion and may monitor and or govern transmissions of information to and from the client application through the API.

In general information that the composite image user interface control portion may need to request or otherwise obtain from various parts of a machine vision system control portion or its API may include for example the current X Y position in the machine coordinate system MCS the current image size the current pixel size or more generally information which allows the current magnification to be determined the current video image data when capturing images to be assembled into the highest resolution composite image . Action or control requests from the composite image user interface control portion to various parts of a machine vision system control portion or its API may include for example Move stage to MCS coordinates X Y and lighting change and or focus change requests for example when acquiring the video images over the coverage area . Action or control requests from various parts of a machine vision system control portion or its API to the composite image user interface control portion may include for example Update display based on new parameters A B C . . . where for example the FOV indicator position may be updated to correspond to position parameters or magnification parameters or the like.

While exemplary methods user controls user interface arrangements and sequences of operations have been outlined when describing various exemplary embodiments of the composite image user interface portion it will be appreciated that in other exemplary embodiments certain operations may be performed in other sequences and or one or more of the described methods user controls user interface arrangements and other disclosed features may be modified or omitted and the other inventive aspects of the systems and methods disclosed herein may still provide substantial benefits. Thus while the preferred embodiment of the invention has been illustrated and described it will be appreciated that various changes can be made therein without departing from the spirit and scope of the invention.

