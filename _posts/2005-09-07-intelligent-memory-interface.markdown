---

title: Intelligent memory interface
abstract: Many computer processing tasks require large numbers of memory intensive operations to be performed very rapidly. For example, computer network requires that packets be placed into and removed from First-In First-Out (FIFO) queues, numerous counters to be maintained and routing table look-ups to be performed. All of these operations must be performed at very high-speeds in order to keep up with today's high-speed computer network traffic. To help perform these high-speed memory tasks, a high-speed intelligent memory subsystem has been developed. The high-speed intelligent memory subsystem handles the intricacies of these memory operations such that a main process is relieved of some of its duties. Various different high-level memory interfaces for interfacing with the intelligent memory subsystem. The memory interfaces may be hardware-based or software-based. In one embodiment, two layers of interfaces are implemented such that an internal interface may evolve over successive generations without affecting an externally visible interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09274586&OS=09274586&RS=09274586
owner: Cisco Technology, Inc.
number: 09274586
owner_city: San Jose
owner_country: US
publication_date: 20050907
---
The present patent application claims the benefit of the previous U.S. Provisional Patent Application entitled Intelligent Memory Interface filed on Sep. 8 2004 having Ser. No. 60 608 237. The present patent application also hereby incorporates by reference in its entirety the U.S. patent application entitled High Speed Memory Control and I O Process System filed on Dec. 17 2004 having Ser. No. 11 016 572.

The present invention relates to the field of memory control subsystems. In particular the present invention discloses application programming interfaces and protocols for high speed memory subsystems.

Modern digital networking devices must operate at very high speeds in order to accommodate ever increasing line speeds and large numbers of different possible output paths. Thus it is very important to have a high speed processor in a network device in order to be able to quickly process data packets. However without an accompanying high speed memory system the high speed network processor may not be able to temporarily store data packets at an adequate rate. Thus a high speed digital network device design requires both a high speed network processor and an associated high speed memory system.

In addition to the high performance required networking applications place a wide variety of memory demands on their associated memory systems. For each packet that passes through a network router that packet must be buffered such that high speed packet buffering services are needed. Each packet must also be routed such that very frequent routing table look ups must be performed to determine where each packet will travel on its next network hop. Each packet will also cause a number of statistics counters to be affected such that these statistics counters must be updated. All of this packet buffering table look ups and statistics counter memory operations must be performed at very high speeds.

To provide adequate memory services for such demanding network applications traditional network routers generally employ multiple different types of memory technology coupled to a network processor. A high speed network device may contain a network processor that is coupled to three or more different types of memory subsystems. For example a first memory system may be used for packet buffering a second memory system may be used for storing statistics counters and a third memory system is used for storing look up tables. Such a memory arrangement is very complex and cumbersome. That type of memory arrangement of requires an individual connection to each different type of memory such that many input output pins are required on the network processor and the printed circuit board layout due to the multiple memory buses. Furthermore such a memory arrangement may use a large amount of memory such that the memory system will be expensive to implement consume large amounts of power and require significant amounts of printed circuit board area.

Due to the overwhelmingly complexity and high cost of conventional memory systems for high speed network devices it would be desirable to find an improved method of creating high speed memory systems for network devices that require a large variety of memory services at very high speeds. Ideally such a memory system would provide a consistent memory interface that would provide all of the different types of memory services needed by network devices.

The present invention introduces various high level memory interfaces for interfacing with an intelligent memory system. The memory interfaces may be hardware based or software based. In one embodiment two layers of interfaces are implemented such that an internal interface may evolve over successive generations without affecting an externally visible interface.

Other objects features and advantages of present invention will be apparent from the accompanying drawings and from the following detailed description.

Methods and apparatuses for implementing high speed memory systems for digital computer systems are disclosed. For example a number of memory system interface techniques are disclosed. In the following description for purposes of explanation specific nomenclature is set forth to provide a thorough understanding of the present invention. However it will be apparent to one skilled in the art that these specific details are not required in order to practice the present invention. Similarly although the present invention is mainly described with reference to packet switched network processing applications the same techniques can easily be applied to other types of computing applications. For example any computing application that uses FIFO queues may incorporate the FIFO queue methods and apparatus teachings of the present invention.

High speed digital networking applications place very heavy demands on memory systems. For each data packet that passes through a network device such as a network router that data packet must be buffered routing tables that specify where the data packet will travel must be consulted and statistics counters affected by the data packet must be updated. All of these operations require memory accesses that must be performed at very high speeds.

To provide memory services for such applications network routers generally employ multiple different types of memories. illustrates a high level block diagram of a network device that contains a network processor coupled to three different types of memory. Specifically a first memory system is used for packet buffering. The packet buffering memory system may be constructed with conventional DRAM if the network speeds are slow enough. However today s high speed networks often require a specialized low latency DRAM or SRAM is needed to handle the higher packet rates. A second memory system possibly SRAM is used for storing statistics counters that keep track of the network traffic. Finally a third memory system possibly conventional DRAM or a specialized low latency DRAM is used for storing look up tables such as network routing tables.

The memory arrangement of is very complex and cumbersome since it may use many different memory types memory interfaces and memory chips. For example the memory arrangement of requires a different type of connection for each different type of memory that is used. Such a design requires a very large number of pins on the network processor . Furthermore the large number of memory connections makes the printed circuit board layout difficult due to the large number of different memory buses. Furthermore the memory arrangement of uses a large amount of memory such that the memory system will be costly consume large amounts of power generate significant amounts of heat and require sizable amounts of printed circuit board area.

To provide memory services in a more efficient manner an intelligent memory subsystem has been introduced. illustrates a high level block diagram of an intelligent memory subsystem implemented within a network device such as a network router. The intelligent memory subsystem of provides a single memory interface to the network processor for all of the network processor s memory services. In one embodiment the intelligent memory subsystem uses a specially designed Network Memory Interface NMI . Memory operations for packet buffering table look ups and statistics counters can all be handled by the single Network Memory Interface NMI . Thus the need for multiple memory interfaces is eliminated. The Network Memory Interface NMI that couples the intelligent memory subsystem to the network processor has been optimized to efficiently use the bandwidth.

The intelligent memory subsystem of contains sophisticated memory logic in order to provide high level memory services beyond simple write to memory and read from memory. For example the intelligent memory subsystem of contains logic to provide abstracted First In First Out FIFO queue services to the network processor . The First In First Out FIFO queue services allow the network processor to specify a logical queue number and provide a command to access the FIFO queue.

By providing high level memory services the intelligent memory subsystem offloads some work normally performed by the network processor . For example to maintain multiple FIFO queues the network processor would normally need to maintain a set of linked lists by allocating memory as needed and keeping track of pointers connecting the items in the various linked lists. Thus the task of the network application running on the network processor is simplified. As demonstrated networking applications are an ideal application for the intelligent memory system this document will use the terms intelligent memory system and network memory system interchangeably.

By providing high level memory services the memory subsystem can be optimized to most efficiently perform those high level memory services. Thus an intelligent memory subsystem may implemented in a hierarchical memory system that stores frequently accessed information in a local high speed memory such as SRAM while less frequently accessed information can be placed into less expensive slower speed memory such as DRAM . Such a design reduces the cost of the system. Many additional details about implementing an intelligent memory system can be found in the U.S. patent application entitled High Speed Memory Control and I O Process System filed on Dec. 17 2004 having Ser. No. 11 016 572.

To simplify the task of designing network devices the network memory system provides a consistent and easy to use network memory application programming interface API . illustrates a block diagram of a network memory system with a network memory API . The network memory API provides a large set of memory services to network processor . The memory services may range from traditional read write memory services to abstracted First In First Out FIFO queue services and automatic updating counters. The network memory API may be considered to include two interfaces a management interface MI and a transaction interface. The management interface MI allows the network processor to configure and maintain the intelligent network memory subsystem . The transaction interface allows the network processor to make memory service requests by communicating with a transaction controller .

In one embodiment the memory service requests received on the network memory API are processed and related commands are sent across a second interface known as the network memory interface to a network memory unit . Note that there is not necessarily a one to one mapping of requests on the network memory API and the network memory interface . Thus a request on the network memory API may translate into zero one or multiple corresponding requests on the network memory interface to a network memory unit . The network memory interface allows the network memory unit to be physically separate from the network processor. Furthermore if the network memory interface is carried on a switched bus then multiple instances of network memory units may be used on the same switched bus.

In the embodiment of the network memory interface is divided into three layers. The top layer of the network memory interface is a configuration layer . The configuration layer is used to carry configuration information to a management interface in the network memory unit . The configuration information enables the network memory unit to allocate its memory resources according to the requests of the network processor .

The second layer of the network memory interface is a transaction layer . The transaction layer is the main communication layer used during operation of the network memory system . For example the routine memory service requests for basic read write memory services FIFO queue buffering services statistic counter services and table look up services all take place on the transaction layer of the network memory interface .

The bottom layer of the network memory interface is a physical layer for carrying the information communicated on the higher layers. In one embodiment the physical layer comprises a high speed serial bus. The bus may comprise physical and link layers as provided by a PCI Express bus or some other SERDES Serializer Deserializer based bus. Encoded request and response packets may be carried by the high speed serial bus.

The use of two successive interface layers the network memory API and the network memory interface provides a number of advantages for the network memory system . For example by having the second interface that is internal to the network memory system the network memory system may evolve over successive generations internally while keeping a consistent application programming interface to the network processor with the higher level network memory API . In this manner the physical layer may be changed from one bus technology to another the protocol in transaction layer may be expanded and additional configuration information may be added to the configuration layer without modifying the network memory API presented to the network processor .

Another advantage of the two layer interface system is that multiple different network memory application programming interfaces APIs may be presented to network device designers that design in different environments without changing the details of the internal network memory interface . For example both a hardware based interface and a software based interface may be supported on the network memory API . The hardware and software interfaces to the network memory services could be translated in the format needed by the internal network memory interface .

In the embodiment of the hardware interface comprises a library of network memory interface logic that may be incorporated into networking ASIC logic. The circuits in the network memory interface logic handle all processing necessary to send and receive information across the network memory interface in the network memory interface protocol. Specifically the circuits in the network memory interface logic translate all hardware requests from the processor logic into packets that will travel on the network memory interface . Translation of hardware based network memory ASI requests into packet requests is very useful if an external memory controller needs to be supported.

The network memory ASI should present a simple interface to the processor logic in an ASIC that will directly access memory services. The network memory ASI is application specific so that each logic module in an ASIC buffering module counter module etc. can independently interface with the network memory chips. Thus there would be distinct ASI interfaces for buffering counters semaphores etc. Every different ASI interface should allow compatibility with a memory interface in order to support raw reads writes. In one embodiment the hardware based ASI is an asynchronous interface where command requests and request results flow independently. Such an embodiment is capable of supporting a variety of operations with varying latency characteristics. Ideally a hardware based ASI should support variable length data transactions. Furthermore the hardware based ASI should have minimal overhead so as to minimize the read latency of transactions. Details on one hardware based ASI will be presented in a later section of this document.

If a network device designer decides to learn the details of the network memory interface then the network device designer may elect to directly access the memory services via the lower level network memory interface . illustrates an embodiment of network device wherein a designer has decided to directly access memory services using the network memory interface . By directly accessing the network memory interface a designer may create a very efficient interface system since there is no interface translation. For example if the network memory interface comprises an interface that uses encoded memory service packet requests then a designer may choose to encode his own memory service packets to directly interface with the network memory unit without any additional overhead.

To provide flexible design choices to a network device designer an ideal network memory system may provide both a higher level API interface and a lower level NMI packet interface to the network memory system. illustrates an example embodiment of the network memory system that provides both a higher level Application Specific Interface ASI and a lower level NMI packet interface. While it is expected that most network designers will opt to use the higher level ASI interface a network device designer especially a designer that is using a network processor and can write code may choose to directly use the NMI packet format to prevent conversion from one format to another.

Note that the higher level ASI Block may or may not translate memory requests into NMI packet based requests as set forth in respectively. Similarly the lower level NMI packet interface may use a set of memory service software routines or a direct packet interface as set forth in respectively. In one preferred embodiment the higher level ASI is implemented in parallel to the lower level NMI packet interface without any translation since forcing the higher level ASI to translate requests into NMI packet based requests would cause undesired latency.

The network memory higher level ASI may be implemented in a number of different ways. This section defines one possible method of implementing a hardware based interface.

In one embodiment of a hardware based ASI each application block within an ASIC buffering counters etc. executes memory service requests via the application block s own dedicated ASI interface. For example illustrates a network memory system wherein a buffering application block in an ASIC accesses memory services via a buffering ASI and a counters application block in the ASIC accesses memory services via a counters ASI.

In the embodiment of the hardware based ASI for a particular application is designed specifically for that application block. The specific ASI implements the functionality of the associated NMI memory requests for that application block. The hardware based ASI is asynchronous such that the ASIC sends commands while the network memory unit responds with results on its data interface independently.

Common Format of Application Specific Interface ASI While each hardware based ASI is specific to one application buffering counters etc. there are characteristics which are common amongst individual hardware based Application Specific Interfaces. illustrates one list of the types of signals that will commonly be used in a hardware based Application Specific Interface ASI . Depending on the specific application buffering counters etc. one or more of the signals illustrated in may not be present. Furthermore the size of the interfaces and clock speeds differ.

The following list defines the signals on the hardware based FIFO queue buffering ASI illustrated in 

The embodiment of presumes that 18 bits are sufficient for the change counter command since the byte counter for a specific packet cannot be incremented by more than a jumbo size packet. Furthermore it is expected that set counter and read counter need to transfer the complete counter value which can be as large as 72 bits. The data for such transactions with large amounts of data may be transferred over multiple clock cycles. Note that the TS bits transaction status are used to allow a transaction to use multiple clocks to transfer its data.

As previously set forth a preferred embodiment of the network memory system incorporates a second interface known as the network memory interface to a network memory unit . The network memory interface has been designed for in order to provide memory services at a high speed. Each layer includes features designed to optimize the performance of the network memory interface for providing memory services. Each layer of the network memory interface will be examined in detail starting from the lowest layer the physical layer.

As previously set forth the physical layer of the network memory interface may comprise any suitable high speed bus. In one particular implementation the network memory interface employs a high speed serializer deserializer interface commonly known as SERDES as illustrated in . A SERDES interface provides speed scalability up to 6 Gbit s per pin pair and even higher rates on the network memory interface . The SERDES interface also greatly reduces the number of input output pins needed by the network processor. With such high speeds and a low pin count a SERDES interface can provide two to three times more bandwidth per pin as compared to typical memory interfaces such as DRAM and SRAM based interfaces today.

In one embodiment the PCI Express implementations of the SERDES physical interface have been selected due to a number of advantages. The PCI express bus system has been standardized and is receiving a broad base of industry support. This will ensure that a wide variety of PCI Express circuits will become available and competition will create low costs. Furthermore the PCI Express bus system provides number of additional communication features beyond simple serial data transfer such as data integrity with Cyclic Redundancy Check CRC protection packet switching failure detection etc.

A first method of the using the PCI Express bus is to use the Advanced Switching Interface. illustrates the communication stack of the PCI express bus used to carry network memory interface protocol over the Advanced Switching system. Using the AS interface would make the system similar the X.25 packet communication system wherein packets can be routed. In such an embodiment the network memory protocol packets would be routed across PCI express AS infrastructure switches links . This method would provide around 25 inefficiency with 200 to 500 nanoseconds of latency. Such a system would be provide very good performance for statistical counter operations and table look ups.

A second method of using the PCI Express bus is to start only at the Data Link Layer. illustrates the communication stack of the PCI express bus used to carry network memory interface protocol using only the Data Link Layer and below of the PCI Express bus. Using only the Data Link Layer and below would make the system similar to the LAPB packet communication system that is only point to point with retransmission capabilities when errors are detected. This method would provide only 10 inefficiency with only 20 to 120 nanoseconds of latency. Such a system would be good for packet buffering and high performance table look ups.

The third method of using the PCI Express bus would be to provide a very simple data link layer without retransmission capabilities that would be similar to the HDLC communication protocol. illustrates the communication stack of the PCI Express bus used to carry network memory interface protocol using only the Data Link Layer and below of the PCI Express bus without any retransmission capabilities. This method of operation would sacrifice reliability for speed. This method would achieve a mere 5 inefficiency with only 5 to 120 nanoseconds of latency. This very lean implementation would provide excellent high performance packet buffering and memory access speeds.

Referring to the network memory interface is a packet based communication system. Network memory requests received on the Network memory API are encoded into packets that are transmitted across the physical layer of the network memory interface to the network memory unit . Response packets are encoded in the same manner by the network memory unit and transmitted back to the modules that handle the network memory API .

Three different NMI packet formats have been defined. All three of these packet formats may potentially be used. Each different NMI packet format will be individually described. In all of the different packet formats a first section describes an operation type and operation code along with operation control information a second section describes fixed length parameters associated with the operation type and operation code specified in the first section and the third section comprises a variable length parameter section that contains zero or more variable length parameters associated with the operation type and operation code specified in the first section. Note that specific bit lengths are provided in the accompanying figures however different bit lengths may be used.

The operation control field of the standard packet format includes a channel identifier and a transaction identifier. The channel identifier identifies a particular logical channel in the network memory unit such that multiple different entities that require memory services can use a single network memory unit by specifying different logical channels. The transaction identifier identifies a particular transaction. The transaction identifier allows multiple transactions of the same operation type and operation code to be outstanding by assigning different transaction identifiers to each transaction. Note that the bit border between the channel identifier and the transaction identifier is not fixed such that it may be configured as needed for a particular application.

The second section of the standard packet format is a fixed length parameter field. The fixed length parameter field contains all of the fixed length parameters associated with the operation type and operation code specified in the first section. The order and size of each fixed length parameter is dependent on the operation type and operation code specified in the first section. Examples of fixed length parameters that may be contained in the fixed length parameter field include Counter identifiers Queue identifiers Address

Finally the third section of the standard packet format is a set of variable length parameters. The variable length parameter field contains all of the variable length parameters associated with the operation type and operation code specified in the first section. Note that each variable length parameter specifies a parameter identifier and a length of the parameter. The variable length parameters that may appear are dependent on the operation type and operation code specified in the first section.

The remainder of the packet format with sequence is the same as the standard packet format. Specifically the remainder of the packet contains a fixed length parameter field and a set of variable length parameters. Thus the simple packet format differs from the standard packet format only in that no channel or transaction identifiers are used.

Each of the NMI packets is encoded with NMI transaction information for a specific transaction. In general the NMI packets form a request and response system. Requests to the network memory unit may comprise configuration commands or specific memory transaction commands. The response packets that are received back from the network memory unit specify success or failure of the request and any the response information that was requested.

There are a large number of different memory service operation types and specific memory operations are supported. A raw memory operation type includes read write copy and reset operations. A semaphore memory service type includes set test set and exchange operations. A FIFO queue buffering useful for packet buffer memory service type includes write to queue and read from queue operations. A statistics counter memory service type includes set increment add read and reset operations. A table look up memory service type includes operations for selecting the largest item in a table selecting an exact item in a table a longest prefix match operation and a number of other look up operations.

The NMI packet format is a very generic format that allows many different operations to be defined. Furthermore the NMI packet format is not fully mapped out such that it is easily expandable. The higher level API implemented usually as a software based API is the interface normally used by a designer. The memory service operations described below will be described with reference to associated API commands for accessing the memory services. The API commands all map to associated NMI packets that can be used to carry the memory service request across the NMI interface.

Probably the most important type of memory service type for network applications is a First In First Out FIFO queue buffering memory service type. The FIFO queue buffering type of commands will be used for packet buffering. By providing high level FIFO queue operations the network memory system eliminates the need for a network processor to maintain linked lists free memory lists for queues and pointer management.

An ideal set of FIFO queue buffering operations would provide a wide set of features that would enable many different types of FIFO queue buffering. For example both unicast and multicast buffering should be supported. Furthermore the FIFO queue buffering should support both variable length element packet buffering and fixed size element cell buffering. The ability to buffer complete or incomplete FIFO elements at time of buffering would be very useful. Furthermore it would be beneficial to be able to buffer an element without immediately having the logical queue number. Additional features to support include multicasting by queue bitmap linked list multicast group packet tail drop packet head drop CRC drop among others.

In one particular embodiment a set of four buffering operation types in addition to the buffering configuration was found to provide all of the above features. The follow list contains those four FIFO queue buffering operations. Note that in this document the term packet may be used interchangeably with the term element since most FIFO queues will contain packets in a network application. The following list presents an Application Programming Interface API for FIFO queue operations

The Read Queue Write Queue and Drop operations are relatively straightforward. However the Prepend and move operation is non trivial and requires further explanation. The Prepend and move operation allows for sophisticated FIFO element header and FIFO queue manipulation. The following sophisticated queuing operations can be performed with the Prepend and move operation 

As previously set forth the four FIFO queue buffering operations specified allow for a wide variety of types of FIFO queue buffering to be performed. The following examples illustrate how different types of queuing may be performed.

The most simple type of FIFO queuing is unicast queuing. Unicast queuing occurs when each element is only read only once. The following commands may be used to perform unicast queuing when the queue identifier is known when element arrives and a complete element is buffered 

To performing unicast queuing when the queue identifier known when fully complete element is available but the element is written in multiple parts since element may be very large

If in the preceding example the most recently added element must be removed for example a packet may be dropped when a CRC error is found the following operations may be performed 

To perform unicast queuing when the queue identifier is known when packet arrives but the packet header will not be available until later then the following commands may be issued 

If neither the queue identifier is known nor is the packet header available when the packet arrives then the previous steps may be used except that a temporary queue is used to buffer the packet payload temporarily. Later when the queue identifier becomes known then the packet is moved to the correct queue.

One way to perform multicast queuing is by keeping multicast packets in a specific multicast queue s . Then a queue element may be read a number of times and is not removed until the last of multiple recipients reads the queue element. The following operations may be performed to write a packet to a multicast queue and then read from the queue once for each recipient of the multicast 

If the destination queue of a multicast packet is not immediately known or the entire packet is not yet available immediately at the time of arrival then the same techniques described in the unicast section above may be applied.

However the headers of multicast packets can introduce some difficulties. Specifically the headers for different multicast queue groups may need to be different. To accommodate such a situation the following operations may be used 

As set forth in a configuration layer is used to communicate configuration information. The configuration information is used to allocate the resources of the network memory unit . A configuration compiler may be used to examine a configuration to determine if the configuration is possible and if so how the configuration information should be formatted. Examples of formatted configuration information include downloadable firmware a set of configuration packets or a set of register settings.

Each different type of memory service has its own particular configuration requirements. Thus each type of memory service has a set of configuration information. In one embodiment a generic object oriented interface is for configuration. Specifically resources are categorized into different classes and each class has its own associated attributes and parameters. The following list provides an example of a generic object oriented configuration system that may be used 

The following list contains one possible list of instance parameters for a FIFO buffering queue class 

For example the following list configuration commands may be used to create a set of logically addressed 4 Gigabit FIFO queue for packet buffering with 8000 queues.

Additional configuration values should be specified to describe the specific requirements of the packet buffering memory sub system. A network memory system compiler will receive the set of configuration commands and output whether a specific network memory unit i.e. a specific version of a chip can support a requested configuration or not.

Network devices generally maintain a large number of statistics counters in order to monitor the performance of the network and the network device. These counters may be modified very frequently such as on a per packet basis. The counters are generally read out on a much less frequent basis. Thus a set of memory operations has been created to efficiently address the high speed statistical counter requirements of a network device.

The counter operations have been designed to optimize counter performance. Thus the two memory accesses of a standard read modify write cycle have been reduced to a single counter operation. Similarly the multiple memory accesses required for very wide counters has been reduced to a single counter operation.

To provide a wide variety of counter services a rich feature set of counter operations is needed. For basics a counter service should provide for the ability to set read add subtract increment and decrement counters. Those features should be available for handling one or more counters simultaneously. The counters should be addressable by a memory address or by a logical counter number. Advanced counter features would include special packet operations and byte counter pairs. In one embodiment the counter system allows for the auto increment of packet counter in a paired counter. Compound counter operations would further enhance the counter system such that compound read clear and read reset to value operations are provided.

In one embodiment a mere three different counter operations can provide all of the features described above. The following list provides an API for three counter operations 

In an alternate embodiment a number of optional counter operations are also provided. The following list contains a set of optional counter operations that allow for optimization of the previous commands 

The following list contains one possible list of instance parameters for a statistical counters class 

Many applications require semaphores in order to enforce database coherency and resource allocation without deadlock. A network memory unit may provide semaphore services. One embodiment of NMI based semaphores uses only two operations. The following presents an API for addressing the two semaphore operations 

The network memory system allows conventional raw memory accesses to be performed. Specifically the network memory system provides commands to perform raw reads and writes on memory. This allows for backward compatibility to existing memory sub systems. Ideally the raw memory system would further include special commands to optimize reads optimize writes read a number of locations and write to number of locations.

In one embodiment the simple raw memory access operations are provided with only two commands. The following list describes the API for the two raw memory operations 

To provide more efficient raw memory access features described above the following optional raw memory access commands may also be implemented 

The following list contains one possible list of instance parameters for a raw memory operation class 

The foregoing has described a number of techniques for implementing an intelligent memory system interface. It is contemplated that changes and modifications may be made by one of ordinary skill in the art to the materials and arrangements of elements of the present invention without departing from the scope of the invention.

