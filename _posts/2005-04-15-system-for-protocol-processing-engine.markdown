---

title: System for protocol processing engine
abstract: A protocol processing engine system, which is arranged between a host side and a media access control (MAC) layer to thus speed up data transmission. The system includes a host interface, a network media access control (MAC) interface, a receiving subsystem and a transmitting subsystem. The system uses the host interface to interface with the host side and the network MAC interface to interface with the network MAC layer. The receiving subsystem classifies a connection received from the network MAC layer into a priority connection, fast connection or slow connection based on a pre-stored information, and activates its respective hardware circuit based on the connection classified in order to speed up data transmission of the connection. The transmitting subsystem transmits a connection received from the host interface to the network MAC interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07447230&OS=07447230&RS=07447230
owner: Industrial Technology Research Institute
number: 07447230
owner_city: Hsinchu
owner_country: TW
publication_date: 20050415
---
The invention relates to a technical field of local area networks LANs and more particularly to a general purpose Transmission Control Protocol Internet Protocol TCP IP system for protocol processing engine.

Currently the TCP IP protocol is applied to Internet data accesses widely. Well developed Internet applications greatly increase applicable servers such as web servers and media servers. Such servers generally use an operating system to control a CPU on packaging unpackaging and associated protocol processing of TCP IP packets. However the CPU requires a great number of processing cycles for processing the TCP IP packets as a result of service requests rapidly increasing on the Internet thereby reducing the performance.

In addition with rapidly developed network transmission and semiconductor technologies high speed 1 Gbps Ethernet and fiber networks are popular. In this case the operating system is applied to package and unpackage TCP IP packets and process the associated protocol stack modes so that the CPU has heavy load and the TCP IP packet processing consumes most time. is a schematic graph of CPU performance in different network maximum transmitting units MTUs . As shown in if a network server has a CPU clock of 500 MHz a memory and peripheral bus bandwidth of 370 Mbps and a network maximum transmitting unit MTU range of 1.5 to 32 Kbytes it is clear that the time spent on processing protocol by the CPU is reduced from 90 to 10 after different offloading functions are added when the MTU is at 32 Kbytes. This indicates that a network interface controller NIC capable of supporting a Gbps level and above bandwidth requires not only a wider MTU but also a protocol offloading function for CPU load balance.

However a typical NIC has not such a protocol offload engine for packaging unpackaging and associated protocol stack processing of the TCP IP packets current 10 100 Mbps Ethernet networks all use an OS based processing mode. is a schematic diagram of an OS based protocol stack processing mode. In a session layer a payload to be sent is produced by a left hand application of in a memory and next the application uses a system call to transfer the following performance to the OS. The OS divides the payload into segments and prefixes a transport header to each segment in a transport layer where the TCP locates to thus form transport packets. The transport packets are sent to a procedure of a network layer where the IP locates . The procedure optionally performs a fragmentation based on sizes of the packets and an MTU supported by a media access control MAC and adds a network header in each fragment to thus form network packets. The network packets are established in a NIC that integrates the MAC and a physical layer to further add a MAC header in each packet and thus form Ethernet packets. The physical layer sends the Ethernet packets to a receiving side through an Ethernet. An unpackaging procedure is performed reversely from the physical layer to the session layer to accordingly restore the payload required by the application. Therefore the conventional processing mode applies the OS to process the packaging and unpackaging of the transport layer TCP and network layer IP .

Since most NICs in current local area networks can support over 1 G bps bandwidth to lessen the CPU load on the TCP IP protocol processing a NIC has to provide an offload function. is a schematic diagram of a job sharing of a typical general purpose TCP IP protocol offload engine TOE and a host which shifts the protocol layer processed by the NIC to the transport layer. The application uses an OS call to deliver the size of a payload to be sent and its base address to a driver of the TOE. The TOE sends TCP IP packets completely packaged to the receiving side. A TOE of the receiving side unpackages the packets received and sends the payload obtained and unpackaged to a destination memory area of the application thereby completing the transfer.

As shown in it is an ideal sharing way that a job is shared by a TCP IP protocol offload engine TOE and a host but by a view of cost and economic benefit due to numerous protocols on Ethernet and other networks it does not meet with the cost benefit when the typical NIC adds in the TOE. The TOE depends on an application environment to select accelerating items with a higher efficiency to cost ratio. Other protocol processing with a low efficiency to cost ratio still uses the conventional protocol stack processing.

A transmission mechanism is disclosed in U.S. Pat. No. 6 591 302 that assembles a header required for each protocol processing from a protocol stack combines the header and an appropriate size of data and sends the combined to next network layer. is a schematic diagram of the transmission process disclosed in U.S. Pat. No. 6 591 302. A session sequencer sends the data to a transport sequencer via a multiplexer. The transport sequencer adds H headers in a packet buffer of divided transport segments to form transport packets. The transport packets are delivered to a network sequencer via the multiplexer to thus add H headers in the packet buffer of divided network fragments to form network packets. The network packets are delivered to a media access control MAC sequencer via the multiplexer to thus add a H header in front of each network packet to form Ethernet packets to output.

A RISC microprocessor with a protocol offload engine disclosed in U.S. Pat. No. 6 591 302 can speed up protocol offload performance but cannot easily obtain heavy complicated protocol errors and associated error recovery which leads to a disadvantage of firmware cost increase. The errors occupy a slight part of the entire packets but it deeply affects the CPU load when certain errors not necessary to interrupt the host CPU cannot be filtered effectively.

A SRAM disclosed in U.S. Pat. No. 6 591 302 is responsible to offload operation and performance. Any packet or data not able to offload immediately is moved from the SRAM to a DRAM having greater room and lower cost in order to wait until all other packets or information is arrived. Next all information arrived is moved from the DRAM to the SRAM for offload performance. As such it can simplify the offload engine design and the complexity of internal data path. However data moved between the SRAM and the DRAM can relatively consume the bandwidth of internal circuits. In addition the SRAM may lead to a bandwidth bottleneck. For a network bandwidth of 1 Gbps a full duplex protocol offload engine needs a bandwidth of 2 Gbps but a high speed dual port SRAM costs expensive. Therefore it is desirable to provide an improved protocol offload engine to mitigate and or obviate the aforementioned problems.

The object of the invention is to provide a system for protocol processing engine to eliminate the problem in that protocol errors and associated error recovery cannot be easily achieved and the problem of memory bandwidth in the prior art.

According to a feature of the invention a system for protocol processing engine is provided which is arranged between a host side and a media access control layer MAC to speed up data transmission. The system includes a host interface a network MAC interface a receiving subsystem and a transmitting subsystem. The host interface is used to interface with the host side. The network MAC interface is used to interface with the MAC layer. The receiving subsystem is coupled between the host interface and the network MAC interface such that a connection received by the network MAC interface is classified into a priority connection fast connection or slow connection based on a pre stored information and a hardware circuit corresponding to the connection classified is activated to speed up data transmission of the connection. The transmitting subsystem is coupled between the host interface and the network MAC interface in order to transmit a connection received by the host interface to the network MAC interface.

According to another feature of the invention a system for protocol processing engine is provided which is arranged between a host side and a media access control MAC layer to speed up data transmission. The system includes a host interface a network MAC interface a dynamic random access memory DRAM interface a DRAM a cache static random access memory SRAM a content addressable memory CAM a receiving controller a scorekeeper a receiving buffer a receiving queue a memory allocator a connection information table CIT updater a CIT seeker a packet processing assistant PPA a microprocessor MCU a protocol control information PCI controller a direct memory access receiving DMA Rx engine a descriptor maker a descriptors and command buffer a transmitting queue a direct memory access DMA transmitting engine a receiving queue management and a transmitting controller. The host interface interfaces with a host side. The network media access control MAC interface interfaces with the MAC layer. A dynamic random access memory DRAM is accessed through the DRAM interface. The DRAM stores packets to be processed and a connection information table CIT . The cache static random access memory SRAM stores priority connection information. The CAM provides the information of priority connections in the cache SRAM for fast searching. The receiving controller is coupled to the network MAC interface in order to receive a connection transmitted by the network MAC interface. The scorekeeper is coupled to the receiving controller in order to handle accesses and associated use situations of the CAM. The receiving buffer is coupled to the receiving controller in order to store a received packet. The receiving queue is coupled to the receiving controller in order to store the received packet or its payload. The memory allocator is coupled to the receiving controller in order to allocate the receiving buffer such that the receiving controller is able to move the payload in place. The CIT updater establishes information of a new connection information in the DRAM. The CIT seeker is coupled to the receiving queue in order to search the DRAM for a matched fast connection information in accordance with a receiving event in the CIT seeker queue and next write a base memory address which starts to store the fast connection information back to the receiving event. The PPA is coupled to the receiving queue in order to obtain a destination information of the received packet according to a content of the PPA queue. The MCU is coupled to the receiving queue in order to handle supported communication protocols. The PCI controller is coupled to a PCI interface to transmit and receive data of the host side. The DMA Rx engine arranges an address of a host destination in the PPA or the MCU places the address in a respective packet header information stored in the receiving queue and sends a packet or its payload corresponding to the respective packet header information to the host side through the PCI controller. The descriptor maker produces a receiving descriptor corresponding to the packet or its payload sent by the DMA Rx engine and uses the PCI controller to send the receiving descriptor produced to the host side. The descriptors and command buffer stores transmitting descriptors of host transmitting connections for further processing by the MCU. The transmitting queue stores processed transmitting descriptors that are processed by the MCU. The transmitting buffer stores payload data of the host transmitting connections. The header generator produces a packet header of a host transmitting connection. The global register file stores control information connection information payloads packets of slow path connections and processed status. The DMA transmitting engine moves the CIT descriptors to the descriptors and command buffer. The receiving queue management manages the receiving queue. The transmitting controller sends the received packet to the MAC layer.

Other objects advantages and novel features of the invention will become more apparent from the following detailed description when taken in conjunction with the accompanying drawings.

The receiving subsystem is coupled between the host interface and the network MAC interface such that a connection received by the network MAC interface can be classified into a priority connection fast connection or slow connection based on a pre stored information and activates its respective hardware circuit based on the connection classified thereby speeding up data transmission of the connection.

The transmitting subsystem is coupled between the host interface and the network MAC interface such that a connection received from the host interface can be transmitted to the network MAC interface . The host interface can be a PCI interface.

The receiving subsystem includes a cache static random access memory cache SRAM a receiving controller Rx controller a scorekeeper a content addressable memory CAM a receiving buffer Rx buffer a receiving queue Rx queue a memory allocator a connection information table CIT updater a CIT seeker a packet processing assistant PPA a microprocessor MCU a PCI controller a direct memory access receiving DMA Rx engine a descriptor maker a global register file a local bus a receiving queue management Rx queue management a DRAM controller and a receiving buffer controller .

The external DRAM stores packets for processing and a connection information table CIT . The cache SRAM stores priority connection information. The Rx controller is coupled to the network MAC interface to receive a connection. The scorekeeper is coupled to the Rx controller in order to handle accesses and associated use situations of the CAM . The CAM is coupled to the scorekeeper in order to perform fast searching on the cache SRAM for required priority connection information. The Rx controller defines destination and source s IP addresses and TCP ports which is extracted from a header of a packet that receives the connection as a connection identification CID and accordingly searches the CAM through the scorekeeper thereby determining if the connection is a priority connection.

The Rx buffer is coupled to the Rx controller in order to store a received packet. The Rx queue is coupled to the Rx controller in order to store a received packet or its payload.

The memory allocator is connection to the Rx controller in order to allocate the Rx buffer or addresses of available space in an external DRAM such that the Rx controller can move the payload in an appropriate storage space. The Rx controller sends a request to the memory allocator for temporarily storing the packet or its payload in the Rx buffer or the external DRAM .

The Rx controller re arranges a header of the received packet into a data structure of a receiving event Rx event and sends it to the Rx queue . The CIT updater produces a new connection information in the external DRAM . The CIT seeker is coupled to the Rx queue management in order to search the external DRAM for a desired fast connection information based on the Rx event in the CIT seeker queue and write a memory base address of the fast connection information stored to the Rx event. The packet processing assistant PPA is coupled with the Rx queue through the Rx queue management and obtains a packet destination information according to data stored in the PPA queue . The MCU is coupled to the local bus in order to handle a supported communication protocol.

The PCI controller is coupled to the PCI interface in order to transmit and receive data of the host side. The DMA Rx engine arranges a host destination address in the PPA or MCU adds it to a respective packet header information in the Rx queue and sends a packet or its payload with the respective packet header information to the host side through the PCI controller . The descriptor maker produces a receiving Rx descriptor corresponding to a packet or its payload and uses the PCI controller to send the Rx descriptor produced to the host side.

The transmitting subsystem includes a descriptor and command buffer a transmitting Tx queue a Tx buffer a header generator a Tx controller a DMA transmitting DMA Tx engine and a Tx queue controller .

The descriptors and command buffer stores Tx descriptors of a host transmitting connection for the MCU to further processing. The Tx queue stores the Tx descriptors processed by the MCU . The Tx buffer stores payload data of the host transmitting connection. The header generator produces a packet header of the host transmitting connection. The Tx controller sends a packet to the MAC layer.

The host interface can be a typical PCI PCI X interface and the like. The host interface and its control circuit are a standard interface communicating with a host. Applications on the host side use application programming interface API provided by the protocol offload engine system to send a network connection to be speeded to the OS. A device driver exchanges control information connection information payload packets of slow path connection and processing status with such as the global register file and memory of the system through the host interface and its control circuit.

For applying an offload function of the system offloaded connections are established. In this embodiment the offloaded connections are referred to as fast path connections or briefly fast connections. The host can establish a new fast connection through the system . The MCU completes an analysis of the control information and drives associated circuit to download connection information arranged by the host through the host interface to a connection information table CIT . Accordingly the host can refer the CIT and determine if a connection packet is a fast connection. The CIT is stored in the cache memory .

The CIT applies a two level management thereby obtaining a balance of performance and cost. The first level consists of a content addressable memory CAM and a static random access memory SRAM . The second level consists of an external dynamic random access memory DRAM having a slower access speed. An initial CIT is stored in the external DRAM . The SRAM is used to quickly fetch fast connections in the DRAM . The fast connections fetched are referred to as priority fast path connections or briefly priority connections.

Since the SRAM functions as a cache it is also referred to as a cache SRAM . Each of the priority connection has a connection identification CID for distinction and the CIDs are stored in the CAM . Accordingly the CAM in the first level can provide fast searching for the priority connections in the SRAM . The memory address of a CID in the CAM directly indicates to a block base address of a priority connection with the CID stored in the cache SRAM . Therefore the CAM can be used to quickly determine if a connection packet is a priority connection and obtain an address corresponding to the priority connection in the cache SRAM .

After the connection is established the host can start to use the system in packet generation. The host sends a control information which contains transmitting packet commands and associated information to the MCU through the host interface and the descriptors and command buffer for further analysis. After the analysis the MCU place an analyzed result in the Tx queue . The associated circuit performs the analyzed result based on a use situation of memory resources of the Tx buffer . The procedure performed includes that downloading payload arranged by the host through the host interface and sending the downloaded to the Tx buffer constructing a header according to a packet connection information assembling the header and the payload into a packet and updating the connection information. After the packet is complete the Tx controller appropriately sends the packet to a network through a network media access control MAC at an appropriate time point.

In a packet receiving side the network MAC interface receives the packet and sends the header to the Rx controller . The Rx controller uses a CID of the packet to check the content of the CAM for finding a desired priority connection. If the packet is belong to one of the priority connections of the cache SRAM and has a packet length not less than the maximum transmitting unit MTU its payload is stored in the built in Rx buffer of the Rx controller otherwise in the external DRAM temporarily. Available addresses of the Rx buffer or the DRAM are provided by the memory allocator . Accordingly the Rx controller can successfully move the payload in place. In addition the Rx controller arranges the header of the packet in the Rx buffer and adds other associated information such as temporary addresses of the payload. When the packet passes error detection such as checksum the arranged and added information is sent to the Rx queue for further processing. If a data error is detected in the packet the Rx controller discards the information stored in the Rx queue and requests the memory allocator to release the provided memory space.

To avoid over accumulation of the payload in the expensive Rx buffer that is caused by a low processing rate of the MCU it is necessary to effectively disperse the payload. To achieve this the packet processing assistant PPA is used to fast fetch a host destination information of the packet in order to rapidly forward the payload to a memory on the host while the protocol associated information of the packet in the Rx queue is processed at a lower rate. In addition the PPA can support some simple protocol routines such as an auto acknowledgement and other simple but high present protocol processing procedures. Complicated protocol processing procedures such as processing of optional fields quality of service QoS are completed by a firmware of the MCU .

The CIT records all connection information of the fast connections which is stored in the DRAM . There are k preferably 16 fast connections of the CIT are copied to the cache SRAM as priority connections. Also the corresponding CIDs of the k priority connections are placed in the CAM and thus the priority connections are recognized by the Rx controller at first time when a packet comes. is schematic diagram of establishment and update of a connection information table.

The connection information in the CIT is provided by the host. When the host desires to establish a new fast connection a conventional protocol stack on the host is applied to a memory of the host for establishing connection information associated with the new fast connection. A content of the connection information has partial information required by the system that is arranged into CIT descriptors by the host and placed in a separate memory space. When the descriptors are completely arranged the host records memory addresses which indicate a location where the CIT descriptors are placed to the global register file of the system through the host interface and sets a New Connection Establishment flag of the file to high to thus notice the MCU this event occurrence.

When the MCU finds that the flag is set to high it forwards the memory addresses of the CIT descriptors to the DAM Tx engine . The engine accordingly moves the CIT descriptors to the descriptors and command buffer . The CIT updater uses the CIT descriptors in the descriptors and command buffer to start to establish a new connection information in a CIT space of the DRAM . In addition if the cache SRAM still has free memory space the CIT update also establishes a connection information in the cache SRAM which is the same as the new connection information and has a CID. The CIT update registers the CID in the CAM through the scorekeeper . Next the CIT updater sets the New Connection Establishment flag of the global register file to low which indicates that the new connection is established and another new connection can start to be established.

The Rx controller performs first analysis and recognition on a received packet and extracts destination and source s IP addresses and TCP ports from the received packet to thus define the information as a CID. The CID is applied to the CAM for fast searching through the scorekeeper . If a matched CID in the CAM exists it indicates that the received packet is of a priority connection. The CAM accordingly responds a memory address to the Rx controller . The memory address records addresses of the cache SRAM for storing connection information of the received packet. If the matched CID does not exist it indicates that the connection information of the received packet is not stored in the cache SRAM but may be placed in the DRAM or even not established by the system .

According to a searching result of the CAM and a size of the received packet the Rx controller sends a request to the memory allocator for temporarily storing the packet or its payload to the Rx buffer or the DRAM . The memory allocator assigns appropriate buffer addresses in response. The Rx controller uses the buffer addresses assigned by the memory allocator to store the packet or its payload.

Briefly if a packet is of priority connection the Rx controller sends a payload of the packet to buffer area assigned by the memory allocator for temporary store. However if the packet is not of priority connection the Rx controller sends the entire packet to the buffer area assigned by the memory allocator for temporary store.

While a packet is processed the Rx controller re arranges a header of the packet into a data structure for internal processing and sends the header re arranged to the Rx queue . The data structure is referred to as an Rx event. The packet or its payload is sent to addresses assigned by the memory allocator for temporary store.

According to a searching result of the CAM and a size of a received packet the Rx controller sends a request to the memory allocator for temporarily storing the received packet or its payload to the Rx buffer or the DRAM . If the CAM contains a CID matched with that of the received packet it indicates that connection information of the received packet can be accessed quickly in the cache SRAM and protocol information contained in the header of the received packet can be processed quicker. In this case the memory allocator provides available addresses of the Rx buffer to thus temporarily store the payload of the received packet.

On the contrary if the CAM does not contain the CID matched with that of the received packet it indicates that the cache SRAM has not the connection information. Moreover the connection information may not be established in the CIT of the DRAM namely the received packet may not be processed effectively and quickly. In this case the Rx controller requests the memory allocator to provide current available addresses of the DRAM for temporarily storing the received packet. Accordingly the Rx buffer of the system is used to reserve the priority connections in the cache SRAM .

When a received packet has a size smaller than the network MTU defined by the LAN the packet belongs to an odd transmission not a large transmission. In order to have the best use the Rx buffer is generally applied in large data or packet transmission. Therefore even the CID of the packet is recorded in the CAM the Rx controller typically requests the memory allocator to provide such an odd packet transmission with available addresses of the DRAM for temporarily store. However when the Rx buffer has large use space the memory allocator still responds available memory addresses of the Rx buffer to the Rx controller . In this case the Rx controller temporarily stores the packet or its payload in the Rx buffer .

In addition the memory allocator can ignore the request from the Rx controller and force the Rx controller to temporarily store the packet or its payload in the DRAM which occurs only when the Rx buffer has memory space insufficient or surplus.

After a packet header is arranged into a Rx event the Rx controller places the Rx event in the Rx queue by segmentation. is a schematic diagram of pipelining of the receiving queue . The Rx queue is logically shared by five queues. The five queues consist of a CIT seeker queue a PPA queue a MCU queue a DMA Rx queue and a descriptor maker queue which are connected in series and used respectively by the CIT seeker the PPA the MCU the DMA Rx engine and the descriptor maker . The five processing modules i.e. the devices thus obtain pipelining mechanism through the five queues and reduce the use of the MCU local bus.

As shown in the Rx controller places the Rx event in the CIT seeker queue or the PPA queue by segmentation. When the CIT seeker and the PPA completely process their respective queues the Rx event flows to the MCU the DMA Rx engine and the descriptor maker sequentially. A path A in provides the PPA capable of skipping the MCU queue and directly sending a host destination information to the DAM Rx queue .

The PPA essentially handles packets that present CIDs matched in the CAM i.e. the packets are of the priority connections of the cache SRAM. For such packets the PPA can assist the MCU in processing a part of high regularity and repeat protocol processing events but the Rx events are still processed by the MCU . In addition the speedup of the PPA can allow the predication of a memory size of the Rx buffer to be easier which can be appreciated in and .

The most complicated task of the system in the receiving part is to perform protocol processing on the header of each packet particularly on optional fields of the header. Due to high variety of the optional fields the system uses the MCU to process the various optional fields. In suppose that the system only uses the MCU to handle any situation to be happened in the protocol processing of each packet header and x system clock cycles are required for temporarily storing a received packet in the buffer of the system and moving the received packet to the host respectively. If the MCU requires y system clock cycles for processing the header of a current packet and y is greater than x it is still processing a previous header after the current packet is stored completely. In this case under the same packet input and output rate the buffer in the system will be insufficient to store packets or their payloads sooner or later.

To overcome this the PPA also provides a function of reducing packets to be output by quickly finding the host destination information of a packet and accordingly skipping the MCU to directly send the host destination information to the DMA Rx queue as shown in the path A of . Thus the DMA Rx can send the packet or its payload to the host earlier. The reduction effect is shown in in which packets accumulated in the buffer can be quickly forwarded to the host without continuous accumulation in the system because a short processing time for the PPA does not cause a bottleneck of packet output thereby reducing packet number accumulated. As compared to an appropriate size required for the buffer can be estimated easily. After the packets or their payloads are moved to the host with the help of the PPA the MCU may be still processing the header information in the Rx queue but since a memory cost of Wait To Be Processed is successfully eliminated from the Rx buffer the remaining cost is fallen only in the Rx queue .

Accordingly it is clear that the MCU is the last solution of the protocol processing. The MCU can process all communication protocols supported by the system such as IP TCP ARP and the like. Regarding to protocols not supported by the system the MCU has no operation except for periodic network running analysis to ensure no hacker attack. In this case the PPA contributes a packet being moved to the host for further processing by a conventional protocol stack.

The PPA quickly processes only the priority connection packets in the cache SRAM and packet header information not matched with that of the CAM is not moved to the PPA sub queue by the Rx controller but moved to the CIT seeker queue . The CIT seeker searches the CIT of the DRAM for matching a fast connection information with a Rx event of the CIT seeker queue and stores a base memory address of the fast connection information in the Rx event for further processing by the MCU . The MCU accesses the connection information of the packet and accordingly determines whether or not the connection information is copied to the cache SRAM to be a priority connection. If the connection for the packet is determined to be the priority connection the CIT updater also registers a CID of the connection in the CAM through the scorekeeper in addition to the copy thereby completing an establishment procedure of the priority connection.

As cited only the MCU and the PPA access the content of connection information in the DRAM or the cache SRAM . In addition in this embodiment a process of write back not write through is applied to cache management for the cache SRAM .

For write through cache management a priority connection or fast connection has to find its CIT address in the CIT of the DRAM . In addition when updating data connection information associated with both the DRAM and the cache SRAM has to be updated concurrently. As such if the priority connections of the cache SRAM have very high probability of read and write write through increases huge signal switching and occupies numerous bandwidth of the DRAM and further reduces processing rate. However with contrast to write through if the priority connections of the cache SRAM also have very high probability of read and write write back only occupies a bandwidth of connection between the cache SRAM and the PPA or the MCU and the DRAM is used only when a priority connection is updated. Therefore write back costs lower than write through and this is the reason that write back is applied.

The DMA Rx engine sends packet or its payload corresponding to a packet header information to a host memory through the PCI controller after the PPA or the MCU arranges a host destination address and places it in the packet header information recorded in the Rx queue . After the packet or its payload is sent to the host memory the descriptor maker generates a Rx descriptor for the packet or its payload and sends the Rx descriptor to a host Rx queue of the host memory through the PCI controller . All descriptors accumulated in the Rx queue are processed by the central processing unit CPU in batches after an interrupt coalescing event of the host side occurs thereby completing the entire packet receiving flow.

One of two logical DRAMs in stores the CIT and the other stores packet or its payload . However in fact the two data types are placed in a same physical DRAM but different memory spaces and controlled by a DRAM controller . Because multiple modules share the DRAM a DRAM management not shown is introduced to distribute corresponding DRAM bandwidth.

The spaces of the DRAM that stores CIT and packet or its payload are referred to as a CIT space and a buffer space respectively. In addition the DRAM has one more space for managing the buffer space which is referred to as a list space.

The system has five processing modules associated with the receiving configuration to use the Rx queue in successive processes for pipelining. The five processing modules may need different packet header information. Accordingly resources used in the successive processes can be arranged appropriately to thus obtain more effective control. A receiving Rx event is a data structure established in the Rx controller based on the header of a received packet. Such a data structure is designed by referring the hardware features of the system to thus effectively use memory resource and increase the take over performance of the five processing modules.

As shown in the packet is divided into two parts in the Rx controller . Namely the header of the packet forms the Rx event and is temporarily stored in the Rx event buffer and the packet or its payload is directed by the in stream distributor into an assigned buffer. When the header and payload of the packet pass all error detection the Rx event is sent by the event buffer controller to the Rx queue to be a valid Rx event. Conversely if any error of the packet is detected the Rx event is discarded to be a false Rx event and remained in the Rx event buffer .

The Rx event buffer consists of three physical SRAMs each having a size of 128 bytes to contain a Rx event having option fields. Accordingly the Rx event buffer can store at most three Rx events to be sent to the Rx queue at a time which is of the worst case in predication. In addition the Rx event buffer is managed by the event buffer controller in a way of allowing the three SRAMs to input and output the Rx events in a form of ring queue.

As cited according to required header fields are classified into Event Element A D and O. is a relational table of the Rx event elements and corresponding processing devices according to the invention. In the use time of each event element in the Rx queue can be known. Since the Rx queue distributes memory space in a unit of event element the size of a unit memory block in the Rx queue is the same as that of an event element.

As shown in when an event element is not required by a subsequent processing module or device memory space occupied by the event element is released. Such a release mechanism first regards an address of the unit memory block of each event element as an element of the list of data structure and then manages the five queues corresponding to the processing modules or devices by their respective queue lists and corresponding connective form shown in . is a schematic diagram of a receiving event element flow in the Rx queue according to the invention. In addition to the management of the five queue lists a free queue list is added to manage free unit memory block space in the Rx queue .

For receiving and releasing the receiving event element in the Rx queue the six lists in can be stored in a same separate SRAM. Each list can exist only one receiving event element that is an address of next element to be linked. In addition the address of each element can be mapped to a unit block storing an event element.

An Rx event contains several Rx event elements and thus needs to occupy a plurality of memory blocks in the Rx queue . As shown in some Rx event elements can be released sequentially with the Rx event elements respectively processed by the five processing modules in relay. Namely used memory blocks in the Rx queue can be released one after another as the five processing modules take over their respective processing in sequence. As such the number of queued Rx events in the Rx queue is increased.

Initially the status of heads tails and lengths of the six lists in can be recorded respectively in registers of while link status of the six lists is recorded in the Rx queue list.

When the Rx controller finds a current packet having a priority connection matched to that of the CAM it places a base address of the priority connection in the cache SRAM in an Rx event of the current packet and pushes the Rx event into the Rx queue . While the Rx event is processed sequentially by the five processing modules i.e. devices through the Rx queue the PPA and the MCU require accessing the cache SRAM . At this point the priority connection corresponding to the Rx event cannot be replaced otherwise the PPA and or the MCU may access a wrong connection. Namely priority connections corresponding to all Rx events in the Rx queue cannot be replaced unless a priority connection is not required by any Rx event in the Rx queue .

To ensure that devices that use the cache SRAM can access accurate priority connection information the scorekeeper is provided which is a controller with a function of scoring and positioned by the CAM to thus record use status of the priority connections as well as search the CAM for required CIDs. The use status recorded in the scorekeeper has two types first recording total number of a priority connection locked briefly referring to as total locked number second recording current number of the priority connection currently locked by the devices briefly referring to as current locked number . The total locked number information can provide the CIT updater to find a priority connection with the least use as a replaced candidate. The current locked number information can be used to assure accurate accessing of the priority connection. As shown in or the scorekeeper is connected to the Rx controller the DMA Rx engine a header generator and the CIT updater .

When the Rx controller receives a new coming packet it searches the CAM through the scorekeeper for a priority connection matched. If the priority connection matched is found the scorekeeper sends an address of the priority connection in the cache SRAM to the Rx controller and adds two registers corresponding to the priority connection respectively by one. The two registers record the total locked number and the current locked number respectively.

When an Rx event corresponding to the packet is sent to the DMA Rx engine the Rx event does not require accessing the cache SRAM and the DMA Rx engine signals the scorekeeper to reduce the current locked number register by one.

In addition the header generator requires the connection information of the cache SRAM so as to search the CAM through the scorekeeper for a CID matched. If the CID matched is found total locked number and current locked number corresponding to a priority connection with the CID matched are added by one. When the header generator need not the priority connection information used it signals the scorekeeper to reduce the current locked number by one as well.

The CIT updater checks the cache SRAM to see if the connection information of current locked number equals to zero before performing a write back operation on the cache SRAM . The CIT updater performs the write back operation when the current locked number equals to zero. Conversely the CIT updater sends a the priority connection busy message to the MCU and give up the write back operation. For a special situation such as a new priority connection to be established the MCU can force a write back operation and thus establish the new priority connection in the cache SRAM . However this may cause the Rx controller not to receiving a new coming packet until the connection information of current locked number in the cache SRAM is decreased to zero and the new priority connection is established completely.

The memory space of the Rx buffer is allocated in a unit block of 512 bytes. For a size of 64K bytes the Rx buffer can be divided into 128 unit blocks addressed by seven bits. When the Rx buffer is managed by listing it requires a 128 7 bit memory to store corresponding list i.e. the system will reserve 1K bytes to manage the Rx buffer .

The buffer space of the DRAM is in a unit block of 2048 bytes. If a 64M bytes space of the DRAM is used to temporarily store packet payloads the space can be divided into 215 unit blocks addressed by 15 bits. When listing is applied for management it requires 215 15 bytes 60K bytes which cannot be placed in the system in a form of SRAM but is placed in a list space of the DRAM .

The memory allocator responds an assigned buffer based on a request sent by the Rx controller except that one of the following conditions occurs. First the buffer space of the DRAM is arranged compulsorily as the assigned buffer when the Rx buffer has no space. Second the Rx buffer is arranged compulsorily as the assigned buffer when its use rate is lower than P 100 P 0 .

As cited a unit block in the Rx buffer is not enough to contain a packet or its payload greater than 512 bytes and thus a segmentation operation is required for temporary storing. For example a MTU size packet in the Ethernet is distributed typically into three unit blocks to temporarily store. Further Gigabit level Ethernet supports a Jumbo Frame which needs more unit blocks to temporarily store packet or its payload of the Jumbo Frame. The unit blocks for temporary storing are linked and managed by an expression of list in data structure.

When a content of the used block is not referred and the used block can be released into the free block queue the queue or DRAM management provides a first and a last block addresses storing a current packet or its payload. Accordingly all blocks used can be released sequentially to the free block queue through the first block address the last block address and the used block queue.

When the host desires to send data it places a transfer data or packet set to be a fast path connection in a host memory and informs the system of a Data or Packet Ready message through a driver and the host interface . Next after the MCU receives the message through the driver it asks the DMA Tx engine to read a transmitting Tx descriptor from the host memory to the descriptors and command buffer . Next the MCU processes the Tx descriptor and sends a result such as each segment s base address length . . . etc. after data segmentation to the Tx queue .

On system initialization or connection establishment for a current transmission the MCU determines to support Jumbo Frame or not and sets corresponding flags. The MCU first checks the flags in the protocol processing engine system for finding 9K or 16K Jumbo Frame supported or not. If supported the MCU sets a field of the Tx descriptor to be a Jumbo Frame field. Next the MCU stores the Tx descriptor with the Jumbo Frame field in the Tx queue and determines an appropriate transmitting frame size as 1.5K 9K or 16K bytes. Next the DMA Tx moves data from an address recorded in the transmitting frame to the Tx buffer . The header generator generates a header to be transmitted. When the data is moved completely and the header is generated a complete packet is generated for the Tx controller to send it out. is a schematic diagram of a transmitting configuration according to the invention.

The CIT management mechanism performs comparison on the current transmission to determine if the current transmission is one of the fast paths set while the MCU determines the transmitting frame size. If the current transmission is a fast path transmission the MCU computes sizes and base addresses of fixed data block lengths and stores the base addresses in the Tx queue which is referred to as segmentation. Transmitted data is segmented to obtain a transmitting frame size of packets. Next segmented information is stored in the Tx queue. If the Tx queue is full the information is stored in the DRAM until the Tx queue has free space. If the current transmission is a slow path transmission the MCU does not perform the segmentation and the transmitting frame size is determined as 1.5K bytes.

The Tx queue controller manages the Tx queue . Because transmitted data flow can be easily predicted and managed the Tx queue can be a FIFO or ring buffer. For a 1 Gbps to 10 Gbps network bandwidth the Tx queue can be designed to have three to five queues with different frame sizes of 64K 32K 16K 9K and 1.5K bytes. The Tx queue controller manages R W and status indication of the Tx queues .

The DMA Tx engine computes a checksum of packets or their payloads when moving them and sends a result to the header generator . The DMA Tx engine asks the memory allocator for space and addresses of the Tx buffer before reading transmitting description information of the Tx queue . The memory allocator distributes the addresses to the DMA Tx engine and thus the DMA Tx engine reads transmitted payloads segmented transmitted segments and stored in the host memory through the DMA based on the information of the Tx queue . Next the DMA Tx engine writes each transmitted segment to the Tx buffer and reserves some space for writing a packet header into each transmitted segment by the header generator .

After the transmitted segments are written in the Tx buffer the header generator generates a TCP IP header to add in front of each packet data. An information of the header is extracted from the transmitting description information and the CIT and accordingly performs the addition of the header. If the connection is a slow path the header generator does not add the header.

The Tx controller sends the packet processed by the header generator to the MAC. After the packet is sent the Tx controller informs the memory allocator of releasing the buffer occupied by the packet and placing a wait for acknowledge information corresponding to the packet in a wait for acknowledge queue.

The Tx buffer can be a SRAM. The SRAM is divided into a transmitting Tx page size of blocks such that the Tx buffer can be manageable by using a bitmap management to directly determine each page used or not. For example if the Tx buffer has 16K bytes and a page of 512 bytes blocks can be managed directly by 32 bits an internal microprocessor with a word of 32 bit to fast allocate and release. When the capacity of each page is greater than 512 bytes the link of multiple blocks uses several bits on the tail of a page as information of linking blocks.

A routine circuit for packet transfer as a data checksum generation is applied to each packet. The invention uses the DMA Tx engine to move transmitted Tx payloads from the host to the Tx buffer and concurrently forward to the checksum generation for immediately computing a packet checksum without waiting all payloads stored thereby speeding offloading. Thus all payloads are stored and concurrently a corresponding checksum is obtained. Next the checksum is sent to the header generator to generate a corresponding checksum for all headers.

The interleaved memory uses multiple SRAM modules to implement parallel access. If a port s SRAM module has a bandwidth of b MHz of m SRAM modules have a total bandwidth of m b MHz. In addition data in successive addresses of the interleaved memory is distributed to the SRAM modules by interleaving. Interleaved distribution can avoid a single device occupying one SRAM module too long which causes other devices to queue up for using it. In this embodiment full duplex bandwidth switching is obtained by space interleaving and time interleaving.

The interleaved shown in is referred to as low order interleaved. This is because such an interleaved uses some least weight bits to determine a memory module to access i.e. bank selection shown in . The remaining higher weight bits can determine a memory unit s to access i.e. unit selection shown in .

As shown in the 4 to 4 switch can switch different I O ports to corresponding addresses of the physical SRAM modules for data access thereby achieving space interleaving of the physical SRAM modules. The scheduler arranges use timing of the I O ports for SRAM module access requests thereby achieving time interleaving.

The interleaved memory is suitable for data access to successive addresses if the use number is smaller than or equal to the number of physical memory modules. As such multiple devices can use different physical memory modules at a same time thus a parallel effect is obtained. For data access to non successive addresses a higher memory use rate cannot be obtained but the time to wait for data access right is deterministic. Therefore the system accordingly applies the memory design way of timing and module interleaved in its memory modules.

In summary the invention uses high efficient content addressable memory CAM to manage connections that currently has the highest demand for speeding i.e. priority connections and are temporarily stored in a middle speed SRAM. In addition the remaining fast connections are temporarily stored in a low cost DRAM. Such a graded access management can lower cost and also obtain preferred whole performance.

Although the present invention has been explained in relation to its preferred embodiment it is to be understood that many other possible modifications and variations can be made without departing from the spirit and scope of the invention as hereinafter claimed.

