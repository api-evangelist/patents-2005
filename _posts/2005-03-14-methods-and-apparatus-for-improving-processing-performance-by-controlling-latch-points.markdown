---

title: Methods and apparatus for improving processing performance by controlling latch points
abstract: Methods and apparatus provide for performing pre-execution processes to prepare instructions of an instruction set for further processing; executing the instructions in a pipeline of execution stages using digital logic for processing data in accordance with the instructions within one clock cycle per stage; latching the data each clock cycle for delivery to a next execution stage using one or more of a plurality of latch point circuits; and controlling each of the latch point circuits to operate as a buffer or as a latch.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07395411&OS=07395411&RS=07395411
owner: Sony Computer Entertainment Inc.
number: 07395411
owner_city: Tokyo
owner_country: JP
publication_date: 20050314
---
The present invention relates to methods and apparatus for improving processing performance by controlling the latch points in a processing system such as a pipelined system.

In recent years there has been an insatiable desire for faster computer processing data throughputs because cutting edge computer applications involve real time multimedia functionality. Graphics applications are among those that place the highest demands on a processing system because they require such vast numbers of data accesses data computations and data manipulations in relatively short periods of time to achieve desirable visual results. These applications require extremely fast processing speeds such as many thousands of megabits of data per second. While some processing systems employ a single processor to achieve fast processing speeds others are implemented utilizing multi processor architectures. In multi processor systems a plurality of sub processors can operate in parallel or at least in concert to achieve desired processing results.

Semiconductor process technologies increase about every 18 months with the current process being 90 nm. With the increase in process technology comes an increase in processing frequency and resultant increase in power dissipation. Although the increase in frequency improves processing performance the increase in power dissipation is not desirable. Although some have proposed decreasing the operating voltage to reduce the power dissipation this has an undesirable complication the leakage current increases.

One or more embodiments of the present invention may provide for improving processing performance in new processing technologies without increasing the frequency of operation thereby controlling power dissipation. In accordance the invention the frequency of operation is reduced while the programmer is provided with the ability to control the depth of the pipeline the position and number of the latch points and or the location and number of the clock distribution points.

The processing system may employ a plurality of configurable latch buffers which are each operable to perform a latch function or a buffer function depending upon a control signal. The latch buffers are preferably disposed at a pitch corresponding to a fabrication metric of the processor and at higher pitches such as the 10F04 metric the 15F04 metric the 30F04 metric etc. For example the 10F04 metric of the 90 nm process dictates that latches are functionally positioned relatively close together. The 15F04 metric dictates that latches are functionally positioned further apart as compared to the 10F04 metric. And the 30F04 metric dictates that latches are functionally positioned even further apart. As each latch buffer can operate as a latch or as a buffer the control signals can establish the number and extent of the latch points in the system.

In response to API code the processing system is preferably operable to adjust the number of execution stages in the pipeline and also establish proper signaling to the latch buffers to match the number and position of the latch points to the execution stages. For example as the number of execution stages decreases the number of latch points may decrease thereby reducing power dissipation. Further the system may disable a number of clock distribution points in the system to match the decrease in execution stages thereby further reducing power dissipation.

In accordance with one or more aspects of the present invention a processing system includes an instruction pre execution circuit operable to prepare instructions of an instruction set for further processing and an instruction execution circuit having a plurality of execution stages operable to execute the instructions in a pipeline fashion using one clock cycle per stage each execution stage including digital logic for processing data in accordance with the instructions and at least one latch point circuit adapted to latch the data each clock cycle for delivery to a next execution stage. The number of execution stages of the instruction execution circuit is variable in response to application programming interface API code invoked by a software program running on the processing system.

Each of the latch point circuits may be operable as a buffer or as a latch in response to control signaling. Preferably the latch point circuits are operable as a buffer or as a latch in response to commands from API code.

The number of execution stages of the instruction execution circuit is preferably controllable by API code. By way of example the number of execution stages of the instruction execution circuit may be controllable by controlling the number of latch point circuits operating as buffers and the number of latch point circuits operating as latches.

The processing system preferably includes a plurality of clock distribution nodes that are operable to be enabled and disabled. The clock distribution nodes associated with the latch point circuits operating as buffers are preferably disabled while those associated with latch point circuits operating as latches are preferably enabled. The clock distribution nodes may be enabled and disabled as a function of API code.

In accordance with one or more further aspects of the present invention a method includes fabricating a processor using a fabrication process of X nano meters which is an advanced process over a Y nano meter process operating the processor at a frequency of F despite that the X nano meter process would permit a frequency of operation of greater than F such that power dissipation is reduced and adjusting a number of execution stages of the instruction execution circuit in response to application programming interface API code invoked by a software program running on the processing system to counter a trend of reduced processing power resulting from the lower frequency of operation.

In accordance with one or more further aspects of the present invention a method includes performing pre execution processes to prepare instructions of an instruction set for further processing executing the instructions in a pipeline of execution stages using digital logic for processing data in accordance with the instructions within one clock cycle per stage latching the data each clock cycle for delivery to a next execution stage using one or more of a plurality of latch point circuits and controlling each of the latch point circuits to operate as a buffer or as a latch.

In accordance with one or more further aspects of the present invention a storage medium contains at least one software program capable of causing a processing system to perform actions comprising performing pre execution processes to prepare instructions of an instruction set for further processing executing the instructions in a pipeline of execution stages using digital logic for processing data in accordance with the instructions within one clock cycle per stage latching the data each clock cycle for delivery to a next execution stage using one or more of a plurality of latch point circuits and controlling each of the latch point circuits to operate as a buffer or as a latch.

Other aspects features advantages etc. will become apparent to one skilled in the art when the description of the invention herein is taken in conjunction with the accompanying drawings.

With reference to the drawings wherein like numerals indicate like elements there is shown in at least a portion of a processing system that may be adapted for carrying out one or more features of the present invention. For the purposes of brevity and clarity the block diagram of will be referred to and described herein as illustrating an apparatus it being understood however that the description may readily be applied to various aspects of a method with equal force.

The processing system is preferably implemented using a processing pipeline in which logic instructions are processed in a pipelined fashion. Although the pipeline may be divided into any number of stages at which instructions are processed the pipeline generally comprises fetching one or more instructions decoding the instructions checking for dependencies among the instructions issuing the instructions and executing the instructions. In this regard the processing system may include an instruction buffer not shown an instruction fetch circuit an instruction decode circuit a dependency check circuit instruction issue circuitry not shown and instruction execution stages .

The instruction fetch circuitry is preferably operable to facilitate the transfer of one or more instructions from a memory to the instruction buffer where they are queued up for release into the pipeline. The instruction buffer may include a plurality of registers that are operable to temporarily store instructions as they are fetched. The instruction decode circuit is adapted to break down the instructions and generate logical micro operations that perform the function of the corresponding instruction. For example the logical micro operations may specify arithmetic and logical operations load and store operations to the memory register source operands and or immediate data operands. The instruction decode circuit may also indicate which resources the instruction uses such as target register addresses structural resources function units and or busses. The instruction decode circuit may also supply information indicating the instruction pipeline stages in which the resources are required.

The dependency check circuit includes a plurality of registers where one or more registers are associated with each execution stage of the pipeline. The registers store indications identification numbers register numbers etc. of the operands of the instructions being executed in the pipeline. The dependency check circuit also includes digital logic that performs testing to determine whether the operands of an instruction for entry into the pipeline are dependent on the operands of other instructions already in the pipeline. If so then the given instruction should not be executed until such other operands are updated e.g. by permitting the other instructions to complete execution .

The instruction execution circuitry preferably includes a plurality of floating point and or fixed point execution stages to execute arithmetic instructions. Depending upon the required processing power a greater or lesser number of floating point execution stages and fixed point execution stages may be employed. It is most preferred that the instruction execution circuitry as well as the other circuits of the processing system is of a superscalar architecture such that more than one instruction is issued and executed per clock cycle. With reference to any given instruction however the execution circuitry executes the instructions in a number of stages where each stage takes one or more clock cycles usually one clock cycle. Further details concerning the structure and operation of the instruction execution circuit will be discussed hereinbelow.

Reference is now made to which is a graphical illustration of certain performance parameters of the system of in accordance with one or more aspects of the present invention. While the present invention is not limited to any theory of operation it has been discovered that advantageous operation of the system as discussed hereinabove may be achieved when these performance characteristics are taken into consideration during the fabrication design implementation and programming phases of the development of the system. The graph of shows time along the abscissa axis and relative changes in magnitude along the ordinate axis. The plotted magnitudes as a function of time include the available fabrication processes for semiconductor processing systems a propagation metric for the fabrication process the potential frequency of operation of the process and the power dissipation of a system operating at such frequency.

The semiconductor fabrication process technologies advance about every 18 months where the state of the art process is 90 nm. Future fabrication processes will likely be 65 nm 45 nm etc. As the fabrication process advances over time the frequency of operation of a processing system employing the fabrication process increases in a corresponding fashion. The increase in operating frequency generally improves the processing performance of a system however such increase in frequency is accompanied by an increase in power dissipation which is not desirable. The propagation metric also improves as a function of the fabrication process advancement.

With reference to the propagation metric of interest here is the theoretical signal propagation delay through a series of logic gates fabricated in accordance with the fabrication process. For the purposes of discussion herein the signal propagation delay is compared against a specific time period such as one clock cycle. A 1F04 propagation metric indicates that the propagation delay through a single stage of inverter logic gate s takes one clock cycle. A 2F04 propagation metric indicates that the single propagation delay through two stages of inverter logic gates takes one clock cycle. A 3F04 propagation metric indicates that the single propagation delay through three stages of inverter logic gates takes one cycle and so on. Thus an advancement in the fabrication process from the 90 nm process to the 65 nm process results in a significant improvement in the propagation metric such as from 10F04 to 15F04 or 20F04 etc.

With reference to the instruction execution circuit preferably includes a plurality of execution stages labeled STAGE STAGE etc. Each instruction execution stage includes digital logic for processing data in accordance with the instructions being executed. The instruction execution circuit also includes at least one latch point circuit adapted to latch the data each clock cycle for delivery to a next execution stage. For example if the STAGE execution stage carries out an ADD micro operation using its digital logic the resultant data are preferably stored in the latch point circuit for delivery to a next execution stage and or to some other point in the pipeline.

The latch point circuits are preferably disposed at a sufficient pitch such that the data may propagate through the digital logic between successive latch point circuits within one clock cycle. In the example illustrated in the processing system is assumed to have been fabricated utilizing a fabrication process having an 30F04 propagation metric. It is noted that this is an arbitrary fabrication process for illustration purposes only and is not intended to imply that such a process has been employed experimentally or commercially. Irrespective of the particular fabrication process employed the propagation metric of the fabrication process dictates a minimum pitch for the latch point circuit . This minimum pitch is illustrated in as being the pitch between the STAGE execution stage and the STAGE execution stage each of which includes a designator of 30F04.

In accordance with one or more aspects of the present invention the latch point circuits are preferably disposed at a pitch higher than the minimum pitch. In particular one or more of the execution stages are disposed at a pitch corresponding with a less advanced fabrication process such would be associated with a 15F04 10F04 or 5F04 propagation metric. For example the STAGE STAGE STAGE and STAGE execution stages have latch point circuits disposed at a pitch corresponding with a less advanced fabrication process having a propagation metric of 15F04. Similarly the STAGE STAGE STAGE STAGE and STAGE execution stages have latch point circuits disposed at a pitch corresponding with a less advanced fabrication process having a propagation metric of 10F04. Finally the latch point circuits associated with STAGE STAGE STAGE STAGE STAGE . . . STAGE are disposed at a pitch corresponding with a fabrication process having a propagation metric of 5F04.

In accordance with one or more further aspects of the present invention each of the latch point circuits is preferably operable to act as a buffer or as a latch in response to control signaling. is block diagram illustrating this functionality of the latch point circuits . In particular each latch point circuit preferably includes a buffer circuit a latch circuit and a control switch circuit . When the control signaling is such that the control switch circuit engages a left node then the data into the latch point circuit flows through the buffer circuit . Alternatively when the control signaling causes the control switch circuit to engage the right node then the data is subject to the latch circuit whereby the data are latched in accordance with a clock.

In a preferred embodiment the control signaling is responsive to application program interface API code that are invoked by a software program running on the processing system . Advantageously this permits a software designer to change the configuration of a given latch point circuit or a group of latch point circuits by way of software running on the processing system . When the latch point circuit operates as a buffer the data are not held up by a latch but rather continue through to the data output node of the latch point circuit for subsequent use. As will be discussed hereinbelow this functionality may be utilized in conjunction with varying the depth of the pipeline e.g. by varying the number of execution stages in the instruction execution circuit .

Reference is now made to which is a block diagram illustrating further details of the instruction execution circuit of the processing system . When the latch point circuit operates as a buffer there is no need to provide a clocking signal to the latch circuit . Indeed the buffer circuit does not require a clock to perform its function. Consequently careful routing of clock signals utilizing a clock distribution network yields advantageous results.

A system clock signal is often used by digital circuitry such as digital circuitry implemented using a LSI circuit to synchronously execute certain logic functions. The system clock signal of a given LSI circuit is often split into many paths to service many different portions of the digital circuitry. Ideally the system clock signals at different portions of the digital circuitry exhibit exactly the same timing characteristics so that the different portions of the digital circuitry operate in exact synchronization. In practice however the system clock signals at various points throughout the digital circuitry exhibit differing timing characteristics such as differing rising and or falling edges i.e. transitions differing duty cycles and or differing frequencies. This problem is countered by employing a plurality of clock distribution nodes located on the semiconductor chip that ensure synchronous delivery of the system clock to various portions of the LSI circuit.

The processing system of preferably employs a plurality of clock distribution nodes that are operable to deliver system clock signaling to various portions of the system . In at least some of these clock distribution nodes are labeled CLK DIST A CLK DIST B CLK DIST C and CLK DIST D. The clock signaling from the distribution node CLK DIST A is input to the STAGE and STAGE instruction execution stages. It is noted that the latch point circuits of these stages are disposed at a pitch consistent with the 30F04 propagation metric. The clock signaling from distribution node CLK DIST B is input to the STAGE and STAGE instruction execution stages. It is noted that the latch point circuits associated with the execution stages receiving clock signaling from nodes CLK DIST A and CLK DIST B are disposed at a pitch corresponding with the 15F04 propagation metric. The clock signaling from distribution node CLK DIST C is input to the STAGE STAGE and STAGE instruction execution stages. It is noted that the latch point circuits associated with the instruction execution stages receiving clock signaling from nodes CLK DIST A CLK DIST B and CLK DIST C are disposed at a pitch corresponding with the 10F04 propagation metric. Finally the clock signaling from the distribution node CLK DIST D is input to the STAGE STAGE and STAGE execution stages. It is noted that the latch point circuits associated with the instruction execution stages receiving clock signaling from all clock distribution nodes are disposed at a pitch corresponding to the 5F04 propagation metric.

In accordance with one or more aspects of the present invention the clock distribution nodes are preferably operable to be enabled and disabled. For example the clock distribution nodes associated with latch point circuits operating as buffers are preferably disabled. On the other hand the clock distribution nodes associated with the latch point circuits operating as latches are preferably enabled. The enabling and disabling of the clock distribution nodes is preferably achieved as function of API code such that a programmer may utilize one or more software programs running on the processing system to control the number and extent of the clock distribution nodes. This advantageously permits the programmer to assist in limiting power dissipation of the processing system inasmuch as power dissipation is reduced as the number of clock distribution nodes are disabled.

In accordance with one or more further aspects of the invention the depth of the pipeline is variable e.g. the number of execution stages of the instruction execution circuit is preferably variable. For example it may be desirable to employ a shorter pipeline in some circumstances because this improves processing performance in the face of branch misses cache misses and or interrupts. Giving the programmer the option to select the depth of the pipeline using API code provides a mechanism for employing a shorter or longer pipeline depending on whether branch misses cache misses and or significant numbers of interrupts are expected.

The number of execution stages of the instruction execution circuit is preferably variable in response to application programming interface API code invoked by a software program running on the processing system .

With reference to in accordance with one or more aspects of the present invention the processing system is fabricated utilizing an advanced fabrication process of for example 65 nm as opposed to 90 nm action . Counter to the conventional wisdom however the frequency of operation of the processing system is not increased to the theoretical level associated with the advanced fabrication process. Rather the frequency of operation is established at a lower level such as the level associated with the previous fabrication process e.g. the theoretical maximum frequency associated with the 90 nm process action . In order to counter the trend toward a lower processing performance due to the lower or non maximized frequency of operation the depth of the pipeline e.g. the number of execution stages is altered to account for the likelihood of branch misses cache misses interrupts etc. action . When the depth of the pipeline decreases with the number of execution stages decreasing the number of latch point circuits may likewise be decreased. This is preferably achieved by controlling the latch point circuits to operate as buffers. Improved processing in the presence of for example branch misses may be achieved while simultaneously reducing the number of latch point circuits operating as latches. This permits one or more clock distribution nodes to be disabled thereby reducing power dissipation.

Further features that may be employed to improve processing performance while reducing power dissipation in a processing system may be found in co pending U.S. patent application Ser. No. 11 079 566 entitled METHODS AND APPARATUS FOR IMPROVING PROCESSING PERFORMANCE USING INSTRUCTION DEPENDENCY CHECK DEPTH filed on Mar. 14 2005 the entire disclosure of which is incorporated herein by reference.

The local memories are preferably located on the same chip same semiconductor substrate as their respective processors however the local memories are preferably not traditional hardware cache memories in that there are no on chip or off chip hardware cache circuits cache registers cache memory controllers etc. to implement a hardware cache memory function.

The processors preferably provide data access requests to copy data which may include program data from the system memory over the bus into their respective local memories for program execution and data manipulation. The mechanism for facilitating data access is preferably implemented utilizing a direct memory access controller DMAC not shown. The DMAC of each processor is preferably of substantially the same capabilities as discussed hereinabove with respect to other features of the invention.

The system memory is preferably a dynamic random access memory DRAM coupled to the processors through a high bandwidth memory connection not shown . Although the system memory is preferably a DRAM the memory may be implemented using other means e.g. a static random access memory SRAM a magnetic random access memory MRAM an optical memory a holographic memory etc.

Each processor is preferably implemented using a processing pipeline in which logic instructions are processed in a pipelined fashion. Although the pipeline may be divided into any number of stages at which instructions are processed the pipeline generally comprises fetching one or more instructions decoding the instructions checking for dependencies among the instructions issuing the instructions and executing the instructions. In this regard the processors may include an instruction buffer instruction decode circuitry dependency check circuitry instruction issue circuitry and execution stages.

As with the embodiments of the invention discussed hereinabove one or more of the processors and preferably all of them are fabricated using an advanced fabrication process e.g. of X nano meters as opposed to Y nano meters and are adapted to operate at a frequency of F despite that the X nano meter process would permit a frequency of operation of greater than F. This results in reduced power dissipation. One or more of the processors may also include the controllable latch point circuits the variable length pipeline and or the controllable clock distribution nodes discussed in detailed hereinabove with respect to . This functionality provides the programmer with the ability to improve processing in the presence of for example branch misses while simultaneously reducing power dissipation.

In one or more embodiments the processors and the local memories may be disposed on a common semiconductor substrate. In one or more further embodiments the shared memory may also be disposed on the common semiconductor substrate or it may be separately disposed.

In one or more alternative embodiments one or more of the processors may operate as a main processor operatively coupled to the other processors and capable of being coupled to the shared memory over the bus . The main processor may schedule and orchestrate the processing of data by the other processors . Unlike the other processors however the main processor may be coupled to a hardware cache memory which is operable cache data obtained from at least one of the shared memory and one or more of the local memories of the processors . The main processor may provide data access requests to copy data which may include program data from the system memory over the bus into the cache memory for program execution and data manipulation utilizing any of the known techniques such as DMA techniques.

A description of a preferred computer architecture for a multi processor system will now be provided that is suitable for carrying out one or more of the features discussed herein. In accordance with one or more embodiments the multi processor system may be implemented as a single chip solution operable for stand alone and or distributed processing of media rich applications such as game systems home terminals PC systems server systems and workstations. In some applications such as game systems and home terminals real time computing may be a necessity. For example in a real time distributed gaming application one or more of networking image decompression 3D computer graphics audio generation network communications physical simulation and artificial intelligence processes have to be executed quickly enough to provide the user with the illusion of a real time experience. Thus each processor in the multi processor system must complete tasks in a short and predictable time.

To this end and in accordance with this computer architecture all processors of a multi processing computer system are constructed from a common computing module or cell . This common computing module has a consistent structure and preferably employs the same instruction set architecture. The multi processing computer system can be formed of one or more clients servers PCs mobile computers game machines PDAs set top boxes appliances digital televisions and other devices using computer processors.

A plurality of the computer systems may also be members of a network if desired. The consistent modular structure enables efficient high speed processing of applications and data by the multi processing computer system and if a network is employed the rapid transmission of applications and data over the network. This structure also simplifies the building of members of the network of various sizes and processing power and the preparation of applications for processing by these members.

With reference to the basic processing module is a processor element PE . The PE comprises an I O interface a processing unit PU and a plurality of sub processing units namely sub processing unit A sub processing unit B sub processing unit C and sub processing unit D. A local or internal PE bus transmits data and applications among the PU the sub processing units and a memory interface . The local PE bus can have e.g. a conventional architecture or can be implemented as a packet switched network. If implemented as a packet switch network while requiring more hardware increases the available bandwidth.

The PE can be constructed using various methods for implementing digital logic. The PE preferably is constructed however as a single integrated circuit employing a complementary metal oxide semiconductor CMOS on a silicon substrate. Alternative materials for substrates include gallium arsinide gallium aluminum arsinide and other so called III B compounds employing a wide variety of dopants. The PE also may be implemented using superconducting material e.g. rapid single flux quantum RSFQ logic.

The PE is closely associated with a shared main memory through a high bandwidth memory connection . Although the memory preferably is a dynamic random access memory DRAM the memory could be implemented using other means e.g. as a static random access memory SRAM a magnetic random access memory MRAM an optical memory a holographic memory etc.

The PU and the sub processing units are preferably each coupled to a memory flow controller MFC including direct memory access DMA functionality which in combination with the memory interface facilitate the transfer of data between the DRAM and the sub processing units and the PU of the PE . It is noted that the DMAC and or the memory interface may be integrally or separately disposed with respect to the sub processing units and the PU . Indeed the DMAC function and or the memory interface function may be integral with one or more preferably all of the sub processing units and the PU . It is also noted that the DRAM may be integrally or separately disposed with respect to the PE . For example the DRAM may be disposed off chip as is implied by the illustration shown or the DRAM may be disposed on chip in an integrated fashion.

The PU can be e.g. a standard processor capable of stand alone processing of data and applications. In operation the PU preferably schedules and orchestrates the processing of data and applications by the sub processing units. The sub processing units preferably are single instruction multiple data SIMD processors. Under the control of the PU the sub processing units perform the processing of these data and applications in a parallel and independent manner. The PU is preferably implemented using a PowerPC core which is a microprocessor architecture that employs reduced instruction set computing RISC technique. RISC performs more complex instructions using combinations of simple instructions. Thus the timing for the processor may be based on simpler and faster operations enabling the microprocessor to perform more instructions for a given clock speed.

It is noted that the PU may be implemented by one of the sub processing units taking on the role of a main processing unit that schedules and orchestrates the processing of data and applications by the sub processing units . Further there may be more than one PU implemented within the processor element .

In accordance with this modular structure the number of PEs employed by a particular computer system is based upon the processing power required by that system. For example a server may employ four PEs a workstation may employ two PEs and a PDA may employ one PE . The number of sub processing units of a PE assigned to processing a particular software cell depends upon the complexity and magnitude of the programs and data within the cell.

The sub processing unit includes two basic functional units namely an SPU core A and a memory flow controller MFC B. The SPU core A performs program execution data manipulation etc. while the MFC B performs functions related to data transfers between the SPU core A and the DRAM of the system.

The SPU core A includes a local memory an instruction unit IU registers one ore more floating point execution stages and one or more fixed point execution stages . The local memory is preferably implemented using single ported random access memory such as an SRAM. Whereas most processors reduce latency to memory by employing caches the SPU core A implements the relatively small local memory rather than a cache. Indeed in order to provide consistent and predictable memory access latency for programmers of real time applications and other applications as mentioned herein a cache memory architecture within the SPU A is not preferred. The cache hit miss characteristics of a cache memory results in volatile memory access times varying from a few cycles to a few hundred cycles. Such volatility undercuts the access timing predictability that is desirable in for example real time application programming. Latency hiding may be achieved in the local memory SRAM by overlapping DMA transfers with data computation. This provides a high degree of control for the programming of real time applications. As the latency and instruction overhead associated with DMA transfers exceeds that of the latency of servicing a cache miss the SRAM local memory approach achieves an advantage when the DMA transfer size is sufficiently large and is sufficiently predictable e.g. a DMA command can be issued before data is needed .

A program running on a given one of the sub processing units references the associated local memory using a local address however each location of the local memory is also assigned a real address RA within the overall system s memory map. This allows Privilege Software to map a local memory into the Effective Address EA of a process to facilitate DMA transfers between one local memory and another local memory . The PU can also directly access the local memory using an effective address. In a preferred embodiment the local memory contains 556 kilobytes of storage and the capacity of registers is 128 128 bits.

The SPU core A is preferably implemented using a processing pipeline in which logic instructions are processed in a pipelined fashion. Although the pipeline may be divided into any number of stages at which instructions are processed the pipeline generally comprises fetching one or more instructions decoding the instructions checking for dependencies among the instructions issuing the instructions and executing the instructions. In this regard the IU includes an instruction buffer instruction decode circuitry dependency check circuitry and instruction issue circuitry.

The instruction buffer preferably includes a plurality of registers that are coupled to the local memory and operable to temporarily store instructions as they are fetched. The instruction buffer preferably operates such that all the instructions leave the registers as a group i.e. substantially simultaneously. Although the instruction buffer may be of any size it is preferred that it is of a size not larger than about two or three registers.

In general the decode circuitry breaks down the instructions and generates logical micro operations that perform the function of the corresponding instruction. For example the logical micro operations may specify arithmetic and logical operations load and store operations to the local memory register source operands and or immediate data operands. The decode circuitry may also indicate which resources the instruction uses such as target register addresses structural resources function units and or busses. The decode circuitry may also supply information indicating the instruction pipeline stages in which the resources are required. The instruction decode circuitry is preferably operable to substantially simultaneously decode a number of instructions equal to the number of registers of the instruction buffer.

The dependency check circuitry includes digital logic that performs testing to determine whether the operands of given instruction are dependent on the operands of other instructions in the pipeline. If so then the given instruction should not be executed until such other operands are updated e.g. by permitting the other instructions to complete execution . It is preferred that the dependency check circuitry determines dependencies of multiple instructions dispatched from the decoder circuitry simultaneously.

The instruction issue circuitry is operable to issue the instructions to the floating point execution stages and or the fixed point execution stages .

The registers are preferably implemented as a relatively large unified register file such as a 128 entry register file. This allows for deeply pipelined high frequency implementations without requiring register renaming to avoid register starvation. Renaming hardware typically consumes a significant fraction of the area and power in a processing system. Consequently advantageous operation may be achieved when latencies are covered by software loop unrolling or other interleaving techniques.

Preferably the SPU core A is of a superscalar architecture such that more than one instruction is issued per clock cycle. The SPU core A preferably operates as a superscalar to a degree corresponding to the number of simultaneous instruction dispatches from the instruction buffer such as between 2 and 3 meaning that two or three instructions are issued each clock cycle . Depending upon the required processing power a greater or lesser number of floating point execution stages and fixed point execution stages may be employed. In a preferred embodiment the floating point execution stages operate at a speed of 32 billion floating point operations per second 32 GFLOPS and the fixed point execution stages operate at a speed of 32 billion operations per second 32 GOPS .

The MFC B preferably includes a bus interface unit BIU a memory management unit MMU and a direct memory access controller DMAC . With the exception of the DMAC the MFC B preferably runs at half frequency half speed as compared with the SPU core A and the bus to meet low power dissipation design objectives. The MFC B is operable to handle data and instructions coming into the SPU from the bus provides address translation for the DMAC and snoop operations for data coherency. The BIU provides an interface between the bus and the MMU and DMAC . Thus the SPU including the SPU core A and the MFC B and the DMAC are connected physically and or logically to the bus .

The MMU is preferably operable to translate effective addresses taken from DMA commands into real addresses for memory access. For example the MMU may translate the higher order bits of the effective address into real address bits. The lower order address bits however are preferably untranslatable and are considered both logical and physical for use to form the real address and request access to memory. In one or more embodiments the MMU may be implemented based on a 64 bit memory management model and may provide 2bytes of effective address space with 4K 64K 1M and 16M byte page sizes and 256 MB segment sizes. Preferably the MMU is operable to support up to 2bytes of virtual memory and 2bytes 4 TeraBytes of physical memory for DMA commands. The hardware of the MMU may include an 8 entry fully associative SLB a 256 entry 4way set associative TLB and a 4 4 Replacement Management Table RMT for the TLB used for hardware TLB miss handling.

The DMAC is preferably operable to manage DMA commands from the SPU core A and one or more other devices such as the PU and or the other SPUs. There may be three categories of DMA commands Put commands which operate to move data from the local memory to the shared memory Get commands which operate to move data into the local memory from the shared memory and Storage Control commands which include SLI commands and synchronization commands. The synchronization commands may include atomic commands send signal commands and dedicated barrier commands. In response to DMA commands the MMU translates the effective address into a real address and the real address is forwarded to the BIU .

The SPU core A preferably uses a channel interface and data interface to communicate send DMA commands status etc. with an interface within the DMAC . The SPU core A dispatches DMA commands through the channel interface to a DMA queue in the DMAC . Once a DMA command is in the DMA queue it is handled by issue and completion logic within the DMAC . When all bus transactions for a DMA command are finished a completion signal is sent back to the SPU core A over the channel interface.

The PU core A may include an L1 cache an instruction unit registers one or more floating point execution stages and one or more fixed point execution stages . The L1 cache provides data caching functionality for data received from the shared memory the processors or other portions of the memory space through the MFC B. As the PU core A is preferably implemented as a superpipeline the instruction unit is preferably implemented as an instruction pipeline with many stages including fetching decoding dependency checking issuing etc. The PU core A is also preferably of a superscalar configuration whereby more than one instruction is issued from the instruction unit per clock cycle. To achieve a high processing power the floating point execution stages and the fixed point execution stages include a plurality of stages in a pipeline configuration. Depending upon the required processing power a greater or lesser number of floating point execution stages and fixed point execution stages may be employed.

The MFC B includes a bus interface unit BIU an L2 cache memory a non cachable unit NCU a core interface unit CIU and a memory management unit MMU . Most of the MFC B runs at half frequency half speed as compared with the PU core A and the bus to meet low power dissipation design objectives.

The BIU provides an interface between the bus and the L2 cache and NCU logic blocks. To this end the BIU may act as a Master as well as a Slave device on the bus in order to perform fully coherent memory operations. As a Master device it may source load store requests to the bus for service on behalf of the L2 cache and the NCU . The BIU may also implement a flow control mechanism for commands which limits the total number of commands that can be sent to the bus . The data operations on the bus may be designed to take eight beats and therefore the BIU is preferably designed around 128 byte cache lines and the coherency and synchronization granularity is 128 KB.

The L2 cache memory and supporting hardware logic is preferably designed to cache 512 KB of data. For example the L2 cache may handle cacheable loads stores data pre fetches instruction fetches instruction pre fetches cache operations and barrier operations. The L2 cache is preferably an 8 way set associative system. The L2 cache may include six reload queues matching six 6 castout queues e.g. six RC machines and eight 64 byte wide store queues. The L2 cache may operate to provide a backup copy of some or all of the data in the L1 cache . Advantageously this is useful in restoring state s when processing nodes are hot swapped. This configuration also permits the L1 cache to operate more quickly with fewer ports and permits faster cache to cache transfers because the requests may stop at the L2 cache . This configuration also provides a mechanism for passing cache coherency management to the L2 cache memory .

The NCU interfaces with the CIU the L2 cache memory and the BIU and generally functions as a queueing buffering circuit for non cacheable operations between the PU core A and the memory system. The NCU preferably handles all communications with the PU core A that are not handled by the L2 cache such as cache inhibited load stores barrier operations and cache coherency operations. The NCU is preferably run at half speed to meet the aforementioned power dissipation objectives.

The CIU is disposed on the boundary of the MFC B and the PU core A and acts as a routing arbitration and flow control point for requests coming from the execution stages the instruction unit and the MMU unit and going to the L2 cache and the NCU . The PU core A and the MMU preferably run at full speed while the L2 cache and the NCU are operable for a 2 1 speed ratio. Thus a frequency boundary exists in the CIU and one of its functions is to properly handle the frequency crossing as it forwards requests and reloads data between the two frequency domains.

The CIU is comprised of three functional blocks a load unit a store unit and reload unit. In addition a data pre fetch function is performed by the CIU and is preferably a functional part of the load unit. The CIU is preferably operable to i accept load and store requests from the PU core A and the MMU ii convert the requests from full speed clock frequency to half speed a 2 1 clock frequency conversion iii route cachable requests to the L2 cache and route non cachable requests to the NCU iv arbitrate fairly between the requests to the L2 cache and the NCU v provide flow control over the dispatch to the L2 cache and the NCU so that the requests are received in a target window and overflow is avoided vi accept load return data and route it to the execution stages the instruction unit or the MMU vii pass snoop requests to the execution stages the instruction unit or the MMU and viii convert load return data and snoop traffic from half speed to full speed.

The MMU preferably provides address translation for the PU core A such as by way of a second level address translation facility. A first level of translation is preferably provided in the PU core A by separate instruction and data ERAT effective to real address translation arrays that may be much smaller and faster than the MMU .

In a preferred embodiment the PU operates at 4 6 GHz 10F04 with a 64 bit implementation. The registers are preferably 64 bits long although one or more special purpose registers may be smaller and effective addresses are 64 bits long. The instruction unit registers and execution stages and are preferably implemented using PowerPC technology to achieve the RISC computing technique.

Additional details regarding the modular structure of this computer system may be found in U.S. Pat. No. 6 526 491 the entire disclosure of which is hereby incorporated by reference.

In accordance with at least one further aspect of the present invention the methods and apparatus described above may be achieved utilizing suitable hardware such as that illustrated in the figures. Such hardware may be implemented utilizing any of the known technologies such as standard digital circuitry any of the known processors that are operable to execute software and or firmware programs one or more programmable digital devices or systems such as programmable read only memories PROMs programmable array logic devices PALs etc. Furthermore although the apparatus illustrated in the figures are shown as being partitioned into certain functional blocks such blocks may be implemented by way of separate circuitry and or combined into one or more functional units. Still further the various aspects of the invention may be implemented by way of software and or firmware program s that may be stored on suitable storage medium or media such as floppy disk s memory chip s etc. for transportability and or distribution.

Although the invention herein has been described with reference to particular embodiments it is to be understood that these embodiments are merely illustrative of the principles and applications of the present invention. It is therefore to be understood that numerous modifications may be made to the illustrative embodiments and that other arrangements may be devised without departing from the spirit and scope of the present invention as defined by the appended claims.

