---

title: Method and apparatus for providing speech recognition resolution on a database
abstract: A method of providing speech recognition resolution in a database includes receiving an utterance from an end-user or application, and dispatching it to the database which is coupled to or has access to a speech recognition technology. The method further includes converting the utterance to an intermediate form suitable for searching, and transmitting the intermediate form from the database to the speech recognition technology. An item-matching is then performed via the speech recognition technology which returns items matching the utterance to the database along with a confidence metric, and the utterance matches are then provided from the database in the form of an ordered result set.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07752048&OS=07752048&RS=07752048
owner: Oracle International Corporation
number: 07752048
owner_city: Redwood Shores
owner_country: US
publication_date: 20050527
---
The field of the invention relates in general to speech recognition and more particularly to a method and apparatus for providing speech recognition resolution in the database layer.

A Voice application written for example in VoiceXML a derivative of the Extensible Markup Language XML processes spoken input from a user through the use of grammars which define what utterances the application can resolve. VoiceXML VXML allows a programmer to define a graph that steps a user through a selection process known as voice dialogs. The user interacts with these voice dialogs through the oldest interface known to mankind the voice. Hence VoiceXML is a markup language for building interactive voice applications which for example function to provide recognition of audio inputs such as speech and touch tone Dual Tone Multi Frequency DTMF input play audio control a call flow etc.

A VoiceXML application comprises a set of VoiceXML files. Each VoiceXML file may involve one or more dialogs describing a specific interaction with the user. These dialogs may present the user with information and or prompt the user to provide information. A VoiceXML application functions similar to an Internet based application accessed through a web browser in that it typically does not access the data at a dial in site but often connects to a server that gathers the data and presents it. The process is akin to selecting a hyperlink on a traditional Web page. Dialog selections may result in the playback of audio response files either prerecorded or dynamically generated via a server side text to speech conversion .

Grammars can be used to define the words and sentences or touch tone DTMF input that can be recognized by a VoiceXML application. These grammars can for example be included inline in the application or as files which are treated as external resources. Instead of a web browser VoiceXML pages may be rendered through Voice Gateways which may receive VoiceXML files served up by a web or application server as users call in to the gateway.

Voice Gateways typically comprise seven major components as follows a Telephony Platform that can support voice communications as well as digital and analog interfacing an Automated Speech Recognition ASR Engine a Text To Speech synthesis TTS engine a Media Playback engine to play audio files a Media Recording engine to record audio input a Dual Tone Multi Frequency DTMF Engine for touchtone input and a Voice Browser also known as a VoiceXML Interpreter . When a VoiceXML file is rendered by a Voice Gateway the grammars may be compiled by the ASR engine on the Voice Gateway.

The resolution capabilities of standard ASR engines are often fairly limited because performance in resolving utterances declines quickly with size typically limiting grammar sizes to the order of a few thousand possible utterances. In the past this problem with using large grammars for applications such as directory automation services was sometimes addressed through the use of specialized large scale speech recognition technology capable of efficiently resolving greater than a few thousand utterances. This technology often involves hardware and software solutions which included a telephony interface resource manager specialized ASR and TTS engine customized backend data connectivity and proprietary dialog creation environments integrated together in one package. The specialized ASR in these packages is sometimes capable of resolving grammars with millions of allowable utterances. However this specialized hardware and software solution has many drawbacks for example it does not take advantage of the centralization of data and standardization of data access protocols. For example a data synchronization problem can arise when a set of data such as a corporate directory is stored in one location by a enterprise and is replicated by the specialized solution. This problem can occur because any time the underlying data set changes e.g. due to a hiring firing etc. the replicated data state also needs to be refreshed. Furthermore these specialized systems often create a requirement that the call flow elements of a large scale speech recognition application must be designed as part of the proprietary dialog creation environment which effectively makes these applications non portable. Furthermore utilization of these specialized systems often locks users into the particular TTS engines and telephony interfaces provided as part of the specialized system further reducing the ability to switch implementations of the underlying large scale speech recognition technology.

Enabling large scale grammar resolution through an application server has been proposed to resolve some of these drawbacks. Specifically enabling large scale grammar resolution in the application server can allow the information and data resources that will make up the large scale grammar to remain in a centralized location. Application servers make use of a variety of industry standard data connectivity methods and protocols. Taking advantage of this data centralization allows for reduced though not eliminated duplication of data and memory state. Additionally by consolidating large scale grammar resolution through an application server administration of the large scale search technology can be simplified. Large scale grammar resolution through an application server can also allow application developers to write their applications in any language supported by the application server rather than in the proprietary format of third party Dialog Creation Environments. Application developers can therefore make use of standard programming conventions execution models and APIs Application Programming Interfaces when writing their applications. Problems remain with the application server approach however including data state replication data synchronization and recognition result return problems.

Although large scale speech recognition through the application server solves some problems further advantages can be gained by providing the large scale speech recognition in the database layer. In one such approach for example each database row can have an additional key that can be used to access it when the key corresponds to a sound or utterance. Thus the large scale grammar resolution engine can be integrated with the data structures used to store data in the database. As a result data from a table in the database can be selected by voice by performing automatic speech recognition of an utterance from the user against any set of data within the database. This approach can also permit the dataset to be synchronized with the grammar to be resolved and could enable users to search any table via voice without the overhead of initializing or priming a dataset within a specialized automatic voice recognition engine. One implementation for example could augment a relational database with voice access as an additional mode for accessing the data.

In one embodiment a method for speech recognition resolution in a database is provided which includes receiving an audio request utterance from an end user or an application such as a VoiceXML application and dispatching it to a database. The method further includes converting the utterance to an intermediate form suitable for searching transmitting the intermediate form from the database to a speech recognition search technology performing matching via the speech recognition technology returning items matching the utterance to the database with a score or confidence metric for each relevant utterance match and returning an ordered result set from the database based upon the scores.

While the present invention is susceptible of embodiments in various forms there is shown in the drawings and will hereinafter be described some exemplary and non limiting embodiments with the understanding that the present disclosure is to be considered an exemplification of the invention and is not intended to limit the invention to the specific embodiments illustrated.

In this application the use of the disjunctive is intended to include the conjunctive. The use of definite or indefinite articles is not intended to indicate cardinality. In particular a reference to the object or a object is intended to denote also one of a possible plurality of such objects.

In a specific embodiment the communication network allows an end user to make a voice channel connection to the data network for audio communication over a voice based telephony communication system such as a telephony switch control system examples of which include the following systems a public telephone switch a VoIP Voice over Internet Protocol media gateway a Private Branch Exchange PBX system etc. For example the public switch may be any public carrier and may also include other communication networks it may include a wireless telecommunication network that provides connectivity to the public switched telephone network PSTN control functions and switching functions for wireless users as well as connectivity to Voice Gateways.

The exemplary communication network shown comprises a Voice Gateway which is coupled to the telephony switch control system for example the public switch . The telephony switch control system couples the audio communication between the Voice Gateway and a suitable voice receiver end user device . An application server which may be any suitable server is coupled to the Voice Gateway and to a database as shown. In some embodiments the database may include a dedicated database server. The application server may for example run a Web site or Internet application and in one embodiment allows the Website or the application to be accessible from any browser or mobile device.

The database may for example host the dataset to be accessed using speech recognition technology. The database is coupled in the embodiment illustrated to speech recognition technology which provides programming interfaces to facilitate initialization and searching over the data sets. In some embodiments large scale speech recognition technology is utilized.

Referring to the Voice Gateway provides an audio communication bridge between the telephony switch control system and the application server as shown using any standard format. A Voice Gateway can typically support multiple technologies including but not limited to Automated Speech Recognition ASR Text To Speech TTS technologies browser functionalities e.g. for connecting to the Internet using standard TCP IP Transmission Control Protocol over Internet Protocol protocols to interpret voice data in formats such as Voice XML SALT XHTML V etc. . . . media playback and recording capabilities and telephony technologies to connect the system to the telephony network. Examples of other protocols and languages that may be used in various embodiments include but are not limited to WAP Wireless Application Protocol HTML Hyper Text Markup Language HDML WML MXML XHTHL etc.

Referring to the embodiment of the application server interacts with both the Voice Gateways and the database . This interaction can involve for example user management session management and Voice XML VXML delivery to the Voice Gateway end of the communication path. The speech recognition technology may be designed to provide state of the art voice search algorithms and may comprise large scale speech recognition technology. The speech recognition technology typically performs such tasks as transcription comparison and sorting and includes associated API s exposing this functionality. In some embodiments the speech recognition technology may be provided by a third party speech recognition provider and may be integrated within the database server and or may be provided by the provider of other technology in the system.

In the embodiment of voice requests for data in the database are passed through the Voice Gateway to the application server . The application server converts the request to a suitable form for presentation to the database. In one embodiment the request is in the form of a SQL statement which uses an audio SQL function referred to herein as the soundslike function . The soundslike function takes as input an audio file parameter corresponding to a recorded voice utterance. This soundslike function can provide an audio file which may be for example an audio byte stream a reference to an audio file or a phonetic transcription of the audio file. This audio file may then be converted if necessary to the form required by the speech recognition technology and passed thereto. The speech recognition technology performs the required recognition tasks on the passed in audio file and returns a set of results including a recognition score or confidence metric indicating the degree to which the speech recognition technology was able to recognize the input. For example a set of rows from the database determined to match the audio file could be returned with associated confidence metrics. Thus the speech recognition technology returns a ranked result linked to a subset of the original data table. The database then uses that ranked subset to retrieve the information requested from the original data table and puts that information into a useable form.

The methods disclosed herein may be performed by a processor or processors using instructions that may reside on a computer readable medium. The computer readable medium may be any suitable computer readable storage medium such as but not limited to random access memory read only memory flash memory CDROM DVD solid state memory magnetic memory optical memory and the like. The voice enabled systems and methods described herein address three key issues raised by a speech recognition enabled databases especially large scale systems. The first issue concerns the initializing and maintaining a set of searchable data. As described in detail hereinafter with regard to initialization original datasets in the database preferably are initialized on the database by applying the speech recognition technology prior to runtime to make them voice searchable. The second issue relates to the access of a particular speech recognition provider s technology and the querying of a particular dataset with a user utterance using the large scale search technology . The third issue concerns the return of an ordered result set.

In one embodiment the Voice Gateway may capture a user utterance which may originate from any suitable source. If the utterance to be interpreted is part of a large scale voice request for data from the database then in one embodiment the utterance is recorded by the Voice Gateway and sent to the application server . The application code being executed through the application server may take the recorded utterance and pass it for example as an argument to a SQL function to retrieve a set of results from the database that match the recorded utterance. In some embodiments the database is pre initialized with speech recognition searchable tables linked to the original datasets of the database.

Once the voice request is submitted to the database such as by a SQL function the database performs a search algorithm utilizing the speech recognition technology which may in some embodiments be provided by a third party. As described hereinafter with reference to the database in the illustrated embodiment first ensures that the argument is in searchable form and then submits it to the speech recognition technology which then performs a search and returns items matching the utterance for example in the form of a graded table e.g. with confidence metrics . Subsequently in some embodiments the database may use the graded table calculated by the speech recognition technology and return an ordered result set for example composed of the retrieved rows in order of decreasing confidence from the database to the application. In one embodiment the database maintains synchronicity through triggers on insertions deletions and updates.

In one embodiment to initialize the database of to be voice searchable a shadow table may be created in the database corresponding to each data table in the database. The shadow table described in detail hereinafter with reference to is a searchable representation of a voice searchable column from an original dataset in the database which is added to the database. Each row of the shadow table includes a reference or link back to the corresponding row of the original dataset and includes a conversion to voice searchable form of each entry of searchable text in the original dataset. The conversion represents the data in a transcribed form which can be used by the speech recognition technology. This conversion could be performed by a transcription algorithm which for example converts the human readable text of the data table to a representation e.g. a phonetic representation used by the speech recognition technology. The speech recognition technology compares the utterances to this shadow table to generate graded results which are then passed back to the database.

In another embodiment an additional searchable column can be added directly to the table. In this embodiment the transcribed version of the data of a column to be made searchable is inserted as an additional table column e.g. in Table A and linked to the corresponding original data column.

In some other embodiments the original table e.g. table A can include notations e.g. an added parameter or flag associated with a column or each data item to indicate which portions e.g. column are desired to be made searchable. The flagged data items would then be selected to be transcribed to searchable form and for generation of a corresponding shadow table. The notations could be added as a parameter at an early stage prior to initialization of the shadow table or modified later in either case using the capabilities of standard SQL commands such as CREATE TABLE and or ALTER TABLE

The shadow table is passed to the speech recognition technology to do the search matching function to match the audio request to the shadow table. Once the speech recognition technology identifies matching rows with associated confidence metric the resulting graded shadow table is passed to the database with the confidence metrics.

If the argument of the soundslike function is a reference to an audio file see then the audio file is fetched and written into a byte stream as shown at and the fetched byte stream is converted to a searchable transcription of the byte stream as indicated at . If the argument is an audio file byte stream see then it is converted to a searchable transcription as shown at . In either event the searchable transcription is passed to the comparator function of the speech recognition technology see . If the argument is already in the form of a searchable transcription see then it is passed to the speech recognition technology as illustrated at . When transcription is required the speech recognition technology provides transcription of the byte stream into a phonetic transcription as shown at . The searchable transcription is the form the audio input is put into to be usable by the speech recognition technology e.g. the phonetic or other intermediate form the technology uses to do its search matching . This transcription can typically be performed by an API provided by the speech recognition technology provider.

Once in transcribed form the input argument is passed to the speech recognition technology to perform a search comparison function against the shadow table for the table identified for search e.g. Table A . The transcription argument is compared to shadow tables dataset ds which had been stored as part of the database corresponding to the stored data set as illustrated at . The result of this matching process is a graded version of the shadow table passed back to the database with associated score or confidence metric. This graded shadow table is then formed into an ordered result set by the database. The ordered result set is ordered to present those entries in the dataset that most closely match the transcription argument first and those that least closely match are presented last along with a score or confidence metric. As illustrated at the database performs any necessary SQL functions to associate the original request with the results of the comparator function i.e. the graded shadow table to generate as ordered result set that is ordered based on the provided score or confidence measure. Thus the speech recognition technology compares the transcription of the voice utterance against a searchable representation of the original dataset i.e. the shadow table to generate the graded shadow table. Subsequently the database takes the graded a shadow table and re associates it with the actual original dataset providing the ability to return data out of the original dataset in the form of an ordered result set.

In some embodiments the graded shadow table can be limited to only those matches that have a confidence metric that exceeds a threshold. In such an instance the shadow table returned to the database would typically represent a subset of the entire shadow table. Alternatively the entire shadow table with associated confidence metric can be returned to the database and the database can then eliminate those entries that are below a threshold. In another alternative the speech recognition technology can sort the graded shadow table in order of confidence metric magnitude or return the graded shadow table unsorted. If a sorted graded shadow table is returned in some embodiment the returned graded table may exclude the specific confidence metric because the order constitutes an implicit confidence metric. If an unsorted graded shadow table is returned then the database may sort the graded shadow table as well as retrieve the requested data from the original table of data to which the shadow table is linked. The ordered result set is then formed containing the original data associated with the match in the graded shadow table and corresponding confidence metric sorted in order of the confidence metric. The order may be any order specified e.g. ascending descending etc. .

An example of an ordered result set is illustrated in . An ordered result set is a temporary data structure to encapsulate multiple rows or records i.e. the result of data from a database where the ordering of the rows or records is important . The ordered result set as illustrated shows an ordered set returning three rows which were matched to the audio argument or the three rows with a confidence metric that exceeded the minimum threshold. A confidence metric is provided for each row and the rows are presented in order from the higher confidence level at the top to the lowest at the bottom. The column data for each matched row is the original data specified by the initiating SQL statement sorted to placed them in order of descending confidence with the appended confidence metric. Thus the database creates this temporary data structure the ordered result set constituting an ordered list of the rows matched and an appended metric as the response to original voice search request. Other variations of the ordered result set may be utilized by specifying other parameters for example the result set could be limited to the top few matches e.g. the three rows with the highest confidence metric or by omitting the confidence metric and providing the results in order of confidence i.e. the metric is implicit in the order .

Specific embodiments of methods and apparatus for providing speech recognition resolution in the database layer have been described for the purpose of illustrating the manner in which the invention is made and used. It should be understood that the implementation of other variations and modifications of the invention and its various aspects will be apparent to one skilled in the art and that the invention is not limited by the specific embodiments described. Therefore it is contemplated to cover any and all modifications variations or equivalents that fall within the true spirit and scope of the basic underlying principles disclosed and claimed herein.

