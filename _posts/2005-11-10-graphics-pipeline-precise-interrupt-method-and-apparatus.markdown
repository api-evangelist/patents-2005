---

title: Graphics pipeline precise interrupt method and apparatus
abstract: A graphics processing unit (“GPU”) is configured to interrupt processing of a first context and to initiate processing of a second context upon command. A command processor communicates an interrupt signal on a communication path from to a plurality of pipeline processing blocks in a graphics pipeline. A token, which corresponds to an end of an interrupted context, is forwarded from the command processor to a first pipeline processing block and subsequently to other pipeline blocks in the graphics pipeline. Each pipeline processing block discards contents of associated memory units upon receipt of the interrupt signal until the token is reached. The token may be forwarded to one or more additional pipeline processing blocks and memory units so that the token is communicated throughout the graphics pipeline to flush data associated with the first context. Data associated with the second context may follow behind the token through graphics pipeline.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07583268&OS=07583268&RS=07583268
owner: Via Technologies, Inc.
number: 07583268
owner_city: Hsin-Tien, Taipei
owner_country: TW
publication_date: 20051110
---
This application is related to the following copending U.S. utility patent applications 1 U.S. Patent Application entitled INTERRUPTIBLE GPU AND METHOD FOR PROCESSING MULTIPLE CONTEXTS AND RUNLISTS filed on Nov. 10 2005 and having assigned Ser. No. 11 271 169 which is entirely incorporated herein by reference and 2 U.S. Patent Application entitled INTERRUPTIBLE GPU AND METHOD FOR CONTEXT SAVING AND RESTORING filed on Nov. 10 2005 and having assigned Ser. No. 11 272 356 which is also entirely incorporated herein by reference.

The present disclosure relates to graphics processing and more particularly to a system and method for saving and restoring contexts in an interruptible graphics processing unit.

Today s computer systems typically include multiple processors. For example a graphics processing unit GPU is an example of a coprocessor in addition to a primary processor such as a central processing unit CPU that performs specialized processing tasks for which it is designed. In performing these tasks the GPU may free the CPU to perform other tasks. In some cases coprocessors such as a GPU may actually reside on the computer system s motherboard along with the CPU which may be a microprocessor. However in other applications as one of ordinary skill in the art would know a GPU and or other coprocessing devices may reside on a separate but electrically coupled card such as a graphics card in the case of the GPU.

A coprocessor such as a GPU may often access supplemental memory such as video memory for performing its processing tasks. Coprocessors may be generally configured and optimized for performing specialized tasks. In the case of the GPU such devices may be optimized for execution of three dimensional graphics calculations to support applications with intensive graphics. While conventional computer systems and coprocessors may adequately perform when running a single graphically intensive application such computer systems and coprocessors may nevertheless encounter problems when attempting to execute multiple graphically intensive applications at once.

It is not uncommon for a typical coprocessor to schedule its processing workload in an inefficient manner. In some operating systems a GPU may be multitasked using an approach that submits operations to the GPU in a serialized form such that the GPU executes the operations in the order in which they were received. One problem with this approach is that it does not scale well when many applications with differing priorities access the same resources. In this nonlimiting example a first application that may be currently controlling the resources of a GPU coprocessor needs to relinquish control to other applications for the other applications to accomplish their coprocessing objectives. If the first application does not relinquish control to the other waiting application the GPU may be effectively tied up such that the waiting application is bottlenecked while the GPU finishes processing the calculations related to the first application. As indicated above this may not be a significant bottleneck in instances where a single graphically intensive application is active however the problem of tying up a GPU or other coprocessor s resources may become more accentuated when multiple applications attempt to use the GPU or coprocessor at the same time.

The concept of apportioning processing between operations has been addressed with the concept of interruptible CPUs that context switch from one task to another. More specifically the concept of context save restore has been utilized by modern CPUs that operate to save the content of relevant registers and program counter data to be able to resume an interrupted processing task. While the problem of apportioning processing between the operations has been addressed in CPUs where the sophisticated scheduling of multiple operations is utilized scheduling for coprocessors has not been sufficiently addressed.

At least one reason for this failure is related to the fact that coprocessors such as GPUs are generally viewed as a resource to divert calculation heavy and time consuming operations away from the CPU so that the CPU may be able to process other functions. It is well known that graphics operations can include calculation heavy operations and therefore utilize significant processing power. As the sophistication of graphics applications has increased GPUs have become more sophisticated to handle the robust calculation and rendering activities.

Yet the complex architecture of superscalar and EPIC type CPUs with parallel functional units and out of order execution has created problems for precise interruption in CPUs where architecture registers are to be renamed and where several dozens of instructions are executed simultaneously in different stages of a processing pipeline. To provide for the possibility of precise interrupt superscalar CPUs have been equipped with a reorder buffer and an extra stage of instruction commit retirement in the processing pipeline.

Current GPU versions use different type of commands which can be referred as macroinstructions. Execution of each GPU command may take from hundreds to several thousand cycles. GPU pipelines used in today s graphics processing applications have become extremely deep in comparison to CPUs. Accordingly most GPUs are configured to handle a large amount of data at any given instance which complicates the task of attempting to apportion the processing of a GPU as the GPU does not have a sufficient mechanism for handling this large amount of data in a save or restore operation. Furthermore as GPUs may incorporate external commands such as the nonlimiting example of a draw primitive that may have a long sequence of data associated with the command problems have existed as to how to accomplish an interrupt event in such instances.

Thus there is a heretofore unaddressed need to overcome these deficiencies and shortcomings described above.

A graphics processing unit GPU is configured to be interruptible so that it may execute multiple graphics programs at the same relative time. The GPU is configured in hardware to interruptible operation and operates to provide multiple programs access to processing so as to be able to switch between multiple tasks.

The GPU is configured to interrupt processing of a first context and to initiate processing of a second context upon command. A command processor communicates an interrupt signal on a communication path from to a plurality of pipeline processing blocks in a graphics pipeline. A token which corresponds to an end of an interrupted context is forwarded from the command processor to a first pipeline processing block and subsequently to other pipeline blocks in the graphics pipeline. Each pipeline processing block discards contents of associated FIFO memory units upon receipt of the interrupt signal until the token is reached. The token may be forwarded to one or more additional pipeline processing blocks and memory units so that the token is communicated throughout the graphics pipeline to flush data associated with the first context. Data associated with the second context may follow behind the token through graphics pipeline.

The pipeline may include a number of pipeline processing blocks in the graphics pipeline not coupled to the command processor by the communication path. These pipeline processing blocks continue processing data associated with the first context until receiving the token through the graphics pipeline. Upon receiving the token these pipeline processing blocks may also discard data in memory associated with the first context and begin processing data associated with the second context.

Other systems methods features and advantages of this disclosure will be or become apparent to one with skill in the art upon examination of the following drawings and detailed description. It is intended that all such additional systems methods features and advantages be included within this description be within the scope of this disclosure and be protected by the accompanying claims.

This disclosure provides for advanced scheduling so as to virtualize a GPU thereby enabling different processes seeking GPU processing to be assigned a timeslot to be executed and provided some level of service that an operating system can control. While several applications may share the GPU the operating system may be configured to schedule each application according to various criteria such as when as a nonlimiting example a time quantum of one process expires the GPU may schedule another process or even reschedule the same process to run in a next time slot.

A process may comprise a number of contexts or operations related to portions of the process being executed as a whole. As described herein a context may represent all the state of the GPU at the time of a last execution or initial execution of the process on the GPU. The state may include the state registers cache and memory contents all the internal FIFOs internal registers etc. at the time of the last switch from one context to a different context perhaps as a nonlimiting example for a different process being executed by the GPU.

While it may not be practical to save an entire state of a GPU when a context is switched the entire state may also not be needed since a switch may be permitted to transpire between 1 to 3 milliseconds. During this time the GPU can be configured to wrap up some level of processing so as to minimize an amount of a state that is saved.

GPUs may be configured with deep pipelines such that a significant number of triangles and pixels are contained in various stages of completion at any given cycle. Plus a typical GPU may read modify and or write to memory throughout the various stages of the processing pipeline. As a nonlimiting example a GPU may be configured in the Z stages to read compare and conditionally update Z. Additionally a write back unit of the GPU may be configured for destination blending of graphics elements. Thus for these reasons memory may be part of the state that is tracked and if the context is going to be stopped and restarted the GPU should not read modify write the same memory for the same pixel a second time. In this nonlimiting example blending twice would yield different results. Thus the GPU may be configured so that it does not track as part of the saved state all the history of what was written to memory up to the point of the context switch so as to avoid this situation described above.

This disclosure may be implemented by an operating system as a nonlimiting example for use by a developer of services of a device or object and or included within application software that operates in connection with the techniques described herein. Software may be described or represented in the general context of computer executable instructions such as program modules being executed by one or more computers such as client workstations servers or other devices. Program modules may include routines programs objects components data structures and the like that perform a particular task or implement particular abstract data types as one of ordinary skill in the art would know. The functionality of program modules may be combined or distributed as desired in various configurations.

Other well known computing systems environments and or configurations that may be suitable for use with this disclosure include but are not limited to personal computers PCs automated teller machines ATMs server computers handheld or laptop devices multiprocessor systems microprocessor based systems programmable consumer electronics network PCs appliances lights environmental control elements minicomputers mainframe computers and the like. This disclosure may be applied and distributed in computing environments where tasks are performed by remote processing devices that are coupled via communication networks buses or another data transmission medium. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices and client nodes may in turn behave as server nodes.

The computing system of includes a computer . The components of the computer may include as nonlimiting examples a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures as one of ordinary skill in the art would know including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. As a nonlimiting example such architectures may include a peripheral component interconnect PCI bus accelerated graphics port AGP and or PCI Express bus.

Computer may include a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile memory removable and nonremovable memory. As a nonlimiting example computer readable media may comprise computer storage media and communication media. Computer storage media may include both volatile and nonvolatile removable and nonremovable media implemented in any method or technology for storage such as computer readable instructions data structures program modules or other data as one of ordinary skill in the art would know. Computer storage media includes as nonlimiting examples RAM ROM EEPROM flash memory or other memory technology CDROM digital versatile disks DVD or other optical disk storage disks magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to store desired information and which can be accessed by computer .

The system memory may include computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that may help to transfer information between elements within computer such as during startup may be stored in ROM . RAM may contain data and or program modules that are accessible to and or presently being operated on by processing unit . As a nonlimiting example operating system application programs other program modules and program data may be contained in RAM .

Computer may also include other removable nonremovable volatile nonvolatile computer storage media. As a nonlimiting example a hard drive may read from or write to nonremovable nonvolatile magnetic media. A magnetic disk drive may read from or write to a removable nonvolatile magnetic disk . An optical disk drive may read from or write to a removable nonvolatile optical disk such as a CDROM or other optical media. Other removable nonremovable volatile nonvolatile computer storage media that can be used in the exemplary computing system include but are not limited to magnetic tape cassettes flash memory cards DVDs digital video tape solid state RAM solid state ROM and the like.

Hard disk drive may typically be connected to bus system through a nonvolatile memory interface such as interface . Likewise magnetic disk drive and optical disk drive may be connected to bus system by removable memory interface such as interface . The drives and their associated computer storage media described above and shown in may provide storage of computer readable instructions data structures program modules and other data for computer . As a nonlimiting example hard disk drive is illustrated as storing operating system application programs other program modules and program data .

These components may either be the same as or different from operating system application programs other program modules and or program data . At least in this nonlimiting example described herein as shown in these components of software are given separate reference numerals to at least illustrate that they are different copies.

A user may enter commands and information into computer through input devices such as keyboard and pointing device . These devices are but nonlimiting examples as one of ordinary skill in the art would know. Keyboard and pointing device however may be coupled to processing unit through a user input interface that is coupled to system bus . However one of ordinary skill in the art would know that other interface and bus structures such as a parallel port game port or a universal serial bus USB may also be utilized for coupling these devices to the computer .

A graphics interface may also be coupled to the system bus . As a nonlimiting example the graphics interface may be configured as a chip set that communicates with the processing unit and assumes responsibility for accelerated graphics port AGP or PCI Express communications. One or more graphics processing units GPUs may communicate with the graphics interface . As a nonlimiting example GPU may include on chip memory storage such as register storage and cache memory. GPU may also communicate with a video memory wherein application variables as disclosed herein may have impact. GPU however is but one nonlimiting example of a coprocessor and thus a variety of coprocessing devices may be included with computer .

A monitor or other type of display device may be also coupled to system bus via video interface which may also communicate with video memory . In addition to monitor computer system may also include other peripheral output devices such as printer and speakers which may be coupled via output peripheral interface .

One of ordinary skill in the art would know that computer may operate in a networked or distributed environment using logical connections to one or more remote computers such as remote computer . Remote computer may be a personal computer a server a router a network PC a pier device or other common network node. Remote computer may also include many or all of the elements described above in regard to computer even though only memory storage device and remote application programs are depicted in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may include other network buses as one of ordinary skill in the art would know.

In this nonlimiting example of remote computer may be coupled to computer via LAN connection and network interface . Likewise a modem may be used to couple computer via user input interface to remote computer across WAN connection .

As stated above the GPU may be configured to switch processes or contexts during the processing of another context or operation. In this instance the GPU is configured to save an interrupted context and to initiate processing of another context which itself may have been previously interrupted and saved.

In regard to saving state context states and restoring previously saved context states is an illustration of the major states of a context switch process that may be implemented by GPU . At stage GPU may be configured to execute a current GPU state context in regard to a given operation. However as shown in a first step the processing unit may communicate an interrupt command or event so that GPU operates to save the GPU state context as shown in stage . The method for effectuating the interrupt command or event is described in detail below. Thereafter the GPU state context is saved as in step as the GPU switches GPU state context as shown in stage . GPU may then implement the third step to load a new GPU state context as depicted in stage . Thereafter GPU implements step to return to stage to execute this newly loaded GPU state context.

When the GPU completes execution of this newly loaded GPU state context a fifth step is implemented at the end of the newly loaded context so that the GPU returns to stage to switch GPU state context back to the previously executed context as shown in step . In so doing the GPU moves to stage to restore the GPU state context previously saved in step as described above. Thereafter in step the GPU returns to stage to execute this newly restored GPU state context at the point where it left off prior to receiving the interrupt command in step .

GPU is configured according to to support sequential execution of multiple GPU programs commands belonging to the same context that have also the name of the ring buffer which comprises processor functions and command DMA buffer pointers in memory. As described above the GPU switches from one context to another upon receipt of an interrupt command and also at the end of the ring buffer as corresponding to steps and respectively. In the case of the interrupt command the GPU saves the state context so that it is able to continue execution of that context subsequent in time at the precise point saved.

The following constitutes a nonlimiting exemplary list of elements in the state context save data structure 

The ring buffer also contains in this nonlimiting example DMA memory command and associated DMA pointer that points to DMA buffer which may contain commands and data related to the context for this ring buffer . Additionally ring buffer may contain DMA commands such as DMA command and associated DMA pointers such as pointer that point to a DMA buffer with commands and data such as DMA buffer . Ring buffer of also contains place holders and which in this nonlimiting example is skip 1 DWORD and null position to hold the place for a context save command and address pointer respectively after a save restore operation as described below.

In application when GPU begins to execute the ring buffer GPU receives both head pointer and tail pointer and checks for a saved context. Placeholder which in this nonlimiting example is configured as a skip 1 DWORD which causes the GPU to skip or ignore null to the next command which is DMA command . In this instance the ring buffer is not interrupted at this point and GPU otherwise continues to execute the commands and instructions of ring buffer of and also the contents of DMA buffers and such as draw commands primitives instances and tiles .

As the GPU processes the ring buffer of upon recognizing restore command GPU acknowledges the context save address of a previous run state context that should be retrieved from state context save buffer . Data retrieved from state context save buffer as discussed above may also provide a DMA offset for DMA buffer so that processing can resume at the precise point interrupted.

The GPU may include a command stream processor as shown in and described below. The command stream processor CSP may be configured to include a pair of parsers including a front end parser and a back end parser which communicate with a 3D pipeline .

CSP front end parser may begin to parse the ring buffer such that it receives head and tail pointers . Thereafter CSP front end parser may check for a saved context according to whether command pointer is a skip or restore command. If the command is a skip command this indicates that ring buffer was not previously interrupted. Thus CSP executes the remaining portion of the context which may include one or more DMA commands and pointers. If however the CSP front end parser recognizes command as a restore command such as restore command in the restore command is executed so that the previous run state context save data structure is retrieved as shown in and restored at CSP front end parser . Thereafter this restored context is executed by 3D pipeline and forwarded to CSP back end parser which operates on it as the current run state save context data structure .

The 3D pipeline of may be further represented by the 3D pipeline architectural blocks as shown in . These 3D pipeline architectural blocks may in one nonlimiting example be represented by a tile shade unit shade generator unit tile generator etc. as one of ordinary skill in the art would know. See .

To prepare all states for saving when an interrupt command is received each 3D pipeline architectural block may be configured to forward a copy of its state command to the CSP . Upon receipt of each architecture block s state command the CSP may write this information in a state FIFO 128 512 reference and later into memory until subsequently restored as described above. In at least one nonlimiting example the 3D pipeline architectural blocks and are configured to include data paths to back end parser of the CSP . Although discussed in more detail below not every 3D pipeline architectural block includes a data path to back end parser as the paths may share data from multiple blocks . However these paths enable GPU to save data mid processing so that one context may be interrupted and another begun.

Data paths and between architectural blocks and respectively are the two data paths shown in this nonlimiting example. Stated another way at least one nonlimiting example provides that each architectural block in the 3 D pipeline does not have a dedicated data path back to back end parser . Thus the data path to copy the state entry of each architectural block to the state FIFO can be dedicated or shared for several 3D pipeline architecture blocks . Because state changes may occur relatively infrequently it may be more economical or desirable to share the data path with multiple blocks so as to reduce the overall number of data paths between the 3 D pipeline and CSP back end parser . Stated another way by having fewer data paths chip real estate may be preserved for other modules and or configurations.

The tile generator may be configured to save the tile ID at the point of interrupt as well as the current head pointer DMA offset instance ID and the primitive ID . This process is depicted in at step and is also described below in greater detail.

In step of the back end part of the 3D pipeline continues to process data related to the old context so that a certain amount of the old context is drained through the pipeline via normal processing. Thus at least a portion of the old context is completed at the receipt of an interrupt operation.

However if the CSP does receive a command switching to the interruptible mode of a currently processed context the 3D pipeline architectural blocks move to stage so as to save the current state context executed by the GPU see step of . Thereafter the architectural block returns to stage to check for the next entry and continues to operate as described above.

In this event the 3D pipeline architectural blocks operate to establish a context saved data structure by forwarding state data to the CSP via data paths and . Blocks and may forward such data to blocks and respectively which each forward the data received to the back end parser via data paths and .

SG communicates processed data to tile generator unit TG . A Z unit level block ZL1 receives data from ASU ZL FIFO AFIFO and also TG FIFO which is also coupled to an output of TG . ZL processes data from these sources and communicates an output to a Z unit level block ZL2 via ZFIFO and ZL FIFO memory .

The 3D architecture pipeline hereinafter 3D pipeline of is but one portion of a graphics pipeline as one of ordinary skill in the art would know. Additionally a write back unit a pixel packer and other logical blocks may be included in a rest of the pipeline as one of ordinary skill in the art would know but are excluded here for simplicity.

The context save and restore state process described above may be implemented in the 3D pipeline of . is a diagram depicting a flowchart for a save and restore process as may be implemented in the 3D pipeline of .

For a context save process when the GPU is processing a context an interrupt command may be received from the CPU processing unit of as shown in step . As shown in step 3D pipeline may be configured to wait until the tile generator or a write back unit not shown in clears of instructions being executed. The CSP may recognize the tile generator clearing due to the fact that the CSP may send a token through the pipeline every time when the head pointer is changed. The tile generator may communicate the token back to the CSP when the tile generator receives and processes it.

The CSP may further communicate a DMA DWORD OFFSET down the pipeline when the CSP is initiating a draw command or starting states. The tile generator may communicate this DMA DWORD OFFSET to the CSP as well. The tile generator may then communicate back each state when the states are communicated down to the tile generator as described above in regard to and the data paths and .

Block of describes the states that may be saved as well as the pointers that may be stored upon receipt of the interrupt from the CPU in step . When the interrupt is received by the tile generator the tile generator may communicate all 0s for IDs to the CSP if the tile generator is processing states during the interrupt. Otherwise as shown in step the tile generator in this nonlimiting example may store the pointers and states including the tile head pointer DMA Offset the instance ID the primitive ID the tile ID and all register states so that the context being saved can be quickly restored. This data may comprise and be stored as the context saved data structure of .

Thereafter the GPU may be configured to switch to another run list as shown in step . The run lists that may be switched to may include one or more other contexts for execution by the 3D pipeline as shown in below.

For the process flow of restoring a state for execution of the pipeline the GPU may be configured to switch back to a previously but partially executed run list as in step that is when the currently executed run list is finished or when an instruction is received to do so. Thereafter the GPU can be configured to restore all previously saved states in step which may have been saved in step during a save state process. The CSP may be configured in step to skip draw commands until the saved draw command is reached according to the previously saved DMA offset as in step and also as discussed in more detail below. Thereafter in step the tile generator may skip draw commands until all of the instance ID primitive ID and tile IDs which were saved in step are received. Thereafter in step the pipeline may execute any unfinished geometries. This process is described in additional detail below.

For the beginning of a context restore process the CSP needs to process the states to the entire engine including the current head pointer DMA offset instance ID primitive ID and tile ID which may have been previously saved in step . Thus the CSP will retrieve the ring buffer head pointer from the ring buffer that was saved through the save process as described above and process the DMA command that is pointed to by the head pointer and then skip all commands until the DMA address is equal to the DMA offset. This enables the CSP to restart execution of the command exactly where it was interrupted which may have been in the middle of a triangle that was being executed as a nonlimiting example.

The CSP may skip instances in the restored state process described above until the instance ID is matched and may also skip primitives until the primitive ID is matched. The instance ID and primitive ID may be stored in state FIFO register so that the CSP may make the comparisons of the instances restored to the instance ID. Additional processing blocks that may be part of the GPU computational core not shown may be configured to skip triangles until the primitive ID for a particular triangle is matched. For example triangle IDs may be stored in a separate execution block register also not shown herein. The tile generator in the restore state process may be configured to skip tiles until the tiles match the tile ID saved at stage during a save state process. Once the IDs are matched as described above the CSP may switch the 3D pipeline from a skip state to a normal state for execution.

The CSP may create one or more tokens that are communicated to the 3D pipeline components bearing the address of the registers for operation with an offset representing the point of processing during the previous interrupt as well as identification of the registers to be restored. To provide flexible changes in the state context save structure CSP may operate in a configuration that includes densely packing all block state data in memory . As a result the CSP may implement a flexible pack register offset. The register offset communicates to each 3D pipeline block the point to resume or restore operation upon an interrupted context. By adding the offset to the register address of the interrupted context the precise point of the interrupted context may be quickly determined. Accordingly for each 3D pipeline architectural block in a block ID may be associated thereto and a corresponding offset for resuming calculations. In this way GPU is able to implement an interrupt with state context save and restore which may be done completely transparent to the application so that the 3D pipeline may be utilized for one or more applications in a time sharing function and operation.

For the context save process described in it may be desirable to provide for flexible changes in the state context save structure. Thus one nonlimiting exemplary process includes densely packing all block state data in the memory . During a GPU subversion design process each block in pipeline may change register specification frequently. As a nonlimiting example if there are in excess of processing blocks i.e. . . . where n 20 it make take a substantial amount of time to change the register data packing hardware.

Therefore implementing a flexible packed register offset can address this issue. The table below provides for a set of registers that configures the state data save in a context save structure. For each block of state data individual offset values are provided which may be a combination of an offset register content and a register ID. The CSP may be configured to issue a set register command targeted to a specific block in the pipeline . In at least one nonlimiting example the offsets are 128 bit aligned. A nonlimiting exemplary table of register set may be configured as follows 

A table of length registers such as depicted below may be configured to describe the length of state data for each block and define the upper limit of data in opcode double word for each block. The length registers may be used by the CSP for an internal test. Unlike the offset register described above the length register may be formatted for the length of 32 bits. The following table is a nonlimiting exemplary length register table 

With this information the CSP can determine the length and offset of each block registers memory location of address block id for register n which is the base address summed with the offset register block id n 

In the nonlimiting example of ring buffer may be referenced in context of run list even. In this nonlimiting example ring buffer contains various GPU commands and or DMA commands with DMA buffer pointers such as described above in regard to ring buffers and . In conjunction with is a flowchart diagram of the CSP processing of a current run list and ring buffer as shown in .

GPU may receive the run list command and thereafter fill the context base address slot as shown in and as also referenced as step of so as to establish a run list for execution. As also shown in CSP thereafter starts at context of the run list to be executed whether run list even or run list odd.

In the CSP may fetch the head and tail pointers and check a next token for a skip or restore command as shown in step and also described above. If the CSP determines that the next token is a restore command as referenced by decision block and as described above restore command in the CSP executes the restore command and fetches all of the GPU state info as also described above and as shown in block . If decision block does not result in detection of a restore command in the next token the CSP may fetch the ring buffer of and execute a DMA command such as DMA command of and its associated DMA buffer such as DMA buffer of .

In CSP and 3D pipeline may access DMA pointer which causes the DMA buffer to be accessed. In this nonlimiting example DMA buffer contains draw command and draw command which is fetched when the head pointer reaches DMA pointer . As the head pointer moves logically from left to right down ring buffer it will reach either a skip or restore command prior to DMA pointer as described above. Yet in this nonlimiting example of the absence of a restore command causes the CSP to start the DMA buffer fetch which results in accessing a DMA buffer and the processing of graphics related data contained in and or referenced by the buffer.

Returning to CSP and 3D pipeline begins to process the current ring buffer location in step after the appropriate ring buffer has been fetched. In the nonlimiting example of the CSP may begin processing context containing run buffer for the run list even group. If the context is empty as determined in step a switch is made to the next context in the run list in step . In this nonlimiting example the CSP would switch from context to context such that the ring buffer would be loaded and executed. In switching to ring buffer the determination would be made whether there is an interrupt from the processing unit CPU as determined in step of . If not then the CSP would return to step for execution of the ring buffer of . However if an interrupt were detected in step the next step would result in processing of the interrupt as shown in step which subsequently would cause the process to return to step . The context save process of would follow as well.

If in step the context is determined not to be empty the front end parser of CSP may thereafter send the current ring buffer pointer and DMA offset and ID as well as register states to the triangle setup unit of . The front end parser may also communicate the instance ID primitive ID and vertex ID as well as indices to the appropriate locations for execution. Additionally the front end parser may execute any initial CSP commands as needed.

In continuing to execute the ring buffer of the process may flow to step wherein additional logical blocks of the 3D pipeline are communicated to back end parser thus signifying the approaching end of the execution of the ring buffer. Finally in step the CSP executes the back end portion of any command that stores all states resulting from the processed operation.

The process of of fetching the head and tail pointer of a ring buffer and processing continues until the end of the ring buffer is found or until interrupted. is a flowchart diagram depicting the operation of the CSP of as it executes a ring buffer structure and searches for a ring buffer end command. In step of the CSP may be idle awaiting for receipt of a ring buffer for execution. In step which may correspond to step of the CSP fetches the head and tail pointer of a ring buffer such as ring buffer of context of run list even in and awaits in step for the head and tail pointer to be referenced and ultimately filled in step . In decision step of if a restore context command is encountered the operation as referenced in moves to step for loading the restored state which also corresponds to block of . Once the load state is done the operation moves to step such that the ring buffer is fetched and operation may continue as corresponding to step of .

If a restore command is not activated or recognized at step a determination is made whether the head pointer is equal to tail pointer meaning whether the context is empty thereby corresponding to step of . If the head pointer does not equal the tail pointer then the ring buffer contains operations for processing thereby causing the process to move from step to step as described above. At step the current pointer is moved from its initial position toward the tail pointer and with each calculation a determination is made whether the current pointer which may be the head pointer equals the tail pointer. As processing continues the head pointer or current pointer moves toward the tail pointer to where it ultimately equals the tail pointer. Step may repeat in a loop several times until the current pointer CUR PTR becomes equal to the tail pointer TAIL PTR .

When the current pointer or head pointer reaches the tail pointer step is reached which causes the CSP to wait for a tail pointer update. As shown in the tail pointer may be moved if additional commands or pointers are added to the ring buffer during processing. However if the tail pointer is not moved the process moves to step which connotes the end of the ring buffer as shown in . The CSP returns to an idle state and prepares to repeat the process described above.

Ring buffer data structure in memory may be accessed for restoration of this particular context that is in this nonlimiting example. As similarly described above the ring buffer data structure may contain a tile head pointer slot that may be updated by the GPU during processing of this context. Likewise ring buffer data structure may contain a ring buffer tail slot that may also be updated as described in regard to . If commands or pointers are added to the ring buffer data structure in then the ring buffer tail slot may be adjusted accordingly.

The ring buffer data fetch sequence may entail that a CSP function leads to the execution of DMA command pointer as also referenced in . The CSP fetches the tile head pointer as shown in in association to the ring buffer data structure which was in the ring buffer data structure when this context in run list was previously interrupted. The CSP may load all of the data corresponding to DMA command pointer to the CSP head pointer which points to the CSP function into the pipeline .

The DMA command pointer references the DMA buffer structure in memory that may at least on one nonlimiting example contain draw command through draw command . In restoring this context the CSP also processes a DMA offset as similarly described above which enables matching of the processing of the context to the precise point where previously interrupted. The DMA offset is the logical distance between the DMA buffer head pointer and the current command pointer for the DMA buffer structure .

In this nonlimiting example of the CSP is configured to recognize that the DMA offset establishes draw command as the point to resume processing. Draw command may be comprised of multiple instances of the draw command . More specifically draw command may include instances that are executed sequentially. After instance is processed in this nonlimiting example processing may then turn to draw command in the DMA buffer structure which as with any draw command may itself have multiple instances.

In reestablishing this context from run list the CSP discards all instances previously executed until matching the instance ID corresponding to the value stored at the previous interrupt. In this nonlimiting example the instance ID points to instance of the multiple instances . Thus the CSP discards all instances until reaching instance for it is this logical position where the instance ID matches.

Each instance of the multiple instances contains one or more primitives which may be sequenced as shown in primitive structure . In this nonlimiting example primitives M may form an instance which M is an integer. The interrupted draw command in the DMA buffer structure is processed by the CSP until the primitive ID matches the value corresponding to the point of prior interrupt which was saved as described above. In this nonlimiting example the primitive ID points to primitive which means that the CSP would skip primitive as primitive was processed prior to the previous interrupt.

Each primitive M references one or more tiles which may be processed for drawing triangle . In this nonlimiting example primitive may contain tiles to construct triangle . Also in this nonlimiting example the tile generator TG will skip tiles until reaching the tile ID that references tile thereby corresponding to the tile ID value stored in memory for the point where previously interrupted.

In this manner the data structure depicted in illustrates at least one method for restoring a graphics operation at the precise point where the operation may have been previously interrupted. By the CSP skipping the draw commands instances primitives and TG skipping tiles until reaching the IDs corresponding to the values saved when the context was previously interrupted processing can resume quickly at the correct point thereby avoiding duplicative processing of previously processed data.

In in step the CSP fetches the restored context from the ring buffer which may be ring buffer of . In so doing the ring buffer tile head pointer is accessed so as to ascertain the precise context restart address or logical location in ring buffer . In the nonlimiting example of the tile head pointer points to DMA command pointer .

Continuing to step the CSP processes the DMA command as similarly described above and operates to match the DMA offset also in to the correct draw command in the DMA buffer structure . Upon identifying the precise draw command which in the nonlimiting example of is draw command the CSP moves to step and matches the instance ID and the primitive ID as discussed above. In making these matches the CSP identifies the precise triangle where processing was previously interrupted.

At that point the CSP may identify the precise tile ID where prior processing was interrupted so that the tile generator may finish processing the triangle as shown in step . After this precise DMA command draw command instance ID primitive ID and tile ID are identified the context can be fully restored in the 3D pipeline as if it had not previously been interrupted that is in at least one nonlimiting example.

In instances where a triangle may have been partially processed when previously interrupted due to a context change a triangle ID may be forwarded to the TSU . Individual triangles within a draw primitive having a single primitive ID may have unique triangle IDs some of which may have been processed in whole or in part at the time the context was previously interrupted. In this instance a tessellated triangle ID which may be generated by an execution unit may be forwarded to the TSU for resuming processing operations on this partially processed primitive as shown in step of . The result is that a precise triangle is forwarded to the tile generator that may be matched to a corresponding tile ID as discussed above. So regardless of whether a primitive has been previously tessellated or not the resumption of processing of a previously interrupted context can be seamless and precise.

Thus far the focus of this disclosure has been on the structure and the switching of contexts upon receipt of an interrupt command from the processing unit of including the data structure that provides for precise restoration. However when a context is interrupted for future restoration the GPU and more particularly the 3D pipeline of should be configured so as to terminate the interrupted process at a logical point so that a next process whether restored or not may be executed by the 3D pipeline in accordance with the process described above.

At least one nonlimiting example prescribes that a context may be saved and a new context restarted in approximately one to two million cycles which should generally be sufficient to provide enough wrap up time for some processing so as to minimize the state that may be tracked and to minimize the complexity when a save state is restarted subsequently in time. Consequently as described above one logical location to break the 3D pipeline of is at tile generator . However as an alternate nonlimiting example another location of breakpoint could be ZL unit instead of tile generator . In this nonlimiting example the same rules and IDs could apply and the saved Tile ID would go to the ZL unit to be compared.

However according to the nonlimiting example where the break is at the tile generator any tiles that are already admitted by the tile generator to the remaining portions of the pipeline including ZL unit and the subsequent units up until the time of a context switch is signaled by the processing unit will be allowed to drain through the 3D pipeline . However any triangles or tiles that have not reached the tile generator when an interrupt is received by the GPU may be in this nonlimiting example discarded and regenerated when the context is subsequently restored. Stated another way for the portion of the 3D pipeline above the tile generator all processing results are discarded and are subsequently regenerated when the context is restored see .

At least one reason for interrupting at the tile level at tile generator is that except in the case of extremely long pixel shader programs the 3D pipeline may be configured to process all tiles in the pipeline below the tile generator within the target one to two million cycles. As a nonlimiting example if an interrupt is configured at the triangle setup unit of it is possible that the pipeline may not be able to drain in the 1 to 3 milliseconds desired in this nonlimiting example. Interrupting on a scale smaller than tiles may not have much impact on how fast the pipeline can be drained. By configuring interrupts at the tile generator level of the pipeline a certain amount of processing may continue a certain amount of processing may be aborted and the point of interrupt may be saved for subsequent restart. This nonlimiting example results in that some data will be reparsed and repeated in order to restore the 3D pipeline to the point in which it was stopped at the interrupt.

According to at least one nonlimiting example in order for the CSP to know where in the command parsing that it will need to restart a context the CSP may be configured to communicate a token internal fence through the 3D pipeline to the tile generator and then back to the CSP whenever the DMA buffer context is switched. According to this nonlimiting example the CSP may then know when it is safe to discard a DMA buffer. Also this nonlimiting example provides that the position in the DMA buffer corresponding to the processing position in the tile generator is recorded in this way with each new draw command such as shown in in DMA buffer . Thus when a context such as context in run list even in is subsequently restored parsing can start from the draw command that was interrupted such as a draw command in buffer .

If however an interrupt event is recognized in step the CSP may move to step and generate an interrupt signal that may be electrically communicated to one or more of the processing blocks of the 3D pipeline . As stated above a certain portion of the pipeline may be immediately discarded as the continued processing of the upper portions of the pipeline may result in unsatisfactory context switch times due to the delay in clearing the top portion blocks. Yet step provides that a predetermined number of blocks of 3D pipeline receive the interrupt signal as a result of a dedicated communication path with the CSP .

In addition to generating the interrupt signal in step the CSP may also generate an interrupt token to memory FIFOs in the 3D pipeline . This interrupt token operates as a fence between the interrupted context and a next or restored context such as in run list of . The interrupt token fence communicates to each architectural block in the 3D pipeline that the changeover to a next or restored context is complete.

As discussed above the current DMA offset instance ID primitive ID and tile ID may be sent to the CSP context save buffer which may be state FIFO of . More specifically the 3D architectural blocks use paths etc. to forward this information to the CSP as similarly described above.

Thereafter in step each 3D pipeline architectural block such as one or more blocks shown in discard old context entries in the pipeline until the interrupt token generated in step is reached and identified. The old context is the interrupted context. Stated another way upon receipt of the interrupt signal on the dedicated communication path in step the architectural blocks discard associated FIFOs until receiving the interrupt token which signifies that all commands thereafter belong to the next or restored state which are processed in step .

This process depicted in is described in greater detail below in regard to FIGS. and in regard to the individual components of the 3D pipeline . Beginning with when the GPU receives the command from the processing unit to interrupt a process being processed the CSP communicates the hardwired signal to the execution unit pool front module which is shown in as EUP FRONT via hardwire interrupt line . This same hardwire interrupt line is also electrically coupled to tile generator as shown in . Additionally hardwire interrupt may also be coupled to triangle setup unit and attribute setup unit all in accordance with step of .

The tile generator may maintain a counter for the tile number and pipeline registers for triangle number and primitive number of the last tile emitted. This information is sent back to the CSP via data path to be saved as part of the interrupted context state. This information references the position in the command stream where the GPU should start processing again when the saved context is later restored.

The hardwire interrupt signal communicated on line is also communicated to each of triangle setup units and attribute setup unit . Upon receipt of the hardwire interrupt signal on line each of the triangle setup unit attribute setup unit and tile generator immediately discard all data being processed and cease further operations on that particular context as also described in step of . The CSP in addition to issuing the wire signal interrupt on line passes an interrupt end token which may be represented as INT End token in this nonlimiting example down the 3 D pipeline so as to flush all dirty lines in the processing stream as described in step of . This interrupt end token is communicated from the CSP to the triangle setup unit to the attribute setup unit and through the rest of the pipeline .

Upon receiving the interrupt signal on line which may be shown in as the CSP TSU INT signal the triangle setup unit moves to step which prescribes that the triangle setup unit reads checks and discards the EUP FIFO memory . The EUP FIFO memory is a FIFO storing data passed from the EUP Front module to the triangle setup unit . The triangle setup unit invokes a discard loop as shown in to discard the contents of the EUP FIFO memory until reaching the interrupt end token which represents the discarding of all data for the context being saved.

Upon reaching the interrupt end token at EUP FIFO memory the triangle setup unit returns to the CSP FIFO memory to read check and discard its contents in similar fashion as performed on EUP FIFO memory as shown in step of . Triangle setup unit engages in a discard loop to discard all contents in the CSP FIFO memory until reaching the interrupt end token representing the end of the context being saved. Consequently one of ordinary skill in the art would realize that data associated with the context being interrupted is discarded at this stage of the 3D pipeline . The triangle setup unit ultimately moves from step to step in which prescribes the dumping and or resetting of the query state machine in preparation of the next context to be executed. After checking the next FIFO entry type in CSP FIFO memory the triangle setup unit may return to normal operations in step to process a new next context.

In regard to dump reset query state machine stage is a simplified diagram of the process executed by the dump reset query state machine DRQ state machine in each unit of the 3D pipeline of . While the DRQ state machine may initially be in normal operating mode upon execution of a command the DRQ state machine moves to step . Movement to step pertains to a CSP command decode operation which informs the DRQ state machine i.e. TSU etc. what to do next. In the case when an interrupt end token is received by a unit such as triangle setup unit step follows so as to forward the interrupt end token down the 3D pipeline . Thereafter the unit such as triangle setup unit returns to normal operations in step regarding the new context.

After the interrupt end token is processed as described above in regard to and triangle setup unit focus shifts to the attribute setup unit of . As described above the attribute setup unit also receives the wire interrupt signal on line thereby notifying the attribute setup unit to immediately discard all contents related to the current context.

As described above attribute setup unit forwards the interrupt end token to ASU FIFO memory which is ultimately communicated to span generator unit . is a diagram of the process implemented by span generator unit in regard to the handling of the interrupt end token communicated down the 3D pipeline of . While the span generator unit may operate in step to execute normal operations upon checking the entry type in step and recognizing the interrupt end token as communicated from the ASU FIFO memory the span generator unit moves to step . In step the interrupt end token is forwarded on to the tile generator unit thereafter causing the span generator unit to return to step to check the next entry type. In the case of a next context command following the interrupt end token the span generator unit may return to normal operations as in step .

As described above the tile generator is configured to receive a hardwire interrupt signal on line communicated by CSP after being issued by the processing unit of . is a diagram of the process flow implemented by the tile generator upon receipt of an interrupt command from the CSP of . When the hardwire interrupt signal is communicated to the tile generator on line the tile generator operates in step to check the header type and the command associated with the interrupt received from CSP .

Upon determining that an interrupt signal has been received on line the tile generator moves to step to immediately forward a tile generator interrupt token to the Z unit level module ZL module in . One of ordinary skill in the art would understand additionally that the tile generator interrupt token that is communicated to the ZL module is communicated in advance of receipt of the interrupt end token being communicated down the pipeline as described above. This tile generator interrupt token is communicated so as to flush all FIFOs and caches subsequently coupled to the tile generator but to otherwise allow all data in advance of the tile generator interrupt token to be processed in association with the context being saved.

In step the tile generator engages in a discard loop to discard the input entry and check for the interrupt end token being communicated down the pipeline as described above. Ultimately upon execution of step the interrupt end token will reach the tile generator through the 3D pipeline of . At that point the tile generator moves to step as similarly described above. Thereafter the tile generator may check the header type of the next instruction communicated to determine its type and may return to normal operations as in step which may be associated with a next context that has been restored.

The next module in the 3D pipeline of is the ZL module . is a flowchart diagram of the ZL module of as it may operate in regard to receiving a tile generator interrupt token from tile generator unit .

In a first step read decode the ZL module reads the entry from TG FIFO memory and processes instructions as received. However in the instance when an instruction is a tile generator interrupt token as communicated by tile generator unit the ZL module moves to step .

In step the ZL module switches to the ASU ZL FIFO memory and thereafter initiates a discard loop as shown in step . In step the ZL module checks and discards all entries in the ASU ZL FIFO memory until reaching the interrupt end token being communicated down the 3D pipeline . Upon receiving the interrupt end token the ZL module moves to the AFIFO memory in step and subsequently initiates discard loop .

In discard loop the ZL module checks and discards all entries in AFIFO memory until reaching the interrupt end token contained in AFIFO memory . After cleaning these two FIFO memories and the ZL module switches to the TG FIFO memory in step and initiates yet another discard loop as shown in step . In step the ZL module checks and discards all entries in the TG FIFO memory until reaching the interrupt end token in similar fashion as described above. Thereafter the DRQ state machine step is implemented as similarly described above so that the ZL module returns to step for the next instruction after the interrupt end token. Subsequently the ZL module begins processing a next context in normal operation step .

After the interrupt end token is received by ZL module as described above it is forwarded to ZL FIFO memory and then ultimately to Z unit level module hereinafter ZL module . Unlike as described above ZL module does not discard all FIFOs but instead continues processing operations in regard to the context being saved due to the fact that continued processing can be completed within the target 1 to 2 million cycle window that is in at least this nonlimiting example. Nevertheless the interrupt end token is ultimately received from the ZL FIFO memory representing the end of the saved context and the beginning of the new and or restored context.

The interrupt end token is further communicated throughout the remaining portions of the 3D pipeline as one of ordinary skill in the art would know. As described above the interrupt end token may be communicated to additional architectural components to flush all dirty lines associated with the data preceding the interrupt end token.

Consequently as disclosed herein the graphics pipeline of the graphics processor may change states or processing operations as directed to increase the efficiency of graphics processing operations as a whole. In instances where a certain operation needs to wait on other information and data before concluding its own processing operation the graphics pipeline may be interrupted and quickly oriented to execute another context or operation so that the pipeline is not idle. Thus the graphics pipeline as disclosed herein may realize more efficient operation as a nonlimiting example by resolving situations that may previously resulted in a bottleneck in the graphics pipeline. Instead this disclosure enables fast transitions between different contexts to avoid bottleneck situations.

The foregoing description has been presented for purposes of illustration and description. It is not intended to be exhaustive or to limit the disclosure to the precise forms disclosed. Obvious modifications or variations are possible in light of the above teachings. The embodiments discussed however were chosen and described to illustrate the principles disclosed herein and the practical application to thereby enable one of ordinary skill in the art to utilize the disclosure in various embodiments and with various modifications as are suited to the particular use contemplated. All such modifications and variation are within the scope of the disclosure as determined by the appended claims when interpreted in accordance with the breadth to which they are fairly and legally entitled.

