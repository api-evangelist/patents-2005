---

title: Resource accessing with locking
abstract: Thread usage is managed when locking resources during the possibly-contentious accessing of such resources. In a described implementation, a thread that is executing a corresponding access request is not suspended when its corresponding access request is delayed because a targeted resource is currently locked for accessing by another thread. Instead, when a targeted resource is locked, the blocked access request is queued up in a queue of access requests. The corresponding thread is then permitted to perform other work. When the resource is subsequently unlocked and thus becomes available, an access request (e.g., the oldest or highest priority access request) that is queued is retrieved, and a thread is allowed to execute it. Implementations for general locking access schemes, for read/write-bifurcated locking access schemes, etc. are described. Implementations are described from conceptual, functional, temporal, code or function, contention, thread, and other perspectives.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07603502&OS=07603502&RS=07603502
owner: Microsoft Corporation
number: 07603502
owner_city: Redmond
owner_country: US
publication_date: 20050412
---
This disclosure relates in general to accessing a resource and in particular by way of example but not limitation to locking a resource during the accessing thereof in the context of thread management.

In electronic devices and computer networks generally many resources are shared or are otherwise subject to multiple access requests. Some mechanism is usually employed to handle these multiple access requests without causing data corruption and or other errors due to uncoordinated accessing and unmanaged interference. The mechanism that is usually employed is a locking mechanism.

A legend for indicates that threads are represented by diagonal squiggly lines and that APIs are represented by rectangles. Hence acquire lock and release lock are APIs. Access requests on the other hand are represented by squares. Access requests include access request access request and access request .

At acquire lock enables a lock to be secured on resource for access request on behalf of the thread thereof. At the thread of access request accesses resource . At some point after a lock is secured on resource at access request is accepted by acquire lock . Because resource is already locked the thread of access request is suspended at . Similarly when access request is received acquire lock also causes the thread of access request to be suspended at .

When the thread of access request completes the accessing of resource release lock releases the access lock on resource . At release lock causes a thread that has been suspended to be awakened so that the access request thereof may be serviced by enabling access to resource . With this general locking scheme of resource may be locked regardless of whether an access request is a read from or a write to resource .

Access requests are also divided into read access requests R and write access requests W . Specifically two read access requests R and R and two write access requests W and W are shown. Because read access requests R do not modify resource multiple read access requests R may be concurrently serviced. Hence at multiple concurrent reads are permitted by acquire reader lock R .

While resource is locked to service one or more reads threads of write access requests W are suspended at by acquire writer lock W . The completion of all reads is awaited at . After all reads are completed release reader lock R releases the lock on resource and at awakens a previously suspended write thread.

At block a thread for write access request W is awakened. Acquire writer lock W locks resource at . At write access request W writes to resource . Upon completion of the write release writer lock W unlocks resource and at awakens the thread for the next write request W e.g. write access request W . Although this reader writer locking scheme enables concurrent read accesses to expedite the overall handling of both read and write access requests it still entails significant overhead.

Accordingly there is a need for schemes mechanisms techniques etc. that can efficiently facilitate the handling of access requests while reducing attendant processing overhead.

Thread usage is managed when locking resources during the possibly contentious accessing of such resources. In a described implementation a thread that is executing a corresponding access request is not suspended when its corresponding access request is delayed because a targeted resource is currently locked for accessing by another thread. Instead when a targeted resource is locked the blocked access request is queued up in a queue of access requests. The corresponding thread is then permitted to perform other work. When the resource is subsequently unlocked and thus becomes available an access request e.g. the oldest or highest priority access request that is queued is retrieved and a thread is allowed to execute it. Implementations for general locking access schemes for read write bifurcated locking access schemes etc. are described. Implementations are described from conceptual functional temporal code or function contention thread and other perspectives.

Other method system approach apparatus device media procedure API arrangement etc. implementations are described herein.

As described herein above existing schemes that lock resources during the accessing thereof suspend a given thread that is executing an access request until the resource is free and it is the given thread s turn to access the resource. This forces the thread to sit idle for some period of time. During this idle time period the thread is not able to perform work by executing code. Additionally maintaining the thread even in a suspended state occupies system resources. Moreover switching between threads termed context switching consumes processing resources and introduces processing delays.

In a described implementation on the other hand a thread that is executing an access request is not suspended. The thread is not suspended even when the corresponding access request is delayed because a targeted resource is currently locked for accessing by another thread for its corresponding access request. When a targeted resource is locked the blocked access request is queued up in a queue of access requests and the thread is permitted to perform other work. When the resource is unlocked and becomes available an access request e.g. the oldest or highest priority access request that is queued is retrieved and a thread is assigned to execute it.

Device also includes an acquire resource lock ARL function and a release resource lock RRL function . Acquire resource lock and release resource lock facilitate the acquisition and release respectively of locks on resource during the servicing of access requests . The general locking scheme of effectively implements a mutually exclusive or mutex type of lock.

A legend for indicates that threads are represented by diagonal squiggly lines and that APIs are represented by rectangles. Hence acquire resource lock and release resource lock may be realized and exposed as APIs in a described implementation. Access requests on the other hand are represented by squares instead of rectangles. Each access request is a request for some kind of access to resource .

As illustrated access requests include access request access request and access request . At a lock on resource is secured for access request by acquire resource lock on behalf of the thread corresponding thereto. At the thread corresponding to access request performs work by accessing resource in accordance with the request of access request .

Meanwhile one or more other access requests can be received at acquire resource lock . Access requests and are also received at acquire resource lock . In a described implementation such subsequent access requests may be received at any time between when acquire resource lock first begins to handle access request and when the lock is ultimately released e.g. by release resource lock . Alternatively this time frame may be limited or otherwise changed.

Regardless when acquire resource lock receives an access request while resource is already locked acquire resource lock does not suspend the thread corresponding thereto. Instead the received access request is queued for subsequent handling. The thread corresponding to the received access request can therefore be released and freed to perform other work. Also the state and or context information for the thread does not need to be maintained by device . Additionally context switching can be reduced or even eliminated. Any or all of these consequences can increase the processing efficiency and or improve the overall performance of resource and or device .

At the thread corresponding to access request is not suspended and acquire resource lock queues access request . Similarly at the thread corresponding to access request is not suspended and acquire resource lock queues access request . Access requests and are queued in queue of access requests by acquire resource lock .

In a described implementation queue of access requests includes one or more access request identifiers . Queue of access requests may be realized for example as a first in first out FIFO queue a linked list items on a stack some combination thereof and so forth. Each respective access request identifier identifies an associated respective access request . This identification may be a code a pointer an address a function name or location some combination thereof and so forth.

As illustrated queue of access requests includes n access request identifiers . . . n . Specifically queue of access requests includes an access request identifier an access request identifier . . . and an access request identifier n n . If for example queue of access requests is realized as a FIFO queue with access request identifier being the newest entry and access request identifier n being the oldest entry then access request identifier is associated with access request and access request identifier is associated with access request .

Access requests can therefore be handled as locks are released without having one or more threads being suspended by referring to queue of access requests . At when the lock on resource for access request is released by release resource lock the next access request is retrieved from queue of access requests . This retrieval may be effectuated by release resource lock and or acquire resource lock . Specifically in this example access request identifier n n is retrieved from queue of access requests because it is the oldest entry in the queue. The access request that is associated with access request identifier n n is then handled by acquire resource lock .

The general locking scheme of may be implemented in any of many possible alternative manners. For example queue of access requests may be located within device or may be external thereto. Additionally although access requests are shown as arriving at device from an external source access requests may instead originate internal to device .

Access requests are also bifurcated into read access requests R and write access requests W . Specifically two read access requests R and R and two write access requests W and W are shown. Because read access requests R do not modify resource multiple read access requests R may be concurrently serviced. Hence at multiple concurrent reads are permitted by read from resource R .

While resource is locked to service one or more read accesses threads corresponding to write access requests W are not suspended by write to resource W . Instead write access requests W are queued up in queue of write requests W . In a described implementation queue of write requests W includes one or more write request identifiers W . As illustrated queue of write requests W includes n write request identifiers W . . . Wn . Specifically queue of write requests W includes a write request identifier W a write request identifier W . . . and a write request identifier n Wn .

At instead of suspending the threads corresponding to write access requests W write to resource W queues write request identifiers W in queue of write requests W . The completion of all read accesses is awaited at .

At when all of the one or more read access requests R are completed release lock for reads R releases the read lock on resource and release lock for reads R and or release lock for writes W activates the next write access request W from queue of write requests W .

At write to resource W retrieves the next write access request W from queue of write requests W . Specifically the next e.g. in terms of temporal and or priority primacy write request identifier W is retrieved from queue of write requests W and the associated write access request W e.g. write access request W is handled.

At resource is locked by write to resource W . At the thread corresponding to write access request W performs work by writing to resource in accordance with the write request of write access request W .

At when the write is completed release lock for writes W releases the write lock on resource and activates the next write access request W of queue of write requests W . Release lock for writes W and or write to resource W may actually retrieve the next write request identifier W from queue of write requests W .

With the reader writer locking scheme of concurrent reading is enabled while the overhead of thread suspension is avoided. Handling read and write requests without suspending request handling threads can also be described in terms of no contention versus contention scenarios. In other words the manner in which read and write requests are handled can depend on whether there is contention for the target resource.

For the write request example at a writing function is passed to a write to resource function. At the writing function is called. At the writing function returns. At at least one queue for pending reads and or pending writes is checked. At the request handling process continues. As is described more fully herein below with particular reference to a single thread can perform all of the requisite work for handling a read request or a write request in a no contention scenario. Also the request handling thread is not suspended.

For the write request example at a writing function is passed to a write to resource function. At the write request is queued but the request handling thread is not suspended. At the function returns. At the request handling process continues.

As is described more fully herein below with particular reference to at least two threads may be required for accomplishing all of the requisite work for handling a single read request or a single write request in a contention scenario. However because the request handling thread is not suspended significant attendant thread management overhead is avoided.

Although only a single e.g. write oriented request queue is illustrated in and described in conjunction therewith one or more queues may be implemented differently. For example a single queue may hold both pending read requests and pending write requests. Alternatively there may be one queue for pending read requests and another queue for pending write requests.

These example general functions may be designed and provided by a user for instance. The two example access callback functions and are indicative of many possible types of specific functions that enable specific types of accessing of a given resource. Other examples in addition to read and write include access query modify manipulate set some combination thereof and so forth.

In a described implementation lock specific functions include write to resource WTR function W read from resource RFR function R WTR helper function and RFR helper function . Write to resource W and read from resource R are described herein both above and below. WTR helper and RFR helper are described further herein below with particular reference to .

The lock related functions can be implemented as APIs. Moreover they can be categorized i as public or user available APIs or ii as private or internal APIs. In a described implementation write to resource W and read from resource R are implemented as public functions and WTR helper and RFR helper are implemented as internal functions.

At the thread executes WTR helper which checks whether there are any pending write access requests on a write request queue. At the thread returns to write to resource W . Afterwards the thread executes request completion event at .

As is apparent from the thread workflow that is illustrated in a single thread is able to effectuate a write access request in the no contention case. It should be noted that in a real world programming environment returning functions return to the function that called them. Consequently in accordance with commonly applied programming practices the thread may actually switch from executing write to resource W to WTR helper which then calls write access callback function so that write access callback function can directly return to WTR helper .

When the queued write access request is retrieved a thread B executes WTR helper at B . WTR helper calls write access callback function . At B thread B executes write access callback function which writes to the targeted resource. At B write access callback function returns to WTR helper . After executing WTR helper which may entail checking a queue of read and or write access requests thread B is finished with the write access request at B .

As is apparent from the thread workflow that is illustrated in at least two different threads are involved in handling and ultimately completing or retiring a write access request in the contention case. In this sense two different threads means at least that a thread performs work on behalf of an access request performs some other work and then comes back to perform additional work on behalf of the access request. This interpretation is especially true with a system employing a thread pool in which threads are assigned to functions that are queued up to the thread pool. In a thread throttled environment a number of threads being assigned from the thread pool to functions is limited to the number of available CPUs on a device. This can reduce if not prevent context switching.

A specific exemplary detailed algorithm is described in this section. However reader writer locking schemes may be implemented in alternative manners. Some alternative manners are described in preceding sections and or noted as optional approaches in this section.

When a thread executing a function wants access to a resource it calls an overall access API and passes to it the address of a callback function. If the resource is available the calling thread calls the callback function which manipulates the resource and returns. Thereafter the access API returns. Therefore in no contention scenarios one thread can do all of the work. There is no thread contention and performance is not adversely impacted by thread management overhead.

In a contention scenario on the other hand the address of the callback function is added to a queue and the access API returns immediately. Furthermore the calling thread is not suspended waiting for the desired resource consequently the thread is free to do other work. When the thread that currently owns the resource returns from its callback method the access API grabs the next callback out of the queue and executes it.

If a writer callback is being released then the writer callback is queued up to a thread pool. Alternatively it can be executed on the same thread with likely lower ultimate performance. Therefore there is no contention for the CPU and performance is not adversely impacted. However if reader callback s are being released then multiple reader callbacks are queued to a thread pool. While this may seem like too many threads are attempting to run concurrently and possibly contending for the limited number of CPU s this would not be the case if the thread pool uses an I O completion port which is provided by certain Windows operating systems from Microsoft Corporation of Redmond Wash. because the I O completion port only releases as many thread s as there are CPU s in the device. This prevents CPU contention and thread context switches therefore performance is not adversely impacted when handling multiple readers.

In addition to maintaining one or more access request queues reader writer locking schemes as described in this section also maintain state information as described below. Such a reader writer locking scheme may be realized for example using a lock object having the following example state information 

If on the other hand the lock is determined to be neither free nor OBR at block then at block it is determined if the lock is OBRAWP. If true the lock is OBRAWP then the read request is queued up at block as indicated by m qReaders.Enqueue callback . After enqueuing the read request the overall method returns at block . If the lock is not set to OBRAWP as determined at block then at block it is determined if the lock is OBW. If the lock is determined to be OBW then the read request is queued up at block . If the lock is not determined to be OBW at block then the method of flow diagram continues at block e.g. to check if the lock has recently been freed .

If on the other hand the value of m NumReadersReading is not greater than zero as determined at block then at block it is determined if the value of m qWriters.Count is greater than zero. If it is greater than zero then at block the lock is set equal to OBW and a write request is retrieved from the write queue as indicated by Call m qWriters.Dequeue . The overall method then returns at block . If on the other hand the value of m qWriters.Count is not greater than zero as determined at block then at block the lock is set to free. The overall method then returns at block .

If on the other hand the lock is determined to not be free at block then at block it is determined if the lock is OBW. If true the lock is OBW then the write request is queued up at block as indicated by m qWriters.Enqueue callback . After enqueuing the write request the overall method returns at block . If on the other hand the lock is not set to OBW as determined at block then at block it is determined if the lock is either OBR or OBRAWP.

If the lock is determined to be either OBR or OBRAWP at block then at block the lock is set to OBRAWP in case it was previously OBR and the write request is queued up at block . If the lock is determined to be neither OBR nor OBRAWP at block then the method of flow diagram continues at block e.g. to check if the lock has recently been freed .

If on the other hand the value of m qWriters.Count is not greater than zero as determined at block then at block it is determined if the value of m qReaders.Count is greater than zero. If it is greater than zero then at block i the lock is set equal to OBR ii m NumReadersReading is set equal to m qReaders.Count and iii all read requests are retrieved from the read queue as indicated by Call m qReaders.Dequeue for all readers . The overall method then returns at block . If on the other hand the value of m qReaders.Count is not greater than zero as determined at block then at block the lock is set to free. The overall method then returns at block .

Optionally system efficiency can be further enhanced by employing one or more of the following thread utilization strategy examples. For no contention scenarios the thread calling the API can execute the callback too. In other words one thread can execute all of the functions to effectuate a e.g. read or write access. Consequently it is a fast mechanism and no context switch is involved. If readers already own the lock and the thread wants to read the thread can proceed to read from the resource. For contention scenarios the callback method is queued therefore the thread can return essentially immediately so that it can perform other work because the locking mechanism does not put the thread into a wait state.

When handling callback method returns thread utilization strategy examples can also be employed. During writer callback method returns if there is at least one more writer in the write request queue then the thread executes the next writer without involving a context switch. If there are no more writers in the write request queue reader callback methods are queued to a thread pool. The thread pools wakes and assigns a number of threads that is equal to a number of available CPUs to process the read requests that are in the read request queue. Because the number of threads is throttled with regard to the number of available CPUs no context switching results. During reader callback method returns after a given thread completes execution of a final reader callback method return the given thread then executes a first writer callback method return without involving a context switch.

As described above access request queue s can be implemented in alternative manners. Example alternative manners include one queue for all e.g. undifferentiated access requests one combined queue for read and write access requests a separate queue for read requests and a separate queue for write requests and so forth. The manner in which requests are retrieved and or drained from such queue s is also variable. Alternative examples include i retrieving and completing only one write request or a predetermined number of write requests greater than one before returning to the read request queue and ii retrieving and completing only a limited number of read requests before returning to the write request queue e.g. because even with concurrency a large number of read requests are handled somewhat sequentially because of the non infinite number of CPUs . Other factors may also be considered such as the length of time an access request has been queued up regardless of whether it is a write or read request relative priorities of read and write requests at a group level and or for individual requests and so forth.

By way of further explanation and example only the description herein above in this section that references may be modified for implementation s that do not differentiate between read and write requests. Such implementation s lock the resource for any accessing. There is less state information to be maintained and one public API fewer. Also the states become free and OBW which may be reconsidered as and renamed as owned by accesser OBA .

Modifications include eliminating the separate functions for reading which are read from resource RFR of and reader callback returns RCR of . Also blocks and of are not relevant and may be omitted for implementation s that do not bifurcate read and write accesses. In blocks and are not relevant and may be omitted. The counts queues etc. may also be reconsidered and renamed such that access or similar replaces write .

The devices actions aspects features functions procedures modules components etc. of are illustrated in diagrams that are divided into multiple blocks. However the order interconnections interrelationships layout etc. in which are described and or shown is not intended to be construed as a limitation and any number of the blocks can be modified combined rearranged augmented omitted etc. in any manner to implement one or more systems methods devices procedures media apparatuses APIs arrangements etc. for resource accessing with locking. Furthermore although the description herein includes references to specific implementations including a general device of the illustrated and or described implementations can be implemented in any suitable hardware software firmware or combination thereof and using any suitable threaded operating environment s resource s user function s programming paradigm s API division s access request queue division s and or organization s and so forth.

Example operating environment is only one example of an environment and is not intended to suggest any limitation as to the scope of use or functionality of the applicable device including computer network node entertainment device mobile appliance general electronic device etc. architectures. Neither should operating environment or the devices thereof be interpreted as having any dependency or requirement relating to any one or to any combination of components as illustrated in .

Additionally implementations for resource accessing with locking may be realized with numerous other general purpose or special purpose device including computing system environments or configurations. Examples of well known devices systems environments and or configurations that may be suitable for use include but are not limited to personal computers server computers thin clients thick clients personal digital assistants PDAs or mobile telephones watches hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics video game machines game consoles portable or handheld gaming units network PCs videoconferencing equipment minicomputers mainframe computers network nodes distributed or multi processing computing environments that include any of the above systems or devices some combination thereof and so forth.

Implementations of resource accessing with locking may be described in the general context of processor executable instructions. Generally processor executable instructions include routines programs protocols objects functions interfaces components data structures etc. that perform and or enable particular tasks and or implement particular abstract data types. Locking resource accessing realizations as described in certain implementations herein may also be practiced in distributed processing environments where tasks are performed by remotely linked processing devices that are connected through a communications link and or network. Especially but not exclusively in a distributed computing environment processor executable instructions may be located in separate storage media executed by different processors and or propagated over transmission media.

Example operating environment includes a general purpose computing device in the form of a computer which may comprise any e.g. electronic device with computing processing capabilities. The components of computer may include but are not limited to one or more processors or processing units a system memory and a system bus that couples various system components including processor to system memory .

Processors are not limited by the materials from which they are formed or the processing mechanisms employed therein. For example processors may be comprised of semiconductor s and or transistors e.g. electronic integrated circuits ICs . In such a context processor executable instructions may be electronically executable instructions. Alternatively the mechanisms of or for processors and thus of or for computer may include but are not limited to quantum computing optical computing mechanical computing e.g. using nanotechnology and so forth.

System bus represents one or more of any of many types of wired or wireless bus structures including a memory bus or memory controller a point to point connection a switching fabric a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. By way of example such architectures may include an Industry Standard Architecture ISA bus a Micro Channel Architecture MCA bus an Enhanced ISA EISA bus a Video Electronics Standards Association VESA local bus a Peripheral Component Interconnects PCI bus also known as a Mezzanine bus some combination thereof and so forth.

Computer typically includes a variety of processor accessible media. Such media may be any available media that is accessible by computer or another e.g. electronic device and it includes both volatile and non volatile media removable and non removable media and storage and transmission media.

System memory includes processor accessible storage media in the form of volatile memory such as random access memory RAM and or non volatile memory such as read only memory ROM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules instructions that are immediately accessible to and or being presently operated on by processing unit .

Computer may also include other removable non removable and or volatile non volatile storage media. By way of example illustrates a hard disk drive or disk drive array for reading from and writing to a typically non removable non volatile magnetic media not separately shown a magnetic disk drive for reading from and writing to a typically removable non volatile magnetic disk e.g. a floppy disk and an optical disk drive for reading from and or writing to a typically removable non volatile optical disk such as a CD DVD or other optical media. Hard disk drive magnetic disk drive and optical disk drive are each connected to system bus by one or more storage media interfaces . Alternatively hard disk drive magnetic disk drive and optical disk drive may be connected to system bus by one or more other separate or combined interfaces not shown .

The disk drives and their associated processor accessible media provide non volatile storage of processor executable instructions such as data structures program modules and other data for computer . Although example computer illustrates a hard disk a removable magnetic disk and a removable optical disk it is to be appreciated that other types of processor accessible media may store instructions that are accessible by a device such as magnetic cassettes or other magnetic storage devices flash memory compact disks CDs digital versatile disks DVDs or other optical storage RAM ROM electrically erasable programmable read only memories EEPROM and so forth. Such media may also include so called special purpose or hard wired IC chips. In other words any processor accessible media may be utilized to realize the storage media of the example operating environment .

Any number of program modules or other units or sets of processor executable instructions may be stored on hard disk magnetic disk optical disk ROM and or RAM including by way of general example an operating system one or more application programs other program modules and program data . These processor executable instructions may include for example one or more of a resource to be accessed an access request a lock related function or API state information for a locking scheme some combination thereof and so forth.

A user may enter commands and or information into computer via input devices such as a keyboard and a pointing device e.g. a mouse . Other input devices not shown specifically may include a microphone joystick game pad satellite dish serial port video camera scanner and or the like. These and other input devices are connected to processing unit via input output interfaces that are coupled to system bus . However input devices and or output devices may instead be connected by other interface and bus structures such as a parallel port a game port a universal serial bus USB port an infrared port an IEEE 1394 Firewire interface an IEEE 802.11 wireless interface a Bluetooth wireless interface and so forth.

A monitor view screen or other type of display device may also be connected to system bus via an interface such as a video adapter . Video adapter or another component may be or may include a graphics card for processing graphics intensive calculations and for handling demanding display requirements. Typically a graphics card includes a graphics processing unit GPU video RAM VRAM etc. to facilitate the expeditious display of graphics and performance of graphics operations. In addition to monitor other output peripheral devices may include components such as speakers not shown and a printer which may be connected to computer via input output interfaces .

Computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computing device . By way of example remote computing device may be a peripheral device a personal computer a portable computer e.g. laptop computer tablet computer PDA mobile station etc. a palm or pocket sized computer a watch a gaming device a server a router a network computer a peer device another network node or another device type as listed above and so forth. However remote computing device is illustrated as a portable computer that may include many or all of the elements and features described herein with respect to computer .

Logical connections between computer and remote computer are depicted as a local area network LAN and a general wide area network WAN . Such networking environments are commonplace in offices enterprise wide computer networks intranets the Internet fixed and mobile telephone networks ad hoc and infrastructure wireless networks mesh networks other wireless networks gaming networks some combination thereof and so forth. Such networks and logical and physical communications connections are additional examples of transmission media.

When implemented in a LAN networking environment computer is usually connected to LAN via a network interface or adapter . When implemented in a WAN networking environment computer typically includes a modem or other component for establishing communications over WAN . Modem which may be internal or external to computer may be connected to system bus via input output interfaces or any other appropriate mechanism s . It is to be appreciated that the illustrated network connections are examples and that other manners for establishing communication link s between computers and may be employed.

In a networked environment such as that illustrated with operating environment program modules or other instructions that are depicted relative to computer or portions thereof may be fully or partially stored in a remote media storage device. By way of example remote application programs reside on a memory component of remote computer but may be usable or otherwise accessible via computer . Also for purposes of illustration application programs and other processor executable instructions such as operating system are illustrated herein as discrete blocks but it is recognized that such programs components and other instructions reside at various times in different storage components of computing device and or remote computing device and are executed by processor s of computer and or those of remote computing device .

Although systems media devices methods procedures apparatuses techniques schemes approaches procedures arrangements and other implementations have been described in language specific to structural logical algorithmic and functional features and or diagrams it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features or diagrams described. Rather the specific features and diagrams are disclosed as exemplary forms if implementing the claimed invention.

