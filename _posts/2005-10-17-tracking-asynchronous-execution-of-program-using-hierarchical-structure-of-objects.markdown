---

title: Tracking asynchronous execution of program using hierarchical structure of objects
abstract: The invention builds a structure of software objects that captures the historically contingent development of an asynchronous program. The invention builds software objects that represent the resources and subtasks that make up the asynchronous program. The objects are connected into a hierarchy whose structure explicates interactions among the resources and subtasks. When a fault is detected, the structure tells the debugger everything the program was doing at the time of the fault and lays open the developmental history of the program that led to the fault. The debugger uses this information to trace the detected fault back through code and time to its origin. When a new feature is added, the structure tells maintenance personnel how the new feature affects existing functions. Within the structure, the invention provides mechanisms for handling reference counters and software locks. Groups of resources can be handled together, the structure taking care of coordination.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08032886&OS=08032886&RS=08032886
owner: Microsoft Corporation
number: 08032886
owner_city: Redmond
owner_country: US
publication_date: 20051017
---
This application is a continuation of prior application Ser. No. 09 718 567 filed Nov. 22 2000 entitled METHODS AND SYSTEMS FOR STRUCTURING ASYNCHRONOUS PROCESSES which application is incorporated herein by reference.

The present invention relates generally to computer programming and more particularly to structures and debugging aids for asynchronous processes.

Software programs that handle complex tasks often reflect that complexity in their internal structures and in their interactions with other programs and events in their environment. Subtle errors may arise from mismatches between one part of the program and other parts of the same program or between the program and the other programs in the environment. Mismatches include unexpected events unplanned for sequences of events data values outside the range of normalcy updated behavior of one program not matched by updates in its peers etc.

Software developers and debuggers try to control program complexity in order to avoid or to fix these subtle errors. Sometimes developers control complexity by developing their programs according to the synchronous model of programming. A synchronous program proceeds through the steps of its task in a predictable fashion. The program may consist of a complicated hierarchy of functions with many types of interactions but at every stage in the program the developer and the debugger know what has happened already and what will happen next. The program s structure is imposed on it by the developer and the program does not veer from that structure. Once the structure is understood the debugger can use it to narrow down the areas in the code where an error may be hidden. The structure also makes the program repeatable. The debugger can run through test scenarios over and over again each time refining the results produced by the previous run. The debugger quickly focuses on one small part of the program thus limiting the complexity of debugging. The structure also simplifies the testing of an attempted fix because the structure limits how far effects of the fix can propagate.

Many programs however cannot be written according to the synchronous model. Typically these asynchronous programs respond to events beyond their control. Because events may happen at any time and in any order the program s progression is unpredictable. An asynchronous program builds its structure contingently that is the structure at any given time depends upon the history of events that have already occurred. That history can in turn alter the program s response to events yet to occur.

Run twice there is no expectation that the program will run in an identical manner to produce identical results. Debuggers have a much harder time because they cannot rely on a structure pre imposed by the developer to help them narrow their bug search. Debuggers must instead consider all possible structures that the program may create contingently and must consider the program s reaction to all possible events and to all sequences of events. The debuggers also cannot expect that each test run will be a simple refinement of the previous run. For all practical purposes test results may be irreproducible. Even once a fault is found a change made in an attempt to correct the fault is difficult to test because the effects of the change can propagate throughout the program and beyond into the program s environment. As with fixes so with new features added to an existing asynchronous program maintenance personnel adding a new feature find it difficult to verify that the feature works correctly in all situations and that the new feature does not break some aspect of existing functionality.

Lacking a predefined structure asynchronous programs need to use several mechanisms for communication and control among the subtasks that make up the program. A software object contains a reference counter that records how many subtasks need the information in that object. The software object is deleted when and only when the reference counter goes to zero. Software locks prevent one subtask from altering a data store while another subtask is processing data in that store. However there is often no central arbiter of reference counters and software locks. Coding faults can easily lead to miscounting or misapplication of locks leading to data loss and deadlock or race conditions in which the asynchronous program stops working effectively while separate subtasks wait for each other to complete or to release data.

Microsoft s WINDOWS Development Model takes a first step at capturing the structure of asynchronous processes. Data passing between applications and layered protocol drivers are kept in Input Output Request Packets IRPs . The structure of an IRP s header allows each protocol driver in the stack to record information about its processing of the IRP. Thus by examining the IRP s header a debugger can determine the IRP s history and present state including which protocol driver is currently processing it. However this mechanism is limited because the sequence of protocol drivers invoked must be predicted in advance and because the IRP contains no information about the inner workings of each protocol driver.

What is needed is a way to capture the structure of an asynchronous program as it develops from the program s interactions with other programs and with events in its environment.

The above problems and shortcomings and others are addressed by the present invention which can be understood by referring to the specification drawings and claims. The invention builds a structure of software objects that captures the historically contingent development of an asynchronous program. The structure records the program s development but does not impose limits on that development. Specifically the invention can build software objects that represent the resources and subtasks that make up the asynchronous program. The objects are connected into a hierarchy whose structure explicates the interactions among the resources and subtasks.

Developers using the invention may partition their programs into a hierarchy of small subtasks. The smallness of the subtasks makes debugging and maintaining them easier than debugging and maintaining the larger program. The structure of the hierarchy of subtasks built by the invention provides many of the debugging and maintenance advantages of synchronous programs. When a fault is detected the structure tells the debugger everything that the program was doing at the time of the fault and lays open the developmental history of the program that led to the fault. The debugger may use this information to trace the detected fault back through code and time to its origin. When a new feature is added the structure tells maintenance personnel exactly how the new feature affects existing functions.

Within the structure the invention provides mechanisms for handling reference counters and software locks. When these are implemented with reference to the program structure the chance of miscounting or misapplication is lessened.

The structure gives the developer the freedom to implement more complicated interactions than would have been feasible earlier. Whole groups of subtasks or software objects can be handled together the structure taking care of coordination tasks.

Turning to the drawings wherein like reference numerals refer to like elements the invention is illustrated as being implemented in a suitable computing environment. The following description is based on possible embodiments of the invention and should not be taken as limiting the invention in any way. The first section presents an exemplary hardware and operating environment in which the present invention may be practiced. Section II presents synchronous and asynchronous processing and highlights the differences between them. Section III describes how Input Output Request Packets can be used to capture some of the structure of an asynchronous process. Sections IV through VII describe the Asynchronous Processing Environment APE an implementation of the present invention showing how it captures the structure of an asynchronous process counts object references allows a group of objects to be treated as a group and controls software locks. Debug associations are described in Section VIII. Appendix I contains the complete source code for the asynchronous program highlighted in . Appendix II presents internal implementation details of APE.

The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and configurations that may be suitable for use with the invention include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers and distributed computing environments that include any of the above systems or devices.

The invention may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of the computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include the Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

The computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by the computer and include volatile nonvolatile and removable non removable media. By way of example and not limitation computer readable media may include computer storage media and communications media. Computer storage media include volatile nonvolatile and removable non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media include but are not limited to random access memory RAM read only memory ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVDs or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the computer . Communications media typically embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and include any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communications media include wired media such as a wired network and a direct wired connection and wireless media such as acoustic RF and infrared media. Combinations of the any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and nonvolatile memory such as ROM and RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within the computer such as during start up is typically stored in ROM . RAM typically contains data and program modules that are immediately accessible to or presently being operated on by processing unit . By way of example and not limitation illustrates an operating system application programs other program modules and program data . Often the operating system offers services to application programs by way of one or more application programming interfaces APIs not shown . Because the operating system incorporates these services developers of application programs need not redevelop code to use the services. Examples of APIs provided by operating systems such as Microsoft s WINDOWS are well known in the art.

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from and writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from and writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from and writes to a removable nonvolatile optical disk such as a CD ROM. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards DVDs digital video tape solid state RAM and solid state ROM. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing an operating system application programs other program modules and program data . Note that these components can either be the same as or different from the operating system application programs other program modules and program data . The operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies.

A user may enter commands and information into the computer through input devices such as a keyboard and pointing device commonly referred to as a mouse trackball or touch pad. Other input devices not shown may include a microphone joystick game pad satellite dish and scanner. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a Universal Serial Bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or via another appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof So may be stored in a remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

In the description that follows the invention will be described with reference to acts and symbolic representations of operations that are performed by one or more computers unless indicated otherwise. As such it will be understood that such acts and operations which are at times referred to as being computer executed include the manipulation by the processing unit of the computer of electrical signals representing data in a structured form. This manipulation transforms the data or maintains them at locations in the memory system of the computer which reconfigures or otherwise alters the operation of the computer in a manner well understood by those skilled in the art. The data structures where data are maintained are physical locations of the memory that have particular properties defined by the format of the data. However while the invention is being described in the foregoing context it is not meant to be limiting as those of skill in the art will appreciate that various of the acts and operations described hereinafter may also be implemented in hardware.

Sections IV through VIII describe how the present invention controls the complexity of asynchronous processes. This section introduces synchronous and asynchronous processing and explains the differences between them using an example that reappears in latter sections. The example portrays a system for placing telephone calls. This illustrative system is greatly simplified from actual telephony systems so that the focus of discussion can remain on the underlying processing models.

The important lesson of is the orderly linear flow of tasks that characterizes synchronous processing. The telephony system works on one task until that task is completed then the system takes up another task and works on it until that task is completed and so on. At no time is the system working on more than one task. This single mindedness eases debugging and testing as it is always clear exactly what the application is trying to do at any time. However this same single mindedness means that it would be very difficult for the synchronous system of to provide all of the functions illustrated in . The first modem would not be able to support a second call until the first call completed. Similarly the modem driver could not support two calls at the same time and so could not bridge calls and .

By way of contrast with is a flowchart showing how tasks may be used in an asynchronous computing environment to implement the computer telephony system of . The asynchronous processing begins and ends in the same manner as does the synchronous process. At the beginning of the process the modem driver is loaded and an available modem is opened . When all work has been completed the modem is closed and the modem driver unloaded . However the processing between opening and closing the modem differs significantly from the synchronous model of . The asynchronous model replaces the synchronous model s orderly linear structure of task following the completion of task with an event response loop . Just as in the synchronous model when the user indicates that he wishes to place a call by removing the telephone handset from its cradle the system places the call . When the user hangs up the system drops the call . Unlike in the synchronous model however the event response loop allows the asynchronous system to take up another task before the first call completes. When the process to place a call is called in the asynchronous model it returns control to the event response loop while the call is in progress. Thereafter the task runs in parallel with the event response loop. This means that the first modem can place one call and still be available to place another call while the first call remains in progress. The modem driver can support multiple in progress calls and can bridge calls together into a conference call.

The foregoing comparison between the synchronous and asynchronous processing models is intentionally stylized to highlight the differences between the two. Realistically many processes are implemented using a combination of synchronous and asynchronous methods the asynchronous methods used when the expected payoff of improved performance exceeds the expected increase in development and maintenance costs. Despite the stylization of the comparison the differences between the models are nonetheless real. Sections IV through VIII describe how the present invention decreases the costs of asynchronous programming while maintaining its benefits.

One way to control the complexity of asynchronous processing is to capture the structure of a process as it develops. Microsoft s WINDOWS Development Model takes a first step at capturing that structure by its use of Input Output Request Packets IRPs . is a block diagram showing how an IRP can capture some of the structure of an asynchronous communications request.

When an application program needs to communicate it relies on services provided by a Dynamic Linked Library DLL . The application program calls a DLL routine to perform the communications request. In step the DLL routine formats the request and passes it to the Input Output Manager IOM .

The IOM coordinates the disparate elements in the hierarchy of drivers shown in . In step the IOM creates an IRP that captures the details of the application program s communications request. The IRP contains stack locations and one for each driver that will use the IRP. For each driver its stack location contains the information that the driver needs to process the IRP. When the IOM creates the IRP it populates the first stack location before passing the IRP to the high level driver .

Each driver in the stack processes the IRP to the extent that it is able populates the IRP stack location for the next driver lower in the stack and then passes the IRP along to that driver. shows a high level driver and a low level driver but there may be more or fewer drivers involved in servicing a particular communications request.

The Hardware Abstraction Layer provides a bridge between logical communications functions and the implementation details of particular hardware platforms. A communications request is typically completed by hardware effecting changes in the physical world.

The IRP captures in broad outline the structure of the processes that have affected it. The IRP s header allows each driver in the stack to record information about its processing of the IRP. Testing and debugging personnel can examine the IRP s header and determine the IRP s history and present state including which protocol driver is currently processing it.

While useful in its particular application the IRP does not provide a mechanism for capturing and controlling the structure of an arbitrary asynchronous process. Its use is restricted to WINDOWS kernel mode drivers. Also the sequence of protocol drivers invoked must be known in advance. Finally the IRP contains no information about the inner workings of each protocol driver.

The present invention provides tools for capturing and manipulating the structure of an asynchronous process as it develops. Sections IV through VIII describe particular implementations of the invention and should not be taken as limiting the invention in any way. For the sake of this discussion aspects of the present invention are loosely collected under the term APE Asynchronous Processing Environment.

According to one aspect of the present invention the structure of an asynchronous process is automatically captured as it develops. A complex asynchronous process may be broken down into a hierarchy of simpler tasks. The state of the asynchronous process at a given time is characterized by a hierarchical structure of software objects where each object represents one task in the process. The captured structure can be used by developers to ensure that their code does what they want it to and by testing personnel to elucidate what the code is doing in actuality.

One implementation of APE is illustrated by means of the same telephone support system example used in Section II. The invention is in no way restricted to telephony applications but may be used with any asynchronous process or asynchronous portion of a larger process . is a code diagram showing a synchronous program that implements the tasks of . Following an explanation of some of the details of APE contrasts with an APE implementation of the asynchronous tasks of . A step by step description of the process DoCallSync of follows.

The example of is limited by its adherence to the synchronous processing model. From Section II some of those limitations are a modem cannot support a second call until the first call completes and the modem driver cannot support two calls at the same time and so cannot bridge calls.

APE objects are user defined data structures. Typically these structures correspond to control blocks and keep the state of user specific resources for as long as those resources exist in an asynchronous processing system. For example an APE object can be defined to correspond to each entity in for the modem driver for each modem through for each call through and for the call bridge . Each APE user may define the meaning of his own APE objects. A socket application may keep the state of each open TCP IP connection in a separate APE object. A protocol driver may define an APE object for every network adapter to which it is bound an APE object for each client and an APE object for each binding from a specific client to a specific network adapter.

APE objects share a common header of type APE OBJECT. The header includes an object reference counter a pointer to the object s parent object and a pointer to a deletion function. The header may be followed by user specific data as in this example of an APE object for a telephone call 

APE uses the fields in the header to manage the life of APE objects. It attempts to minimize the need for the user to explicitly count object references make it difficult for the user to introduce reference counting errors and make it easier to track down reference counting errors when they occur. In order to minimize explicit object reference counting APE requires that users organize their APE objects in the form of a hierarchical tree structure. The header s parent object pointer is set when the APE object is initialized so the user need not explicitly reference the parent when creating children or de reference the parent when children are deleted .

Each user decides how to organize his APE object tree. The organization may follow a natural hierarchy among control blocks. For example an organization emerges for APE objects that correspond to the components of the modem driver object is at the root of the tree and it has modem objects as its children. Call objects are the children of their corresponding modem objects. is a data structure diagram showing one possible tree of APE objects and that correspond to the components of the computer telephony system of . APE ROOT OBJECT is an extension of APE OBJECT and is described below. In other cases there may be no natural hierarchy for the user s objects. The user must nevertheless pick a hierarchy even if the user can do no better than creating a single global root object and initializing all other APE objects as children of the root object.

For performance reasons APE need not provide a pointer from a parent object to its children. This is why the arrows in point toward the parents. Pointers to children may be provided as an option for diagnostic purposes.

An APE object is typically exceptions are described below initialized by calling ApeInitializeObject which has the following prototype 

One of the primary purposes of the APE object tree is to control how long objects live an object is not deleted as long as it has children. The object reference counter in the header is set to one on return from ApeInitializeObject . APE increments the object reference counter each time a child is added to this object in the object tree and is decremented each time a child is deleted. When the counter reaches zero APE calls a user supplied delete function included in APE STATIC INFO to delete the object.

APE provides other mechanisms to increment and decrement the object reference counter. For example the function ApeCrossReferenceObjects increments the object reference counters of two objects at the same time logically cross referencing the objects. The inverse of this function is ApeDeCrossReferenceObjects which de references both objects by decrementing their object reference counters. A debugging version of ApeCrossReferenceObjects called ApeCrossReferenceObjectsDebug takes additional parameters that enable APE to verify that neither object was deleted until after the cross reference was removed by calling ApeDeCrossReferenceObjectsDebug . This helps catch cases of dangling references among objects.

Root objects are APE objects that have no parent. Typically each module that uses APE initializes a single root object but this need not always be the case. For example a kernel mode protocol driver might initialize one root object for each bound network adapter card.

As its header a root object uses the structure APE ROOT OBJECT an extension of APE OBJECT. A root object includes the following used by all objects in the APE object tree under the root object 

A root object is initialized using the function ApeInitializeRootObject which has the following prototype 

A root object is de initialized after all of the children of the root have been deleted. The function ApeDeinitializeRootObject specifies a completion handler that APE calls when the root object s object reference counter goes to zero.

In a single threaded synchronous environment program complexity is tamed by organizing the program into a hierarchy of functions. Each function concerns itself with a small logical piece of the big picture. While working on this piece partial results are maintained in local variables hidden from the rest of the program. Utility functions that solve a particular kind of subproblem may be called from several places.

Unfortunately this technique is not easily used in an asynchronous environment. The stack needs to unwind after every operation that completes asynchronously so context must be preserving in data structures that persist until the operation completes. When the operation completes the context needs to be retrieved from these data structures and processing resumed. Thus even if there is a logical way to split a complex operation into a hierarchy of subtasks those tasks cannot simply be mapped into a corresponding function hierarchy.

APE provides task objects to represent asynchronous operations within a program. Tasks are analogous to functions in a single threaded synchronous programming environment. They are designed with the following goals in mind 

APE tasks are APE objects and are therefore part of the APE object tree. Each task keeps track of a pending user defined asynchronous operation. Tasks are transient in nature living only as long as the asynchronous operations they represent are active. Tasks have extended APE OBJECT headers of type APE TASK. The APE TASK structure keeps the state associated with the asynchronous operation.

The task starts executing when the user calls ApeStartTask . ApeStartTask calls the user supplied task handler call it TaskHandler . The task is now in the ACTIVE state . At this point the call stack is as follows 

The suspended task resumes in a different context. The context depends upon which APE function was used to suspend the task. For ApeSuspendTask the task resumes when the user explicitly calls ApeResumeTask . This may be in the context of the completion handler of an asynchronous function. For ApePendTaskOnOtherTask the task resumes when the specified other task completes. If the task was suspended by calling ApePendTaskOnObjectDeletion the task resumes when the specified object is deleted. Finally for ApeDeinitializeGroup the task resumes when a group of objects has been emptied out and de initialized. Groups are discussed in Section VI. 

APE resumes a task simply by setting the task s state to ACTIVE and then calling the task s user supplied task handler. Continuing with the example above TaskHandler returns after calling ApeSuspendTask and MakeCall leaving the task in the PENDING state . When MakeCall completes the modem driver calls the user defined completion handler for this operation call it MakeCallCompletionHandler . MakeCallCompletionHandler then calls ApeResumeTask to resume the previously suspended task. ApeResumeTask calls TaskHandler . The call stack is as follows 

The task enters the ENDED state when its task handler returns without calling one of the above pending functions. Once a task T reaches the ENDED state APE resumes any tasks pending on T. These tasks specified T as the other task in calls to ApePendTaskOnOtherTask . APE deletes the task when it reaches the ENDED state and when there are no longer any references to it. When APE deletes a task its task object is removed from the APE object tree.

In sum tasks live as long as they are either executing in the context of their task handler ACTIVE state or are pending on some asynchronous event one of the PENDING states through . A task can switch back and forth between ACTIVE and PENDING until its task handler finally returns with the task still in the ACTIVE state that is without first calling one of the APE functions that switch it to a PENDING state . When this happens APE puts the task in the ENDED state. A task in the ENDED state is deleted by APE when there are no references to it.

A task may also pend on another task. The former task is resumed when the latter task completes. This facility may be used to structure a complex asynchronous program into a hierarchy of simpler asynchronous operations each represented by a task. Assume a program needs to perform the following two complex modem operations make two modem calls and bridge them together and close down all active bridged calls. The program may be implemented as two high level tasks which correspond to the complex modem operations. The high level tasks use the services of four low level asynchronous modem tasks ModemMakeCall ModemDropCall ModemBridgeCall and ModemUnbridgeCall .

This is a hypothetical dump of the list of outstanding tasks while some of the operations are active 

A call tree of a function F within a module M is the execution trace when executing F and any other functions within M that are called in the context of F. APE uses a structure the stack record to store information that is relevant only to a particular call tree. A stack record lives only as long as the call tree rooted on the function that created the stack record. APE uses the stack record for the following purposes 

LocID is a location identifier an integer constant that uniquely identifies a particular location in the source code. LocIDs may be randomly generated constants and are used purely for diagnostic purposes. Several APE functions take a LocID as one of their arguments to identify the location in the user s source code where the APE function is called.

A stack record may be initialized on the stack of functions called from outside the module exported functions completion handlers etc. Once initialized a pointer to the stack record may be passed as an argument to subfunctions. Many APE functions take a pointer to the current stack record as their last argument. The following code sample declares a stack record and passes a pointer to it in a call to ApeStartTask . LocID 0x0989009 marks the location where the stack record is declared and LocID 0x25f83439 marks the location where ApeStartTask is called.

To understand the above features of APE turn to which is a code diagram showing DoCallAsync a function that implements the asynchronous tasks of . This function is an asynchronous version of the synchronous DoCallSync discussed above and shown in . DoCallAsync has the same requirements as DoCallSync initialize the modem driver open a modem make a call drop the call close the modem and de initialize the modem driver. However DoCallAsync must deal with the fact that some of the modem control functions return asynchronously.

When MdmOpenModem completes asynchronously the task handler DoCallTask is called once more. Because the pState variable was set to the value OPENMODEM processing continues at case OPENMODEM. In this manner the task handler proceeds in an orderly fashion performing all of the functionality required of DoCallAsync even though many of the operations may complete asynchronously.

There are at least three important points to note. First an asynchronous task handler cannot complete its processing by means of an orderly march through its code. When a suspended task handler resumes processing does not begin at the point where the task handler suspended itself. Rather processing starts again at the start of the task handler. Because of this DoCallTask uses a state variable to keep track of how far it has gone and uses the value of that state variable to switch to the next appropriate code segment. Second just because a process may complete asynchronously does not mean that it will. MdmOpenModem may complete synchronously in which case it returns a status other than MDM STATUS PENDING. Processing continues at case OPENMODEM without the task handler suspending itself and resuming. Third a task handler does not need to block itself until the asynchronous function completes. Instead it returns to its caller and the current thread can continue to do other work while the asynchronous function does its work.

This discussion of DoCallAsync presents some of the more salient points of implementing an asynchronous process using APE. For a full disclosure of all the intricacies involved see Appendix I which provides the complete source code of an implementation of DoCallAsync .

As discussed in Section IV above APE uses reference counters to determine when to delete an object. When an object is initialized it is added to an APE object tree below its parent object and the parent s reference counter is incremented. An APE object is considered alive as long as its reference counter is above zero. The implicit reference added to a parent when an object is initialized and removed when the object is deleted makes sure that an object s parent is alive for at least as long as the object.

To clarify the use of temporary references consider the example of GetBridgedCall and ProcessBridgedCall . A few preliminary definitions are in order. ApeTmpReferenceObject adds a temporary reference to an APE object while ApeTmpDereferenceObject removes a temporary reference. The MODEM object keeps track of the state of a modem.

GetBridgedCall returns a pointer to the bridged call associated with a given modem if there is one. This function adds a temporary reference to the returned call to make sure that some other thread does not delete the call object in the mean time.

External references are used to ensure that an APE object lives at least as long as some entity outside the module has a reference to it. In a manner analogous to the ApeCrossReferenceObjectsDebug function described in Section IV there is a debugging version of the function that adds external references to objects. The debugging version helps catch cases of dangling external references.

Programs often need to maintain a collection of objects and work with the collection as a whole. For example a program that manages modems and calls may maintain a collection of modem control blocks and for each modem a collection of call control blocks representing calls active on that modem. The program may perform operations on the collection as a whole or on the members of the collection individually. For example the program may enumerate the calls active on a modem or may suspend closing a modem until all calls active on the modem have been deleted. During these operations care must be taken for the management of object reference counters because objects may be created or deleted or otherwise modified in the middle of the program s processing of the collection.

To illustrate the problems that arise when trying to uniformly process all members of a collection consider trying to run a function Foo against all calls currently active on a modem. The following code segments use the MODEM and CALL objects defined in Section V above. MODEM maintains a singly linked list of pointers to the calls active on the modem that is each active call object points to the next active call object.

There are other problems with managing collections of objects in a multi threaded environment. The semantics of the collection may be undesirable or unclear. For example a thread may look up an object and assume that it is still in a collection while another thread removes it from the collection. It may be tricky to de initialize a collection because this may involve waiting for the collection to be empty and for there to be no ongoing attempts to iterate over the objects in the collection.

APE provides groups to address the issues of maintaining collections and objects in collections. APE groups provide the following functionality 

APE provides two types of groups primary and secondary. They use the same structure and share much of their functionality. They differ in the relationship between an object s existence and its membership in the group. Primary groups contain objects whose existence is tied to the group. An object is initialized at the same time that it is inserted into a primary group and the object is only removed from the group when it is deleted. An object can be a member of only one primary group. Secondary groups contain objects whose existence is not tied to the group except for the fact that an object must be alive when it is in a group . The user explicitly inserts an object into and removes an object from a secondary group. An object can be a member of more than one secondary group.

APE provides management functions that are applicable to all types of groups. ApeLookUpObjectInGroup returns an object that matches a specified key. ApeEnumerateObjectsInGroup calls the user provided enumeration function for each object in the group adding a temporary reference to the object before calling the enumeration function with that object and de referencing the object after the enumeration function returns. When performing an iteration ApeGetNextObjectInGroup uses a structure of type APE GROUP ITERATOR initialized by ApeInitializeGroupIterator .

Each object in a group may be given a key. The key is opaque to APE and can be of arbitrary size and structure. APE manipulates keys using two user supplied functions specified in the APE GROUP OBJECT INFO structure associated with the group. The first function generates a UINT sized hash of the key to speed look up. The second function compares two keys to detect an exact key match.

The function ApeCreateObjectInGroup is specific to primary groups and creates an object in a primary group.

ApeAddObjectToGroup adds an object to a secondary group and adds an external reference see Section V above to the object. ApeRemoveObjectFromGroup deletes the object from the group and removes the external reference.

 Locking data which data may include executable code refers to serializing access to those data. In many processing systems it is difficult to verify that locks are acquired in the correct order and are released in the correct places. If multiple objects need to be locked together deadlock may be avoided only by locking the objects in a specific order. However the rules of locking order are often unenforceable and are merely implied by standards of coding conduct such as suggesting that objects be locked in an order based on the types of the objects. Typical code may look like this 

APE lock tracking consists of a set of structures and functions to control locks and to make it easier to isolate locking related errors. APE provides the following functionality for tracking locks 

For purposes of illustrating the use of APE lock tracking define a LOCK object as a trackable version of a critical section.

To initiate APE lock tracking the user associates an APE LOCK STATE structure with each tracked lock. APE uses this structure in conjunction with the stack record to track lock usage. ApeInitializeLockState initializes the structure.

The user calls ApeTrackAcquireLock just before acquiring a lock and calls ApeTrackReleaseLock just before releasing the lock. ApeTrackAcquireLock calls the user specified assertion failure handler associated with the stack record if the lock has already been acquired by some other call tree typically on a different thread . If the stack record has lock tracking enabled then the assertion failure handler is called if the lock s level is less than the level of a lock previously acquired in this call tree or if the lock s level equals that of a lock previously acquired in this call tree and the numerical value of the pointer to the lock is less than or equal to that of a previously acquired lock with the same level.

The following code segment acquires and releases a LOCK object calling operating system locking primitives as well as the APE lock tracking functions.

APE supports operating system specific locks. For example it supports Microsoft s WINDOWS Driver Model spin locks and Critical Sections. The APE OS LOCK structure is equivalent to KSPIN LOCK when in kernel mode and to CRITICAL SECTION when in user mode. When in kernel mode APE saves the current IRQL in the stack record obviating the need for the user to save and restore the previous IRQL when acquiring and releasing locks.

One or more resources may be associated with an object during the lifetime of the object. Generally a resource is anything that needs to be explicitly released when it is no longer needed by the object. Examples include sections of memory handles to system supplied objects and application defined objects. Failure to release a resource is a common programming error with memory leaks a well known result. APE supports resource tracking and allows the assertion that all associated resources are released before an object is deleted. Specifically APE supports resource tracking by providing the following functionality 

APE uses debug associations to track resources. A debug association is a tuple of form AssociationID Entity associated with an object where AssociationID is a user supplied label for the debug association and Entity is an optional integer or pointer representing an instance of the debug association. To track the association of resource R of type T to an object APE attaches the debug association T R to the object.

Debug associations may be used for purposes other than tracking resources. APE for example uses a debug association to verify that a cross reference must be removed before either cross referenced object can be deleted. Also if an object should not be deleted until event E occurs once event E has occurred APE adds the debug association waiting for event E to the object when event E occurs. This debug association is cleared away when event E occurs.

In view of the many possible embodiments to which the principles of this invention may be applied it should be recognized that the embodiments described herein with respect to the drawing figures are meant to be illustrative only and should not be taken as limiting the scope of invention. Therefore the invention as described herein contemplates all such embodiments as may come within the scope of the following claims and equivalents thereof.

Keeps state information about an ongoing iteration over objects in a group. This structure should be treated as opaque except by the collection handling functions.

A user supplied handler to process one APE object in a group. This function is called repeatedly for each object in the group as long as the function returns a non zero value. If the function returns zero then enumeration is stopped.

