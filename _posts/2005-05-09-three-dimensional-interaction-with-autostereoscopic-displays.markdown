---

title: Three dimensional interaction with autostereoscopic displays
abstract: An apparatus and method for 3D interaction with an autostereoscopic display are presented. A motion tracking system may include video cameras that track a 3D motion of a user within an interaction volume defined by the fields-of-view of the video cameras, as the user moves a light source or other optical marker or an anatomical region of the user within the interaction volume. The motion tracking system may generate 3D tracking data containing position information about the 3D motion. An imaging system may create a virtual scene by tracing 3D virtual objects in virtual space, using the position information in the 3D tracking data. The imaging system may synthesize a plurality of views of the virtual scene, and interlace the plurality of views to generate an interlaced image to drive the autostereoscopic display and to be displayed thereon.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07787009&OS=07787009&RS=07787009
owner: University of Southern California
number: 07787009
owner_city: Los Angeles
owner_country: US
publication_date: 20050509
---
This application claims the benefit of priority under 35 U.S.C. 119 e from U.S. Provisional Application Ser. No. 60 569 670 entitled Three Dimensional Interaction With Autostereoscopic Displays and filed on May 10 2004 by inventors Zahir Y. Alpaslan and Alexander A. Sawchuk. This application also claims the benefit of priority under 35 U.S.C. 119 e from U.S. Provisional Application Ser. No. 60 614 306 entitled Multiple Camera Image Acquisition Models For Multi View 3D Display Interaction and filed on Sep. 28 2004 by inventors Zahir Y. Alpaslan and Alexander A. Sawchuk. Provisional application Ser. No. 60 569 670 and provisional application Ser. No. 60 614 306 are both incorporated herein by reference in their entireties.

This invention was made with government support under Contract Nos. EEC 9529152 EIA 0116573 and EIA 0321377 awarded by the National Science Foundation. The government has certain rights in the invention.

Stereoscopic systems may have gained popularity due to inter alia increases in the processing power of computers and the reduced price of high quality displays. Stereoscopic systems may present a viewer with an image of one or more three dimensional 3D objects such that the objects realistically appear to have a 3D depth. Most stereoscopic techniques may involve presenting a different view of the 3D object independently to each eye and stereoscopic systems may employ apparatuses that keeps left and right eye images directed solely at the appropriate eye.

Autostereoscopic AS displays may make the stereo experience more attractive by producing a 3D visual sensation without the use of glasses goggles helmets head tracking or other encumbering viewing aids. With the emergence of autostereoscopic displays it may be possible to view images movies and other kinds of data in 3D space without wearing glasses or other viewing equipment. Autostereoscopic displays that have been developed may include by way of example lenticular based autostereoscopic displays and barrier screen based systems.

Recently researchers have been investigating stereoscopic viewing technology for better 3D data visualization and manipulation. Efforts have been made for example to allow users to interact with an autostereoscopic displays and to combine haptics with stereoscopic viewing technology.

An apparatus may include an autostereoscopic display a motion tracking system and an imaging system. The motion tracking system may be configured to track a 3D motion of a user within an interaction volume as the user interacts with the autostereoscopic display and to generate 3D tracking data containing position information about the 3D motion. The imaging system may be configured to process the 3D tracking data to generate therefrom an image for display on the autostereoscopic display.

An apparatus may include an autostereoscopic display and means for tracking a 3D motion of a user within an interaction volume as the user interacts with the autostereoscopic display and for generating 3D tracking data containing position information about the 3D motion. The apparatus may further include means for processing the 3D tracking data to generate therefrom an image for display on the autostereoscopic display.

A method of interacting with an autostereoscopic display may include tracking a 3D motion of a user within an interaction volume as the user interacts with the autostereoscopic display and generating 3D tracking data containing position information about the 3D motion. The method may further include processing the 3D tracking data to generate therefrom an image for display on the autostereoscopic display.

A plurality of apparatuses may be connected through a network. Each apparatus may include an autostereoscopic display a motion tracking system and an imaging system. The motion tracking system may be configured to track a 3D motion of a user within an interaction volume as the user interacts with the autostereoscopic display and to generate 3D tracking data containing position information about the 3D motion. The imaging system may be configured to process the 3D tracking data to generate therefrom an image for display on the autostereoscopic display.

An apparatus and method are described that allow one or more users to interact with an autostereoscopic display. A motion tracking system may track the 3D motion of one or more users within an interaction volume as the users interact with the autostereoscopic display. 3D tracking data may be processed to create or manipulate objects in virtual space. A plurality of images as seen by virtual cameras that observe the objects may be synthesized and the images may be interlaced to generate an interlaced image that drives the autostereoscopic display. In this way the users may manipulate the position and orientation of displayed objects and may outline draw or otherwise interact with the objects.

The autostereoscopic display may be a multi user display that allows a plurality of users to interacts with the display. The autostereoscopic display may also be a multiple view autostereoscopic display that provide views in addition to those provided by traditional two view right left stereo image display systems. Of course the autostereoscopic display may also be a single user display that allows a single user to interact with the display. The autostereoscopic display may also be a single view autostereoscopic display that displays a single view stereo image.

In the illustrated embodiment of the apparatus the motion tracking system may include a first video camera and a second video camera which track and image the 3D motion of the user within an interaction volume and which generate a plurality of video frames of the 3D motion. The motion tracking system may further include a processor that analyzes frame by frame the plurality of video frames generated by the video cameras and . The interaction volume may be defined by the fields of view of the two cameras and . In particular the interaction volume may be the region of space in which the fields of view of both cameras and intersect. The cameras and may be FireWire cameras for example. Alternatively it may also be possible to use USB based cheaper web cams to track the user input because of the increase in the speed of USB 2.0.

The 3D motion of the user may result from the user holding a light source or other optical marker and moving the light source within the interaction volume as illustrated in . The light source may be a tiny point flashlight just by way of example although many other types of light sources and or optical markers may also be used. The user may draw figures within the interaction volume using the light source . Alternatively the user may move his hand or other part of his anatomy within the interaction volume .

The processor may use a tracking algorithm to analyze the video frames generated by the video cameras and . The processor may extract from each frame position information of the moving light source or other optical marker or user hand motion . The processor may analyze the live video frames frame by frame using the tracking algorithm to find the 3D spatial coordinates in each frame of a center of the light source . The 3D tracking data generated by the motion tracking system may contain the position information extracted by the processor .

In one exemplary embodiment brightness thresholding may be used in order to find the location of the light source in a frame. The threshold may be adjusted so that the light source appears as a small point. Since the light source is very bright the camera gain may be reduced yet a very good image of the light source may still be obtained.

In this embodiment the software in the processor may find the location of the brightest pixels in the image for every one of the plurality of frames generated by the video cameras and . Because of the thresholding nearly all pixels may be black except for the pixels that represent an image of the light source therefore the white pixels may be found with great accuracy. Next the software may do simple averaging to find the center of mass of the whitest area and this center of mass may be the location of the light source in one of the two cameras. The software may repeat the same operation for the frame acquired by the other camera. Finding the 3D location of the light source in pixel coordinates may involve taking two such as z and y coordinates from one of the two cameras and the remaining x coordinate from the other one of the two cameras after applying rotation software to the camera axes. In a special case in which the cameras are positioned exactly perpendicular to each other one coordinate such as z acquired from both cameras may be the same.

In the embodiment of the apparatus that is illustrated in the imaging system may be a virtual imaging system although in other embodiments for example the embodiment illustrated in a real imaging system may also be used. The virtual imaging system shown in may include an image acquisition subsystem that uses the 3D tracking data to create in virtual space a virtual scene representing the interaction of the user with the autostereoscopic display and that synthesizes a plurality N of views of the virtual scene. The virtual imaging system may further include a graphics subsystem that interlaces the plurality N of views of the virtual scene to generate a final interlaced image to be displayed on the autostereoscopic display .

The image acquisition subsystem may include a virtual object tracer which receives the position information from the processor i.e. receives the 3D spatial coordinates of the center of the light source and redraws each point in virtual space. For example the virtual object tracer may accept drawing commands in the form of the 3D spatial coordinates x y z of the cursor and may draw one or more 3D virtual objects in virtual space using these coordinates. In this way the virtual object tracer may create one or more virtual scenes that include these 3D virtual objects.

The image acquisition subsystem may further include a multiple view synthesizer which may use a plurality n of virtual cameras to produce a plurality N of synthesized views of the virtual scene. In one embodiment of the imaging system N may be about nine although other embodiments of the imaging system may use different values for N. A different one of the nine views may be produced by shifting one of the virtual cameras for example horizontally.

The graphics subsystem may include an interlacer that interlaces the plurality N of synthesized views of the virtual scene at a sub pixel level to generate a final interlaced image i.e. to generate digital image data representative of the interlaced image. The graphics subsystem may further include a graphics card that converts the digital data representing the interlaced image into appropriate input signals for the autostereoscopic display i.e. into signals that allow the autostereoscopic display to display the interlaced image.

The image acquisition subsystem and the graphics subsystem may include a 3D drawing and image manipulation program based for example on OpenGL and Visual C although other types of software programs may also be used.

In an exemplary embodiment illustrated in X Y Z may represent the coordinates of the virtual 3D scene. X Y may represent the coordinates of the virtual 3D scene as measured by the detector array of the n th virtual camera out of the plurality of virtual cameras . The apparatus may use from about two 2 to about nine 9 virtual cameras in order to create the sensation of a 3D display although different embodiments of the apparatus may use different numbers of virtual cameras. X Y may represent the coordinates of view n at the surface of the autostereoscopic display . X Y Z may represent the coordinates of the displayed points as perceived by a human observer. These coordinates may depend on viewing distance eye separation and other parameters. X Y Z may represent the coordinates within the effective interaction volume . A cursor or user s hand may have to be located within this volume in order for both cameras and to track its position. X Y and X Y may represent the coordinates of the objects in the interaction volume as measured by the detector array of cameras and respectively. These coordinates may be processed to create X Y Z which may represent the 3D location of an object in the interaction volume as seen by both cameras and .

The software in the apparatus may be built in a modular structure in order to facilitate the adding of new parts. For example in one embodiment not shown a 3D audio system may be added to the apparatus . The 3D audio system may be configured to generate and process 3D auditory signals resulting from the interaction of the user with the autostereoscopic display. The 3D audio system may be for example 3D binaural or 5.1 channel surround sound system although other types of 3D audio systems may also be used. Such a 3D audio system may enhance the stereoscopic experience.

The 3D audio system may for example create and process 3D auditory signals that attach to the motion of a virtual object. In other words whenever the object moves and wherever the object goes it may sound as if the object is making a noise because of the 3D auditory signals created by the 3D audio system. The 3D auditory signals may inform users of various computing process events carried out by either the motion tracking system or the imaging system or both . The 3D auditory signals may inform users about one or more functions of one or more software programs within the motion tracking system and or the imaging system . These function may include but are not limited to the rate of completion of the software program and the amount of work done by the software program. The 3D auditory signals may inform users that they have moved out of the interaction volume.

An exemplary embodiment of an apparatus for 3D interaction with an autostereoscopic display that includes a real imaging system is illustrated in . The apparatus includes a real imaging system in addition to components such as an autostereoscopic display a motion tracking system including video cameras and and a processor a virtual imaging system including an image acquisition subsystem a virtual object tracer and a multiple view synthesizer including a number n of virtual cameras and a graphics subsystem including an interlacer and a graphics card all of which have been described in conjunction with .

By adding multiple camera real time video capture the apparatus may allow the user to interact with 3D video of live scenes. The real imaging system may for example include a real object interaction subsystem which processes real interactions between the user and real objects and a real image acquisition system which uses a plurality n of real cameras to capture a plurality N of real views of the real interactions between the user and real objects.

In one embodiment the cameras and may record video at a 640 480 resolution with 30 fps frame rate thus tracking the light source with an accuracy of 640 pixels in two dimensions and an accuracy of 480 pixels in the third dimension. Cameras having different resolutions and different frame rates may be used in other embodiments.

As the cameras and track the movements of the light source in the interaction volume a pointer or cursor in the virtual space may mimic the motion of the light source to produce a 3D shape which may consist of points or lines. The processor in the motion tracking system may convert the tracking information coming from the cameras and into horizontal vertical and depth parallax coordinates for the display. This information may processed and supplied as the necessary plurality of views for the autostereoscopic display. In one embodiment of the apparatus when the software operates in a moving mode it may use the 3D coordinate information coming from the tracking mode to translate the positions of the virtual objects.

In the illustrated embodiment the interaction volume defined by the fields of view of the two cameras and may occupy a physical volume that is separate from the physical volume occupied by the autostereoscopic display. In other words the interaction volume may be substantially non overlapping with the physical volume occupied by the autostereoscopic display. In other embodiments of the apparatus the interaction volume may overlap with and or be placed in register with the physical volume occupied by the autostereoscopic display.

Because the interaction volume is not in register with the physical volume occupied by the autostereoscopic display the user s hand movements may not need to be tracked right in front of the autostereoscopic display where the images appear. Instead the cameras and may be placed at any desired position. Further when the interaction volume is non overlapping and out of register the interaction volume may have an increased size. By removing the restriction that the user can interact only with objects appearing in front of the display plane of the autostereoscopic display the user may be allowed to interact also with objects appearing behind the display plane as just one example. In this specific case the interaction volume may at least double in size.

In the illustrated implementation of an interaction that is not in register the user may interact with virtual objects with the help of a cursor that represents the light source in virtual space. Using a cursor may avoid the need to accommodate vergence conflict of a human observer. To perceive a 3D sensation the user s eyes may have to focus on the display plane of the autostereoscopic display. However if the user wants to perform in register interaction and to interact without the help of a cursor he may have to focus on his hand and the display plane at the same time. Since the hand may not touch the display plane the user may have to focus on two different planes causing eyestrain. Implementing an interaction that is not in register by using a cursor in virtual space may avoid such a vergence conflict.

Interlacing for vertical lenticular sheets as shown in may create an image generally referred to as a parallax panoramagram. The width Sof a parallax panoramagram may be the sum of the width Sof all the images that contribute to it as indicated in the following formula S S Eq. 1 Assuming that all the input images have the same size then the width Smay simply be given by S NS Eq. 2 where N represents the total number of images that are interlaced.

As seen from each group of N slices may be made to fit exactly under a lenticula in a interlacing process. The total number of pixels of the parallax panoramagram may be N times the number of pixels of one image i.e. N m n.

For LCD liquid crystal display panels lenticular sheet interlacing may be done in a different way. Vertical lenticular LCDs may suffer from a Moire like vertical dark band effect. Slanting the lenticular sheet with respect to the vertical at a small angle may remove the dark bands in the displayed image and may make the transition between views appear to be more continuous. However both the horizontal and vertical effective resolution of each view may be reduced in an embodiment of the apparatus in which N 9 because the nine images are multiplexed onto a fixed resolution of the LCD display.

The pitch of a slanted lenticular sheet may be less than that used for vertically oriented sheets. Because the sub pixels of an LCD panel may typically be rectangular it may not be easy to have an exact register of sub pixels and slanted lenticulae. In one embodiment of the apparatus an interlacing process provided by StereoGraphics and DDD may be used which use an approach similar to an approach developed by van Berkel. In other embodiments different algorithms may be used for the interlacing process.

In an exemplary embodiment of the apparatus the apparatus may track the motion of a flashlight cursor running at 15 fps and draws an image with 640 480 resolution in real time. In this embodiment the functions of the apparatus may include move draw and save. The move function may translate an object s location in stereo space coordinate X Y Z .

In an exemplary embodiment the figures drawn by the virtual object tracer may be saved as text files. Instead of saving the whole image just the 3D coordinates the program uses for creating and transforming a virtual object may be saved. The file size for an image such as the ones shown in may be just a few KBs.

In another exemplary embodiment a rotation tool may be implemented that can rotate a virtual object by clicking buttons on the desktop using a mouse. The rotation and other manipulation functions may be moved to stereo space.

In sum an apparatus and method have been described that allow for interactive input and manipulation of 3D data using a motion tracking system in combination with an autostereoscopic display. Using a two camera configuration the computation and processing time needed for tracking light sources used as a 3D cursor may be considerably reduced. Also there may be no need to track the head to stabilize the image. In one of the embodiments of the apparatus described above the autostereoscopic display may already have a built in look around capability as the autostereoscopic display has nine images inside a viewing zone. The interaction algorithm describe above may allow a user to interact with images both appearing inside the display and in front of the display.

A multi user autostereoscopic display may be used thus allowing more than one user to see the interaction at the same time and allowing more than one person to interact with the display by tracking two or more light sources. By connecting two of the above described apparatuses via Internet it may be possible to have a 3D interaction. For example one user s drawing may appear and float in space in front of another user at a remote location. Multiple users may be able to interact with our AS desktop system at the same time.

The previous description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of what is disclosed above. Thus the apparatus and method described above are not intended to be limited to the embodiments shown herein but is to be accorded the full scope consistent with the claims wherein reference to an element in the singular is not intended to mean one and only one unless specifically so stated but rather one or more. 

All structural and functional equivalents to the elements of the various embodiments described throughout this disclosure that are known or later come to be known to those of ordinary skill in the art are expressly incorporated herein by reference and are intended to be encompassed by the claims. Moreover nothing disclosed herein is intended to be dedicated to the public regardless of whether such disclosure is explicitly recited in the claims. No claim element is to be construed under the provisions of 35 U.S.C. 112 sixth paragraph unless the element is expressly recited using the phrase means for or in the case of a method claim the element is recited using the phrase step for. 

