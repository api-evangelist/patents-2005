---

title: Multimodal fusion decision logic system
abstract: The present invention includes a method of deciding whether a data set is acceptable for making a decision. A first probability partition array and a second probability partition array may be provided. A no-match zone may be established and used to calculate a false-acceptance-rate (“FAR”) and/or a false-rejection-rate (“FRR”) for the data set. The FAR and/or the FAR may be compared to desired rates. Based on the comparison, the data set may be either accepted or rejected. The invention may also be embodied as a computer readable memory device for executing the methods.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07287013&OS=07287013&RS=07287013
owner: Ultra-Scan Corporation
number: 07287013
owner_city: Amherst
owner_country: US
publication_date: 20051115
---
This application claims the benefit of priority to U.S. provisional patent application Ser. No. 60 643 853 filed on Jan. 14 2005.

The United States government provided finding related to this invention via US Army under contract W911NF 04 C 0067 and the United States government may have certain rights in the invention.

The present invention relates to the use of multiple biometric modalities and multiple biometric matching methods in a biometric identification system. Biometric modalities may include but are not limited to such methods as fingerprint identification iris recognition voice recognition facial recognition hand geometry signature recognition signature gait recognition vascular patterns lip shape ear shape and palm print recognition.

Algorithms may be used to combine information from two or more biometric modalities. The combined information may allow for more reliable and more accurate identification of an individual than is possible with systems based on a single biometric modality. The combination of information from more than one biometric modality is sometimes referred to herein as biometric fusion .

Reliable personal authentication is becoming increasingly important. The ability to accurately and quickly identify an individual is important to immigration law enforcement computer use and financial transactions. Traditional security measures rely on knowledge based approaches such as passwords and personal identification numbers PINs or on token based approaches such as swipe cards and photo identification to establish the identity of an individual. Despite being widely used these are not very secure forms of identification. For example it is estimated that hundreds of millions of dollars are lost annually in credit card fraud in the United States due to consumer misidentification.

Biometrics offers a reliable alternative for identifying an individual. Biometrics is the method of identifying an individual based on his or her physiological and behavioral characteristics. Common biometric modalities include fingerprint face recognition hand geometry voice iris and signature verification. The Federal government will be a leading consumer of biometric applications deployed primarily for immigration airport border and homeland security. Wide scale deployments of biometric applications such as the US VISIT program are already being done in the United States and else where in the world.

Despite advances in biometric identification systems several obstacles have hindered their deployment. Every biometric modality has some users who have illegible biometrics. For example a recent NIST National Institute of Standards and Technology study indicates that nearly 2 to 5 of the population does not have legible fingerprints. Such users would be rejected by a biometric fingerprint identification system during enrollment and verification. Handling such exceptions is time consuming and costly especially in high volume scenarios such as an airport. Using multiple biometrics to authenticate an individual may alleviate this problem.

Furthermore unlike password or PIN based systems biometric systems inherently yield probabilistic results and are therefore not fully accurate. In effect a certain percentage of the genuine users will be rejected false non match and a certain percentage of impostors will be accepted false match by existing biometric systems. High security applications require very low probability of false matches. For example while authenticating immigrants and international passengers at airports even a few false acceptances can pose a severe breach of national security. On the other hand false non matches lead to user inconvenience and congestion.

Existing systems achieve low false acceptance probabilities also known as False Acceptance Rate or FAR only at the expense of higher false non matching probabilities also known as False Rejection Rate or FRR . It has been shown that multiple modalities can reduce FAR and FRR simultaneously. Furthermore threats to biometric systems such as replay attacks spoofing and other subversive methods are difficult to achieve simultaneously for multiple biometrics thereby making multimodal biometric systems more secure than single modal biometric systems.

Systematic research in the area of combining biometric modalities is nascent and sparse. Over the years there have been many attempts at combining modalities and many methods have been investigated including Logical And Logical Or Product Rule Sum Rule Max Rule Min Rule Median Rule Majority Vote Bayes Decision and Neyman Pearson Test . None of these methods has proved to provide low FAR and FRR that is needed for modern security applications.

The need to address the challenges posed by applications using large biometric databases is urgent. The US VISIT program uses biometric systems to enforce border and homeland security. Governments around the world are adopting biometric authentication to implement National identification and voter registration systems. The Federal Bureau of Investigation maintains national criminal and civilian biometric databases for law enforcement.

Although large scale databases are increasingly being used the research community s focus is on the accuracy of small databases while neglecting the scalability and speed issues important to large database applications. Each of the example applications mentioned above require databases with a potential size in the tens of millions of biometric records. In such applications response time search and retrieval efficiency also become important in addition to accuracy.

The present invention includes a method of deciding whether a data set is acceptable for making a decision. For example the present invention may be used to determine whether a set of biometrics is acceptable for making a decision about whether a person should be allowed access to a facility. The data set may be comprised of information pieces about objects such as people. Each object may have at least two types of information pieces that is to say the data set may have at least two modalities. For example each object represented in the database may by represented by two or more biometric samples for example a fingerprint sample and an iris scan sample. A first probability partition array Pm i j may be provided. The Pm i j may be comprised of probability values for information pieces in the data set each probability value in the Pm i j corresponding to the probability of an authentic match. Pm i j may be similar to a Neyman Pearson Lemma probability partition array. A second probability partition array Pfm i j may be provided the Pfm i j being comprised of probability values for information pieces in the data set each probability value in the Pfm i j corresponding to the probability of a false match. Pfm i j may be similar to a Neyman Pearson Lemma probability partition array.

A method according to the invention may identify a no match zone. For example the no match zone may be identified by identifying a first index set A the indices in set A being the i j indices that have values in both Pfm i j and Pm i j . A second index set Z may be identified the indices of Z being the i j indices in set A where both Pfm i j is larger than zero and Pm i j is equal to zero. FAR may be determined where FAR 1 P i j . FAR may be compared to a desired false acceptance rate FAR and if FAR is greater than the desired false acceptance rate than the data set may be rejected for failing to provide an acceptable false acceptance rate. If FAR is less than or equal to the desired false acceptance rate then the data set may be accepted if false rejection rate is not important.

If false rejection rate is important further steps may be executed to determine whether the data set should be rejected. The method may further include identifying a third index set ZM the indices of ZM being the i j indices in Z plus those indices where both Pfm i j and Pm i j are equal to zero. A fourth index set C may be identified the indices of C being the i j indices that are in A but not ZM . The indices of C may be arranged such that

In another method according to the invention the false rejection rate calculations and comparisons may be executed before the false acceptance rate calculations and comparisons. In such a method a first index set A may be identified the indices in A being the i j indices that have values in both Pfm i j and Pm i j . A second index set Z may be identified the indices of Z being the i j indices of A where Pm i j is equal to zero. A third index set C may be identified the indices of C being the i j indices that are in A but not Z The indices of C may be arranged such that

The invention may also be embodied as a computer readable memory device for executing any of the methods described above.

Preliminary Considerations To facilitate discussion of the invention it may be beneficial to establish some terminology and a mathematical framework. Biometric fusion may be viewed as an endeavor in statistical decision theory 1 2 namely the testing of simple hypotheses. This disclosure uses the term simple hypothesis as used in standard statistical theory the parameter of the hypothesis is stated exactly. The hypotheses that a biometric score is authentic or is from an impostor are both simple hypotheses.

Given a test observation the following two simple statistical hypotheses are observed The null hypothesis H states that a test observation is an impostor the alternate hypothesis H states that a test observation is an authentic match. Because there are only two choices the fusion logic will partition S into two disjoint sets R S and R S where R R and R R S. Denoting the compliment of a set A by A so that R Rand R R.

The decision logic is to accept H declare a match if a test observation x belongs to Ror to accept H declare no match and hence reject H if x belongs to R. Each hypothesis is associated with an error type A Type I error occurs when His rejected accept H when His true this is a false accept FA and a Type II error occurs when His rejected accept H when His true this is a false reject FR . Thus denoted respectively the probability of making a Type I or a Type II error follows 

The class conditional pdf for each individual biometric the marginal pdf is assumed to have finite support that is the match scores produced by the ibiometric belong to a closed interval of the real line where 

Fusion and Decision Methods Because different applications have different requirements for error rates there is an interest in having a fusion scheme that has the flexibility to allow for the specification of a Type I error yet have a theoretical basis for providing the most powerful test as defined in Definition 2. Furthermore it would be beneficial if the fusion scheme could handle the general problem so there are no restrictions on the underlying statistics. That is to say that 

A number of combination strategies were examined all of which are listed by Jain 4 on page 243. Most of the schemes such as Demptster Shafer and fuzzy integrals involve training. These were rejected mostly because of the difficulty in analyzing and controlling their performance mechanisms in order to obtain optimal performance. Additionally there was an interest in not basing the fusion logic on empirical results. Strategies such as SUM MEAN MEDIAN PRODUCT MIN MAX were likewise rejected because they assume independence between biometric features.

When combining two biometrics using a Boolean AND or OR it is easily shown to be suboptimal when the decision to accept or reject His based on fixed score thresholds. However the AND and the OR are the basic building blocks of verification OR and identification AND . Since we are focused on fixed score thresholds making an optimal decision to accept or reject Hdepends on the structure of R S and R S and simple thresholds almost always form suboptimal partitions.

If one accepts that accuracy dominates the decision making process and cost dominates the combination strategy certain conclusions may be drawn. Consider a two biometric verification system for which a fixed FAR has been specified. In that system a person s identity is authenticated if His accepted by the first biometric OR if accepted by the second biometric. If His rejected by the first biometric AND the second biometric then manual intervention is required. If a cost is associated with each stage of the verification process a cost function can be formulated. It is a reasonable assumption to assume that the fewer people that filter down from the first biometric sensor to the second to the manual check the cheaper the system.

Suppose a fixed threshold is used as the decision making process to accept or reject Hat each sensor and solve for thresholds that minimize the cost function. It will obtain settings that minimize cost but it will not necessarily be the cheapest cost. Given a more accurate way to make decisions the cost will drop. And if the decision method guarantees the most powerful test at each decision point it will have the optimal cost.

It is clear to us that the decision making process is crucial. A survey of methods based on statistical decision theory reveals many powerful tests such as the Maximum Likelihood Test and Bayes Test. Each requires the class conditional probability density function as given by Equation 1. Some such as the Bayes Test also require the a priori probabilities P H and P H which are the frequencies at which we would expect an impostor and authentic match attempt. Generally these tests do not allow the flexibility of specifying a FAR they minimize making a classification error.

In their seminal paper of 1933 3 Neyman and Pearson presented a lemma that guarantees the most powerful test for a fixed FAR requiring only the joint class conditional pdf for Hand H. This test may be used as the centerpiece of the biometric fusion logic employed in the invention. The Neyman Pearson Lemma guarantees the validity of the test. The proof of the Lemma is slightly different than those found in other sources but the reason for presenting it is because it is immediately amenable to proving the Corollary to the Neyman Pearson Lemma. The corollary states that fusing two biometric scores with Neyman Pearson always provides a more powerful test than either of the component biometrics by themselves. The corollary is extended to state that fusing N biometric scores is better than fusing N 1 scores

The Neyman Pearson Lemma Proofs of the Neyman Pearson Lemma can be found in their paper 4 or in many texts 1 2 . The proof presented here is somewhat different. An in common region Ris established between two partitions that have the same FAR. It is possible this region is empty. Having Rmakes it easier to prove the corollary presented.

Neyman Pearson Lemma Given the joint class conditional probability density functions for a system of order N in making a decision with a specified FAR let be a positive real number and let

Proof The lemma is trivially true if Ris the only region for which Equation 7 holds. Suppose R Rwith m R R 0 this excludes sets that are the same as Rexcept on a set of measure zero that contribute nothing to the integration is any other region such that

Corollary Neyman Pearson Test Accuracy Improves with Additional Biometrics The fact that accuracy improves with additional biometrics is an extremely important result of the Neyman Pearson Lemma. Under the assumed class conditional densities for each biometric the Neyman Pearson Test provides the most powerful test over any other test that considers less than all the biometrics available. Even if a biometric has relatively poor performance and is highly correlated to one or more of the other biometrics the fused CAR is optimal. The corollary for N biometrics versus a single component biometric follows.

Corollarn to the Neyman Pearson Lemma Given the joint class conditional probability density functions for an N biometric system choose FAR and use the Neyamn Pearson Test to find the critical region Rthat gives the most powerful test for the N biometric system

Examples have been built such that CAR CARbut it is hard to do and it is unlikely that such densities would be seen in a real world application. Thus it is safe to assume that CAR CARis almost always true.

The corollary can be extended to the general case. The fusion of N biometrics using Neyman Pearson theory always results in a test that is as powerful as or more powerful than a test that uses any combination of M

We now state and prove the following five mathematical propositions. The first four propositions are necessary to proving the fifth proposition which will be cited in the section that details the invention. For each of the propositions we will use the following Let r r r . . . r be a sequence of real valued ratios such that r r . . . r. For each rwe know the numerator and denominator of the ratio so that

Proposition 5 Recall that the r is an ordered sequence of decreasing ratios with known numerators and denominators. We sum the first N numerators to get Sand sum the first N denominators to get S. We will show that for the value S there is no other collection of ratios in r that gives the same Sand a smaller S. For

Cost Functions for Optimal Verification and Identification In the discussion above it is assumed that all N biometric scores are simultaneously available for fusion. The Neyman Pearson Lemma guarantees that this provides the most powerful test for a fixed FAR. In practice however this could be an expensive way of doing business. If it is assumed that all aspects of using a biometric system time risk etc. can be equated to a cost then a cost function can be constructed. The inventive process described below constructs the cost function and shows how it can be minimized using the Neyman Pearson test. Hence the Neyman Pearson theory is not limited to all at once fusion it can be used for serial parallel and hierarchal systems.

Having laid a basis for the invention a description of two cost functions is now provided as a mechanism for illustrating an embodiment of the invention. A first cost function for a verification system and a second cost function for an identification system are described. For each system an algorithm is presented that uses the Neyman Pearson test to minimize the cost function for a second order biometric system that is a biometric system that has two modalities. The cost functions are presented for the general case of N biometrics. Because minimizing the cost function is recursive the computational load grows exponentially with added dimensions. Hence an efficient algorithm is needed to handle the general case.

First Cost Function Verification System. A cost function for a 2 stage biometric verification one to one system will be described and then an algorithm for minimizing the cost function will be provided. In a 2 stage verification system a subject may attempt to be verified by a first biometric device. If the subject s identity cannot be authenticated the subject may attempt to be authenticated by a second biometric. If that fails the subject may resort to manual authentication. For example manual authentication may be carried out by interviewing the subject and determining their identity from other means.

The cost for attempting to be authenticated using a first biometric a second biometric and a manual check are c c and c respectively. The specified FARis a system false acceptance rate i.e. the rate of falsely authenticating an impostor includes the case of it happening at the first biometric or the second biometric. This implies that the first biometric station test cannot have a false acceptance rate FAR that exceeds FAR. Given a test with a specified FAR there is an associated false rejection rate FRR which is the fraction of subjects that on average are required to move on to the second biometric station. The FAR required at the second station is FAR FAR FAR. It is known that P A B P A P B P A P B so the computation of FARappears imperfect. However if FAR P A B and FAR P A in the construction of the decision space it is intended that FAR P B P A P B .

Note that FARis a function of the specified FARand the freely chosen FAR FARis not a free parameter. Given a biometric test with the computed FAR there is an associated false rejection rate FRRby the second biometric test which is the fraction of subjects that are required to move on to a manual check. This is all captured by the following cost function Cost FRR FRR 30 

There is a cost for every choice of FAR FAR so the Cost in Equation 30 is a function of FAR. For a given test method there exists a value of FARthat yields the smallest cost and we present an algorithm to find that value. In a novel approach to the minimization of Equation 30 a modified version of the Neyman Pearson decision test has been developed so that the smallest cost is optimally small.

An algorithm is outlined below. The algorithm seeks to optimally minimize Equation 30. To do so we a set the initial cost estimate to infinity and b for a specified FAR loop over all possible values of FAR FAR. In practice the algorithm may use a uniformly spaced finite sample of the infinite possible values. The algorithm may proceed as follows c set FAR FAR d set Cost and e loop over possible FARvalues. For the first biometric at the current FARvalue the algorithm may proceed to f find the optimal Match Zzone R and g compute the correct acceptance rate over Rby 

Next the algorithm may test against the second biometric. Note that the region Rof the score space is no longer available since the first biometric test used it up. The Neyman Pearson test may be applied to the reduced decision space which is the compliment of R. So at this time h the algorithm may compute FAR FAR FAR and FARmay be i used in the Neyman Pearson test to determine the most powerful test CAR for the second biometric fused with the first biometric over the reduced decision space R. The critical region for CARis R which is disjoint from Rby our construction. Score pairs that result in the failure to be authenticated at either biometric station must fall within the region R R R from which it is shown that

The final steps in the algorithm are 0 to compute the cost using Equation 30 at the current setting of FARusing FRRand FRR and k to reset the minimum cost if cheaper and keep track of the FARresponsible for the minimum cost.

To illustrate the algorithm an example is provided. Problems arising from practical applications are not to be confused with the validity of the Neyman Pearson theory. Jain states in 5 In case of a larger number of classifiers and relatively small training data a classifier my actually degrade the performance when combined with other classifiers . . . This would seem to contradict the corollary and its extension. However the addition of classifiers does not degrade performance because the underlying statistics are always true and the corollary assumes the underlying statistics. Instead degradation is a result of inexact estimates of sampled densities. In practice a user may be forced to construct the decision test from the estimates and it is errors in the estimates that cause a mismatch between predicted performance and actual performance.

Given the true underlying class conditional pdf for Hand H the corollary is true. This is demonstrated with a challenging example using up to three biometric sensors. The marginal densities are assumed to be Gaussian distributed. This allows a closed form expression for the densities that easily incorporates correlation. The general norder form is well known and is given by

To stress the system the authentic distributions for the three biometric systems may be forced to be highly correlated and the impostor distributions to be lightly correlated. The correlation coefficients are shown in Table 2. The subscripts denote the connection. A plot of the joint pdf for the fusion of system with system is shown in where the correlation between the authentic distributions is quite evident.

The single biometric ROC curves are shown in . As could be predicted from the pdf curves plotted in System performs much better than the other two systems with System having the worst performance.

Fusing 2 systems at a time there are three possibilities and . The resulting ROC curves are shown in . As predicted by the corollary each 2 system pair outperforms their individual components. Although the fusion of system with system has worse performance than system alone it is still better than the single system performance of either system or system .

Finally depicts the results when fusing all three systems and comparing its performance with the performance of the 2 system pairs. The addition of the third biometric system gives substantial improvement over the best performing pair of biometrics.

Tests were conducted on individual and fused biometric systems in order to determine whether the theory presented above accurately predicts what will happen in a real world situation. The performance of three biometric systems were considered. The numbers of score samples available are listed in Table 3. The scores for each modality were collected independently from essentially disjoint subsets of the general population.

To simulate the situation of an individual obtaining a score from each biometric the initial thought was to build a virtual match from the data of Table 3. Assuming independence between the biometrics a 3 tuple set of data was constructed. The 3 tuple set was an ordered set of three score values by arbitrarily assigning a fingerprint score and a facial recognition score to each signature score for a total of 990 authentic score 3 tuples and 325 710 impostor score 3 tuples.

By assuming independence it is well known that the joint pdf is the product of the marginal density functions hence the joint class conditional pdf for the three biometric systems x can be written as X 33 

So it is not necessary to dilute the available data. It is sufficient to approximate the appropriate marginal density functions for each modality using all the data available and compute the joint pdf using Equation 33.

The class condition density functions for each of the three modalities and were estimated using the available sampled data. The authentic score densities were approximated using two methods 1 the histogram interpolation technique and 2 the Parzen window method. The impostor densities were approximated using the histogram interpolation method. Although each of these estimation methods are guaranteed to converge in probability to the true underlying density as the number of samples goes to infinity they are still approximations and can introduce error into the decision making process as will be seen in the next section. The densities for fingerprint signature and facial recognition are shown in and respectively. Note that the authentic pdf for facial recognition is bimodal. The Neyman Pearson test was used to determine the optimal ROC for each of the modalities. The ROC curves for fingerprint signature and facial recognition are plotted in .

There are 3 possible unique pairings of the three biometric systems 1 fingerprints with signature 2 fingerprints with facial recognition and 3 signatures with facial recognition. Using the marginal densities above to create the required 2 D joint class conditional density functions two sets of 2 D joint density functions were computed one in which the authentic marginal densities were approximated using the histogram method and one in which the densities were approximated using the Parzen window method. Using the Neyman Pearson test an optimal ROC was computed for each fused pairing and each approximation method. The ROC curves for the histogram method are shown in and the ROC curves for the Parzen window method are shown in .

As predicted by the corollary the fused performance is better than the individual performance for each pair under each approximation method. But as we cautioned in the example error due to small sample sizes can cause pdf distortion. This is apparent when fusing fingerprints with signature data see and . Notice that the Parzen window ROC curve crosses over the curve for fingerprint facial recognition fusion but does not cross over when using the histogram interpolation method . Small differences between the two sets of marginal densities are magnified when using their product to compute the 2 dimensional joint densities which is reflected in the ROC.

As a final step all three modalities were fused. The resulting ROC using histogram interpolating is shown in and the ROC using the Parzen window is shown . As might be expected the Parzen window pdf distortion with the 2 dimensional fingerprint signature case has carried through to the 3 dimensional case. The overall performance however is dramatically better than any of the 2 dimensional configurations as predicted by the corollary.

In the material presented above it was assumed that all N biometric scores would be available for fusion. Indeed the Neyman Pearson Lemma guarantees that this provides the most powerful test for a fixed FAR. In practice however this could be an expensive way of doing business. If it is assumed that all aspects of using a biometric system time risk etc. can be equated to a cost then a cost function can be constructed. Below we construct the cost function and show how it can be minimized using the Neyman Pearson test. Hence the Neyman Pearson theory is not limited to all at once fusion it can be used for serial parallel and hierarchal systems.

In the following section a review of the development of the cost function for a verification system is provided and then a cost function for an identification system is developed. For each system an algorithm is presented that uses the Neyman Pearson test to minimize the cost function for a second order modality biometric system. The cost function is presented for the general case of N biometrics. Because minimizing the cost function is recursive the computational load grows exponentially with added dimensions. Hence an efficient algorithm is needed to handle the general case.

From the prior discussion of the cost function for a 2 station system the costs for using the first biometric the second biometric and the manual check are c c and c respectively and FARis specified. The first station test cannot have a false acceptance rate FAR that exceeds FAR. Given a test with a specified FAR there is an associated false rejection rate FRR which is the fraction of subjects that on average are required to move on to the second station. The FAR required at the second station is FAR FAR FAR. It is known that P A B P A P B P A P B so the computation of FARappears imperfect. However if FAR P A B and FAR P A in the construction of the decision space it is intended that FAR P B P A P B .

Given a test with the computed FAR there is an associated false rejection rate FRRby the second station which is the fraction of subjects that are required to move on to a manual checkout station. This is all captured by the following cost function Cost FRR FRR 30 

There is a cost for every choice of FAR FAR so the Cost in Equation 30 is a function of FAR. For a given test method there exists a value of FARthat yields the smallest cost and we develop an algorithm to find that value.

Using a modified Neyman Pearson test to optimally minimize Equation 30 an algorithm can be derived. Step 1 set the initial cost estimate to infinity and for a specified FAR loop over all possible values of FAR FAR. To be practical in the algorithm a finite sample of the infinite possible values may be used. The first step in the loop is to use the Neyman Pearson test at FARto determine the most powerful test CAR for the first biometric and FRR 1 CARis computed. Since it is one dimensional the critical region is a collection of disjoint intervals I. As in the proof to the corollary the 1 dimensional Iis recast as a 2 dimensional region R I R so that

When it is necessary to test against the second biometric the region Rof the decision space is no longer available since the first test used it up. The Neyman Pearson test can be applied to the reduced decision space which is the compliment of R. Step 2 FAR FAR FARis computed. Step 3 FARis used in the Neyman Pearson test to determine the most powerful test CAR for the second biometric fused with the first biometric over the reduced decision space R. The critical region for CARis R which is disjoint from Rby the construction. Score pairs that result in the failure to be authenticated at either biometric station must fall within the region R R R from which it is shown that

Step 4 compute the cost at the current setting of FARusing FRRand FRR. Step 5 reset the minimum cost if cheaper and keep track of the FARresponsible for the minimum cost. A typical cost function is shown in .

Two special cases should be noted. In the first case if c 0 c 0 and c 0 the algorithm shows that the minimum cost is to use only the first biometric that is FAR FAR. This makes sense because there is no cost penalty for authentication failures to bypass the second station and go directly to the manual check.

In the second case c c 0 and c 0 the algorithm shows that scores should be collected from both stations and fused all at once that is FAR 1.0. Again this makes sense because there is no cost penalty for collecting scores at both stations and because the Neyman Pearson test gives the most powerful CAR smallest FRR when it can fuse both scores at once.

To extend the cost function to higher dimensions the logic discussed above is simply repeated to arrive at

Identification System Identification one to many systems are discussed. With this construction each station attempts to discard impostors. Candidates that cannot be discarded are passed on to the next station. Candidates that cannot be discarded by the biometric systems arrive at the manual checkout. The goal of course is to prune the number of impostor templates thus limiting the number that move on to subsequent steps. This is a logical AND process for an authentic match to be accepted a candidate must pass the test at station and station and station and so forth. In contrast to a verification system the system administrator must specify a system false rejection rate FRRinstead of a FAR. But just like the verification system problem the sum of the component FRR values at each decision point cannot exceed FRR. If FRR with FAR are replaced in Equations 34 or 35 the following cost equations are arrived at for 2 biometric and an N biometric identification system Cost FAR FAR 36 

Algorithm Generating Matching and Non Matching PDF Surfaces. The optimal fusion algorithm uses the probability of an authentic match and the probability of a false match for each p P. These probabilities may be arrived at by numerical integration of the sampled surface of the joint pdf. A sufficient number of samples may be generated to get a smooth surface by simulation. Given a sequence of matching score pairs and non matching score pairs it is possible to construct a numerical model of the marginal cumulative distribution functions cdf . The distribution functions may be used to generate pseudo random score pairs. If the marginal densities are independent then it is straightforward to generate sample score pairs independently from each cdf. If the densities are correlated we generate the covariance matrix and then use Cholesky factorization to obtain a matrix that transforms independent random deviates into samples that are appropriately correlated.

Assume a given partition P. The joint pdf for both the authentic and impostor cases may be built by mapping simulated score pairs to the appropriate P P and incrementing a counter for that sub square. That is it is possible to build a 2 dimensional histogram which is stored in a 2 dimensional array of appropriate dimensions for the partition. If we divide each array element by the total number of samples we have an approximation to the probability of a score pair falling within the associated sub square. We call this type of an array the probability partition array PPA . Let Pbe the PPA for the joint false match distribution and let Pbe the PPA for the authentic match distribution. Then the probability of an impostor s score pair s s p resulting in a match is P i j . Likewise the probability of a score pair resulting in a match when it should be a match is P i j . The PPA for a false reject does not match when it should is P 1 P.

It will be recognized that other variations of these steps may be made and still be within the scope of the invention. For clarity the notation i j is used to identify arrays that have at least two modalities. Therefore the notation i j includes more than two modalities for example i j k i j k l i j k l m etc.

For example illustrates one method according to the invention in which Pm i j is provided 10 and Pfm i j 13 is provided. As part of identifying 16 indices i j corresponding to a no match zone a first index set A may be identified. The indices in set A may be the i j indices that have values in both Pfm i j and Pm i j . A second index set Z may be identified the indices of Z being the i j indices in set A where both Pfm i j is larger than zero and Pm i j is equal to zero. Then determine FAR19 where FAR 1 P i j . It should be noted that the indices of Z may be the indices in set A where Pm i j is equal to zero since the indices where both Pfm i j Pm i j 0 will not affect FAR and since there will be no negative values in the probability partition arrays. However since defining the indices of Z to be the i j indices in set A where both Pfm i j is larger than zero and Pm i j is equal to zero yields the smallest number of indices for Z we will use that definition for illustration purposes since it is the least that must be done for the mathematics of FAR to work correctly. It will be understood that larger no match zones may be defined but they will include Z so we illustrate the method using the smaller Z definition believing that definitions of no match zones that include Z will fall within the scope of the method described.

FAR may be compared 22 to a desired false acceptance rate FAR and if FAR is less than or equal to the desired false acceptance rate then the data set may be accepted if false rejection rate is not important. If FAR is greater than the desired false acceptance rate then the data set may be rejected 25.

If false rejection rate is important to determining whether a data set is acceptable then indices in a match zone may be selected ordered and some of the indices may be selected for further calculations 28. Toward that end the following steps may be carried out. The method may further include identifying a third index set ZM which may be thought of as a modified Z that is to say a modified no match zone. Here we modify Z so that ZM includes indices that would not affect the calculation for FAR but which might affect calculations related to the false rejection rate. The indices of ZM may be the i j indices in Z plus those indices where both Pfm i j and Pm i j are equal to zero. The indices where Pfm i j Pm i j 0 are added to the no match zone because in the calculation of a fourth index set C these indices should be removed from consideration. The indices of C may be the i j indices that are in A but not ZM . The indices of C may be arranged such that

The FRR may be determined 31 where FRR P i j and compared 34 to a desired false rejection rate. If FRR is greater than the desired false rejection rate then the data set may be rejected 37 even though FAR is less than or equal to the desired false acceptance rate. Otherwise the data set may be accepted.

It may now be recognized that a simplified form of a method according to the invention may be executed as follows 

The invention may also be embodied as a computer readable memory device for executing a method according to the invention including those described above. See . For example in a system according to the invention a memory device may be a compact disc having computer readable instructions for causing a computer processor to execute steps of a method according to the invention. A processor may be in communication with a data base in which the data set may be stored and the processor may be able to cause a monitor to display whether the data set is acceptable. Additional detail regarding a system according to the invention is provided in . For brevity the steps of such a method will not be outlined here but it should suffice to say that any of the methods outlined above may be translated into computer readable code which will cause a computer to determine and compare FARand or FRR as part of a system designed to determine whether a data set is acceptable for making a decision.

U.S. provisional patent application No. 60 643 853 discloses additional details about the invention and additional embodiments of the invention. The disclosure of that patent application is incorporated by this reference.

Although the present invention has been described with respect to one or more particular embodiments it will be understood that other embodiments of the present invention may be made without departing from the spirit and scope of the present invention. Hence the present invention is deemed limited only by the appended claims and the reasonable interpretation thereof.

